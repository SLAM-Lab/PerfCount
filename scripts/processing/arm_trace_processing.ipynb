{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd87ff86-58fb-44c7-9251-f8b3f3723418",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Arm Server 1.5GHz\n",
    "import pandas as pd\n",
    "\n",
    "job_lists = [\n",
    "        'job_lists/arm_server/arm_server_1.5GHz_500', \n",
    "        'job_lists/arm_server/arm_server_1.5GHz_502',\n",
    "        'job_lists/arm_server/arm_server_1.5GHz_505',\n",
    "        'job_lists/arm_server/arm_server_1.5GHz_520',\n",
    "        'job_lists/arm_server/arm_server_1.5GHz_523',\n",
    "        'job_lists/arm_server/arm_server_1.5GHz_525',\n",
    "        'job_lists/arm_server/arm_server_1.5GHz_531',\n",
    "        'job_lists/arm_server/arm_server_1.5GHz_541',\n",
    "        'job_lists/arm_server/arm_server_1.5GHz_548',\n",
    "        'job_lists/arm_server/arm_server_1.5GHz_557',\n",
    "        'job_lists/arm_server/arm_server_1.5GHz_503',\n",
    "        'job_lists/arm_server/arm_server_1.5GHz_507',\n",
    "        'job_lists/arm_server/arm_server_1.5GHz_508',\n",
    "        'job_lists/arm_server/arm_server_1.5GHz_510',\n",
    "        'job_lists/arm_server/arm_server_1.5GHz_511',\n",
    "        'job_lists/arm_server/arm_server_1.5GHz_519',\n",
    "        'job_lists/arm_server/arm_server_1.5GHz_521',\n",
    "        'job_lists/arm_server/arm_server_1.5GHz_527',\n",
    "        'job_lists/arm_server/arm_server_1.5GHz_538',\n",
    "        'job_lists/arm_server/arm_server_1.5GHz_544',\n",
    "        'job_lists/arm_server/arm_server_1.5GHz_549',\n",
    "        'job_lists/arm_server/arm_server_1.5GHz_554'\n",
    "]\n",
    "\n",
    "num_counters = 4\n",
    "\n",
    "for job_list in job_lists:\n",
    "\n",
    "    chunks = [1000000000, 2000000000, 5000000000, 10000000000, 20000000000]\n",
    "    \n",
    "    for chunk_size in chunks:\n",
    "        input_file = open(job_list, 'r')\n",
    "        csv_files = input_file.readlines()\n",
    "\n",
    "        merged_df = pd.read_csv('blank.csv')\n",
    "        initial_read = 0\n",
    "    \n",
    "        for csv in csv_files:\n",
    "            csv = csv.strip()\n",
    "            df1 = pd.read_csv(csv)\n",
    "            df1['time'] = df1['time'].astype(float).round(2)\n",
    "            df1.drop(df1.columns[4:], axis=1, inplace=True)\n",
    "            df1.drop(df1.columns[2], axis=1, inplace=True)\n",
    "    \n",
    "            # Get counter 1\n",
    "            df2 = df1.iloc[num_counters-0::4, :]\n",
    "            counter_name = df2.iloc[num_counters-0][2]\n",
    "            df2 = df2.rename(columns={'value': counter_name})\n",
    "            df2.drop(df2.columns[2], axis=1, inplace = True)\n",
    "            df2 = df2.round({'time':2})\n",
    "           \n",
    "            # Get Counter 2\n",
    "            df3 = df1.iloc[num_counters-3::4, :]\n",
    "            counter_name = df3.iloc[num_counters-0][2]\n",
    "            df3 = df3.rename(columns={'value': counter_name})\n",
    "            df3.drop(df3.columns[2], axis=1, inplace = True)\n",
    "            df3 = df3.round({'time':2})\n",
    "            \n",
    "            # Get Counter 3\n",
    "            df4 = df1.iloc[num_counters-2::4, :]\n",
    "            counter_name = df4.iloc[num_counters-0][2]\n",
    "            df4 = df4.rename(columns={'value': counter_name})\n",
    "            df4.drop(df4.columns[2], axis=1, inplace = True)\n",
    "            df4 = df4.round({'time':2})\n",
    "            \n",
    "            # Get Counter 4\n",
    "            df5 = df1.iloc[num_counters-1::4, :]\n",
    "            counter_name = df5.iloc[num_counters-0][2]\n",
    "            df5 = df5.rename(columns={'value': counter_name})\n",
    "            df5.drop(df5.columns[2], axis=1, inplace = True)\n",
    "            df5 = df5.round({'time':2})\n",
    "    \n",
    "            # Merge DFs\n",
    "            df6 = pd.merge(df2, df3, on = 'time', how='inner')\n",
    "            df6 = df6.round({'time':2})\n",
    "            df7 = pd.merge(df4, df5, on='time', how='inner')\n",
    "            df7 = df7.round({'time':2})\n",
    "            df8 = pd.merge(df6, df7, on='time', how='inner')\n",
    "            df8 = df8.round({'time':2})\n",
    "    \n",
    "            #print(df8)\n",
    "    \n",
    "            # Replace all non-numeric values with 0\n",
    "            df8 = df8.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "            inst_per_chunk = chunk_size\n",
    "    \n",
    "            chunk_inst_value = 0\n",
    "            temp_chunk_inst_value = 0\n",
    "            ipc_values = []\n",
    "            row_start = 0\n",
    "                \n",
    "            for start_row in range(0, len(df8)):\n",
    "                temp_chunk_inst_value = df8.iloc[start_row]['instructions']\n",
    "                \n",
    "                if chunk_inst_value >= inst_per_chunk: #or (chunk_inst_value + temp_chunk_inst_value) >= inst_per_chunk:\n",
    "                    chunk_data = df8.iloc[row_start:start_row].sum()\n",
    "                    #chunk_data['instructions'] = inst_per_chunk\n",
    "                    ipc_values.append(chunk_data)\n",
    "                    chunk_inst_value = 0\n",
    "                    row_start = start_row\n",
    "                chunk_inst_value += temp_chunk_inst_value\n",
    "                \n",
    "            # Handle last chunk\n",
    "            chunk_data = df8.iloc[row_start:len(df8)].sum()\n",
    "           # chunk_data['instructions'] = inst_per_chunk\n",
    "            ipc_values.append(chunk_data)\n",
    "\n",
    "            # Create dataframe\n",
    "            ipc_df = pd.DataFrame(ipc_values)\n",
    "            \n",
    "            if initial_read == 0:\n",
    "                merged_df = ipc_df #pd.concat([merged_df, df8], axis=1)\n",
    "                initial_read = 1\n",
    "            else:\n",
    "                ipc_df = ipc_df.drop(columns=['time'])\n",
    "                ipc_df = ipc_df.drop(columns=['instructions'])\n",
    "                merged_df = pd.concat([merged_df, ipc_df], axis=1)\n",
    "    \n",
    "            #print('merged_df')\n",
    "            #print(merged_df)\n",
    "            merged_df.drop(merged_df.tail(10).index,inplace=True)\n",
    "            \n",
    "        merged_df.to_csv('../../Data/traces/inst_aligned_traces/' + job_list[10:] + '_' + str(chunk_size) + '.csv', sep=',', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb410c8-86b5-4f9c-b7c1-e482b06d202d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Arm Server 3.0GHz\n",
    "import pandas as pd\n",
    "\n",
    "job_lists = [\n",
    "        'job_lists/arm_server/arm_server_3.0GHz_500', \n",
    "        'job_lists/arm_server/arm_server_3.0GHz_502',\n",
    "        'job_lists/arm_server/arm_server_3.0GHz_505',\n",
    "        'job_lists/arm_server/arm_server_3.0GHz_520',\n",
    "        'job_lists/arm_server/arm_server_3.0GHz_523',\n",
    "        'job_lists/arm_server/arm_server_3.0GHz_525',\n",
    "        'job_lists/arm_server/arm_server_3.0GHz_531',\n",
    "        'job_lists/arm_server/arm_server_3.0GHz_541',\n",
    "        'job_lists/arm_server/arm_server_3.0GHz_548',\n",
    "        'job_lists/arm_server/arm_server_3.0GHz_557',\n",
    "        'job_lists/arm_server/arm_server_3.0GHz_503',\n",
    "        'job_lists/arm_server/arm_server_3.0GHz_507',\n",
    "        'job_lists/arm_server/arm_server_3.0GHz_508',\n",
    "        'job_lists/arm_server/arm_server_3.0GHz_510',\n",
    "        'job_lists/arm_server/arm_server_3.0GHz_511',\n",
    "        'job_lists/arm_server/arm_server_3.0GHz_519',\n",
    "        'job_lists/arm_server/arm_server_3.0GHz_521',\n",
    "        'job_lists/arm_server/arm_server_3.0GHz_527',\n",
    "        'job_lists/arm_server/arm_server_3.0GHz_538',\n",
    "        'job_lists/arm_server/arm_server_3.0GHz_544',\n",
    "        'job_lists/arm_server/arm_server_3.0GHz_549',\n",
    "        'job_lists/arm_server/arm_server_3.0GHz_554'\n",
    "]\n",
    "\n",
    "num_counters = 4\n",
    "\n",
    "for job_list in job_lists:\n",
    "\n",
    "\n",
    "    chunks = [1000000000, 2000000000, 5000000000, 10000000000, 20000000000]\n",
    "    \n",
    "    for chunk_size in chunks:\n",
    "        input_file = open(job_list, 'r')\n",
    "        csv_files = input_file.readlines()\n",
    "    \n",
    "        merged_df = pd.read_csv('blank.csv')\n",
    "        initial_read = 0    \n",
    "        for csv in csv_files:\n",
    "            csv = csv.strip()\n",
    "            df1 = pd.read_csv(csv)\n",
    "            df1['time'] = df1['time'].astype(float).round(2)\n",
    "            df1.drop(df1.columns[4:], axis=1, inplace=True)\n",
    "            df1.drop(df1.columns[2], axis=1, inplace=True)\n",
    "    \n",
    "            # Get counter 1\n",
    "            df2 = df1.iloc[num_counters-0::4, :]\n",
    "            counter_name = df2.iloc[num_counters-0][2]\n",
    "            df2 = df2.rename(columns={'value': counter_name})\n",
    "            df2.drop(df2.columns[2], axis=1, inplace = True)\n",
    "            df2 = df2.round({'time':2})\n",
    "           \n",
    "            # Get Counter 2\n",
    "            df3 = df1.iloc[num_counters-3::4, :]\n",
    "            counter_name = df3.iloc[num_counters-0][2]\n",
    "            df3 = df3.rename(columns={'value': counter_name})\n",
    "            df3.drop(df3.columns[2], axis=1, inplace = True)\n",
    "            df3 = df3.round({'time':2})\n",
    "            \n",
    "            # Get Counter 3\n",
    "            df4 = df1.iloc[num_counters-2::4, :]\n",
    "            counter_name = df4.iloc[num_counters-0][2]\n",
    "            df4 = df4.rename(columns={'value': counter_name})\n",
    "            df4.drop(df4.columns[2], axis=1, inplace = True)\n",
    "            df4 = df4.round({'time':2})\n",
    "            \n",
    "            # Get Counter 4\n",
    "            df5 = df1.iloc[num_counters-1::4, :]\n",
    "            counter_name = df5.iloc[num_counters-0][2]\n",
    "            df5 = df5.rename(columns={'value': counter_name})\n",
    "            df5.drop(df5.columns[2], axis=1, inplace = True)\n",
    "            df5 = df5.round({'time':2})\n",
    "    \n",
    "            # Merge DFs\n",
    "            df6 = pd.merge(df2, df3, on = 'time', how='inner')\n",
    "            df6 = df6.round({'time':2})\n",
    "            df7 = pd.merge(df4, df5, on='time', how='inner')\n",
    "            df7 = df7.round({'time':2})\n",
    "            df8 = pd.merge(df6, df7, on='time', how='inner')\n",
    "            df8 = df8.round({'time':2})\n",
    "    \n",
    "            #print(df8)\n",
    "    \n",
    "            # Replace all non-numeric values with 0\n",
    "            df8 = df8.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "            inst_per_chunk = chunk_size\n",
    "    \n",
    "            chunk_inst_value = 0\n",
    "            temp_chunk_inst_value = 0\n",
    "            ipc_values = []\n",
    "            row_start = 0\n",
    "                \n",
    "            for start_row in range(0, len(df8)):\n",
    "                temp_chunk_inst_value = df8.iloc[start_row]['instructions']\n",
    "                \n",
    "                if chunk_inst_value >= inst_per_chunk: #or (chunk_inst_value + temp_chunk_inst_value) >= inst_per_chunk:\n",
    "                    chunk_data = df8.iloc[row_start:start_row].sum()\n",
    "                    #chunk_data['instructions'] = inst_per_chunk\n",
    "                    ipc_values.append(chunk_data)\n",
    "                    chunk_inst_value = 0\n",
    "                    row_start = start_row\n",
    "                chunk_inst_value += temp_chunk_inst_value\n",
    "                \n",
    "            # Handle last chunk\n",
    "            chunk_data = df8.iloc[row_start:len(df8)].sum()\n",
    "            #chunk_data['instructions'] = inst_per_chunk\n",
    "            ipc_values.append(chunk_data)\n",
    "        \n",
    "            # Create CSV\n",
    "            ipc_df = pd.DataFrame(ipc_values)\n",
    "            #ipc_df.to_csv('../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/' + file_path + '_' + str(chunk_size), index=False)\n",
    "            #print(ipc_df)\n",
    "            \n",
    "            if initial_read == 0:\n",
    "                merged_df = ipc_df #pd.concat([merged_df, df8], axis=1)\n",
    "                initial_read = 1\n",
    "            else:\n",
    "                ipc_df = ipc_df.drop(columns=['time'])\n",
    "                ipc_df = ipc_df.drop(columns=['instructions'])\n",
    "                merged_df = pd.concat([merged_df, ipc_df], axis=1)\n",
    "    \n",
    "            #print('merged_df')\n",
    "            #print(merged_df)\n",
    "            merged_df.drop(merged_df.tail(10).index,inplace=True)\n",
    "                \n",
    "        merged_df.to_csv('../../Data/traces/inst_aligned_traces/' + job_list[10:] + '_' + str(chunk_size) + '.csv', sep=',', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09edba8c-69f0-4307-a34a-129e0f4be269",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Arm Desktop 1.5GHz\n",
    "import pandas as pd\n",
    "\n",
    "job_lists = [\n",
    "        'job_lists/arm_desktop/arm_desktop_1.5GHz_500',\n",
    "        'job_lists/arm_desktop/arm_desktop_1.5GHz_502', \n",
    "        'job_lists/arm_desktop/arm_desktop_1.5GHz_505', \n",
    "        'job_lists/arm_desktop/arm_desktop_1.5GHz_520', \n",
    "        'job_lists/arm_desktop/arm_desktop_1.5GHz_523', \n",
    "        'job_lists/arm_desktop/arm_desktop_1.5GHz_525', \n",
    "        'job_lists/arm_desktop/arm_desktop_1.5GHz_531', \n",
    "        'job_lists/arm_desktop/arm_desktop_1.5GHz_541', \n",
    "        'job_lists/arm_desktop/arm_desktop_1.5GHz_548', \n",
    "        'job_lists/arm_desktop/arm_desktop_1.5GHz_557', \n",
    "        'job_lists/arm_desktop/arm_desktop_1.5GHz_503', \n",
    "        'job_lists/arm_desktop/arm_desktop_1.5GHz_507', \n",
    "        'job_lists/arm_desktop/arm_desktop_1.5GHz_508', \n",
    "        'job_lists/arm_desktop/arm_desktop_1.5GHz_510', \n",
    "        'job_lists/arm_desktop/arm_desktop_1.5GHz_511', \n",
    "        'job_lists/arm_desktop/arm_desktop_1.5GHz_519', \n",
    "        'job_lists/arm_desktop/arm_desktop_1.5GHz_521', \n",
    "        'job_lists/arm_desktop/arm_desktop_1.5GHz_527', \n",
    "        'job_lists/arm_desktop/arm_desktop_1.5GHz_538', \n",
    "        'job_lists/arm_desktop/arm_desktop_1.5GHz_544', \n",
    "        'job_lists/arm_desktop/arm_desktop_1.5GHz_549', \n",
    "        'job_lists/arm_desktop/arm_desktop_1.5GHz_554' \n",
    "]\n",
    "\n",
    "num_counters = 4\n",
    "\n",
    "for job_list in job_lists:\n",
    "\n",
    "\n",
    "    chunks = [1000000000, 2000000000, 5000000000, 10000000000, 20000000000]\n",
    "    \n",
    "    for chunk_size in chunks:\n",
    "        input_file = open(job_list, 'r')\n",
    "        csv_files = input_file.readlines()\n",
    "    \n",
    "        merged_df = pd.read_csv('blank.csv')\n",
    "        initial_read = 0    \n",
    "        for csv in csv_files:\n",
    "            csv = csv.strip()\n",
    "            df1 = pd.read_csv(csv)\n",
    "            df1['time'] = df1['time'].astype(float).round(2)\n",
    "            df1.drop(df1.columns[4:], axis=1, inplace=True)\n",
    "            df1.drop(df1.columns[2], axis=1, inplace=True)\n",
    "    \n",
    "            # Get counter 1\n",
    "            df2 = df1.iloc[num_counters-0::4, :]\n",
    "            counter_name = df2.iloc[num_counters-0][2]\n",
    "            df2 = df2.rename(columns={'value': counter_name})\n",
    "            df2.drop(df2.columns[2], axis=1, inplace = True)\n",
    "            df2 = df2.round({'time':2})\n",
    "           \n",
    "            # Get Counter 2\n",
    "            df3 = df1.iloc[num_counters-3::4, :]\n",
    "            counter_name = df3.iloc[num_counters-0][2]\n",
    "            df3 = df3.rename(columns={'value': counter_name})\n",
    "            df3.drop(df3.columns[2], axis=1, inplace = True)\n",
    "            df3 = df3.round({'time':2})\n",
    "            \n",
    "            # Get Counter 3\n",
    "            df4 = df1.iloc[num_counters-2::4, :]\n",
    "            counter_name = df4.iloc[num_counters-0][2]\n",
    "            df4 = df4.rename(columns={'value': counter_name})\n",
    "            df4.drop(df4.columns[2], axis=1, inplace = True)\n",
    "            df4 = df4.round({'time':2})\n",
    "            \n",
    "            # Get Counter 4\n",
    "            df5 = df1.iloc[num_counters-1::4, :]\n",
    "            counter_name = df5.iloc[num_counters-0][2]\n",
    "            df5 = df5.rename(columns={'value': counter_name})\n",
    "            df5.drop(df5.columns[2], axis=1, inplace = True)\n",
    "            df5 = df5.round({'time':2})\n",
    "    \n",
    "            # Merge DFs\n",
    "            df6 = pd.merge(df2, df3, on = 'time', how='inner')\n",
    "            df6 = df6.round({'time':2})\n",
    "            df7 = pd.merge(df4, df5, on='time', how='inner')\n",
    "            df7 = df7.round({'time':2})\n",
    "            df8 = pd.merge(df6, df7, on='time', how='inner')\n",
    "            df8 = df8.round({'time':2})\n",
    "    \n",
    "            #print(df8)\n",
    "    \n",
    "            # Replace all non-numeric values with 0\n",
    "            df8 = df8.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "            inst_per_chunk = chunk_size\n",
    "    \n",
    "            chunk_inst_value = 0\n",
    "            temp_chunk_inst_value = 0\n",
    "            ipc_values = []\n",
    "            row_start = 0\n",
    "                \n",
    "            for start_row in range(0, len(df8)):\n",
    "                temp_chunk_inst_value = df8.iloc[start_row]['instructions']\n",
    "                \n",
    "                if chunk_inst_value >= inst_per_chunk: #or (chunk_inst_value + temp_chunk_inst_value) >= inst_per_chunk:\n",
    "                    chunk_data = df8.iloc[row_start:start_row].sum()\n",
    "                    #chunk_data['instructions'] = inst_per_chunk\n",
    "                    ipc_values.append(chunk_data)\n",
    "                    chunk_inst_value = 0\n",
    "                    row_start = start_row\n",
    "                chunk_inst_value += temp_chunk_inst_value\n",
    "                \n",
    "            # Handle last chunk\n",
    "            chunk_data = df8.iloc[row_start:len(df8)].sum()\n",
    "            #chunk_data['instructions'] = inst_per_chunk\n",
    "            ipc_values.append(chunk_data)\n",
    "        \n",
    "            # Create CSV\n",
    "            ipc_df = pd.DataFrame(ipc_values)\n",
    "            #ipc_df.to_csv('../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/' + file_path + '_' + str(chunk_size), index=False)\n",
    "            #print(ipc_df)\n",
    "            \n",
    "            if initial_read == 0:\n",
    "                merged_df = ipc_df #pd.concat([merged_df, df8], axis=1)\n",
    "                initial_read = 1\n",
    "            else:\n",
    "                ipc_df = ipc_df.drop(columns=['time'])\n",
    "                ipc_df = ipc_df.drop(columns=['instructions'])\n",
    "                merged_df = pd.concat([merged_df, ipc_df], axis=1)\n",
    "    \n",
    "            #print('merged_df')\n",
    "            #print(merged_df)\n",
    "            merged_df.drop(merged_df.tail(10).index,inplace=True)\n",
    "                \n",
    "        merged_df.to_csv('../../Data/traces/inst_aligned_traces/' + job_list[10:] + '_' + str(chunk_size) + '.csv', sep=',', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f351ed6-5e41-4e5a-952b-7be2ded0a943",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Arm Desktop 3.0GHz\n",
    "import pandas as pd\n",
    "\n",
    "job_lists = [\n",
    "        'job_lists/arm_desktop/arm_desktop_3.0GHz_500',\n",
    "        'job_lists/arm_desktop/arm_desktop_3.0GHz_502', \n",
    "        'job_lists/arm_desktop/arm_desktop_3.0GHz_505', \n",
    "        'job_lists/arm_desktop/arm_desktop_3.0GHz_520', \n",
    "        'job_lists/arm_desktop/arm_desktop_3.0GHz_523', \n",
    "        'job_lists/arm_desktop/arm_desktop_3.0GHz_525', \n",
    "        'job_lists/arm_desktop/arm_desktop_3.0GHz_531', \n",
    "        'job_lists/arm_desktop/arm_desktop_3.0GHz_541', \n",
    "        'job_lists/arm_desktop/arm_desktop_3.0GHz_548', \n",
    "        'job_lists/arm_desktop/arm_desktop_3.0GHz_557', \n",
    "        'job_lists/arm_desktop/arm_desktop_3.0GHz_503', \n",
    "        'job_lists/arm_desktop/arm_desktop_3.0GHz_507', \n",
    "        'job_lists/arm_desktop/arm_desktop_3.0GHz_508', \n",
    "        'job_lists/arm_desktop/arm_desktop_3.0GHz_510', \n",
    "        'job_lists/arm_desktop/arm_desktop_3.0GHz_511', \n",
    "        'job_lists/arm_desktop/arm_desktop_3.0GHz_519', \n",
    "        'job_lists/arm_desktop/arm_desktop_3.0GHz_521', \n",
    "        'job_lists/arm_desktop/arm_desktop_3.0GHz_527', \n",
    "        'job_lists/arm_desktop/arm_desktop_3.0GHz_538', \n",
    "        'job_lists/arm_desktop/arm_desktop_3.0GHz_544', \n",
    "        'job_lists/arm_desktop/arm_desktop_3.0GHz_549', \n",
    "        'job_lists/arm_desktop/arm_desktop_3.0GHz_554' \n",
    "]\n",
    "\n",
    "num_counters = 4\n",
    "\n",
    "for job_list in job_lists:\n",
    "    chunks = [1000000000, 2000000000, 5000000000, 10000000000, 20000000000]\n",
    "    \n",
    "    for chunk_size in chunks:\n",
    "        input_file = open(job_list, 'r')\n",
    "        csv_files = input_file.readlines()\n",
    "    \n",
    "        merged_df = pd.read_csv('blank.csv')\n",
    "        initial_read = 0    \n",
    "    \n",
    "        for csv in csv_files:\n",
    "            csv = csv.strip()\n",
    "            df1 = pd.read_csv(csv)\n",
    "            df1['time'] = df1['time'].astype(float).round(2)\n",
    "            df1.drop(df1.columns[4:], axis=1, inplace=True)\n",
    "            df1.drop(df1.columns[2], axis=1, inplace=True)\n",
    "    \n",
    "            # Get counter 1\n",
    "            df2 = df1.iloc[num_counters-0::4, :]\n",
    "            counter_name = df2.iloc[num_counters-0][2]\n",
    "            df2 = df2.rename(columns={'value': counter_name})\n",
    "            df2.drop(df2.columns[2], axis=1, inplace = True)\n",
    "            df2 = df2.round({'time':2})\n",
    "           \n",
    "            # Get Counter 2\n",
    "            df3 = df1.iloc[num_counters-3::4, :]\n",
    "            counter_name = df3.iloc[num_counters-0][2]\n",
    "            df3 = df3.rename(columns={'value': counter_name})\n",
    "            df3.drop(df3.columns[2], axis=1, inplace = True)\n",
    "            df3 = df3.round({'time':2})\n",
    "            \n",
    "            # Get Counter 3\n",
    "            df4 = df1.iloc[num_counters-2::4, :]\n",
    "            counter_name = df4.iloc[num_counters-0][2]\n",
    "            df4 = df4.rename(columns={'value': counter_name})\n",
    "            df4.drop(df4.columns[2], axis=1, inplace = True)\n",
    "            df4 = df4.round({'time':2})\n",
    "            \n",
    "            # Get Counter 4\n",
    "            df5 = df1.iloc[num_counters-1::4, :]\n",
    "            counter_name = df5.iloc[num_counters-0][2]\n",
    "            df5 = df5.rename(columns={'value': counter_name})\n",
    "            df5.drop(df5.columns[2], axis=1, inplace = True)\n",
    "            df5 = df5.round({'time':2})\n",
    "    \n",
    "            # Merge DFs\n",
    "            df6 = pd.merge(df2, df3, on = 'time', how='inner')\n",
    "            df6 = df6.round({'time':2})\n",
    "            df7 = pd.merge(df4, df5, on='time', how='inner')\n",
    "            df7 = df7.round({'time':2})\n",
    "            df8 = pd.merge(df6, df7, on='time', how='inner')\n",
    "            df8 = df8.round({'time':2})\n",
    "    \n",
    "            #print(df8)\n",
    "    \n",
    "            # Replace all non-numeric values with 0\n",
    "            df8 = df8.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "            inst_per_chunk = chunk_size\n",
    "    \n",
    "            chunk_inst_value = 0\n",
    "            temp_chunk_inst_value = 0\n",
    "            ipc_values = []\n",
    "            row_start = 0\n",
    "                \n",
    "            for start_row in range(0, len(df8)):\n",
    "                temp_chunk_inst_value = df8.iloc[start_row]['instructions']\n",
    "                \n",
    "                if chunk_inst_value >= inst_per_chunk: #or (chunk_inst_value + temp_chunk_inst_value) >= inst_per_chunk:\n",
    "                    chunk_data = df8.iloc[row_start:start_row].sum()\n",
    "                    #chunk_data['instructions'] = inst_per_chunk\n",
    "                    ipc_values.append(chunk_data)\n",
    "                    chunk_inst_value = 0\n",
    "                    row_start = start_row\n",
    "                chunk_inst_value += temp_chunk_inst_value\n",
    "                \n",
    "            # Handle last chunk\n",
    "            chunk_data = df8.iloc[row_start:len(df8)].sum()\n",
    "            #chunk_data['instructions'] = inst_per_chunk\n",
    "            ipc_values.append(chunk_data)\n",
    "        \n",
    "            # Create CSV\n",
    "            ipc_df = pd.DataFrame(ipc_values)\n",
    "            #ipc_df.to_csv('../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/' + file_path + '_' + str(chunk_size), index=False)\n",
    "            #print(ipc_df)\n",
    "            \n",
    "            if initial_read == 0:\n",
    "                merged_df = ipc_df #pd.concat([merged_df, df8], axis=1)\n",
    "                initial_read = 1\n",
    "            else:\n",
    "                ipc_df = ipc_df.drop(columns=['time'])\n",
    "                ipc_df = ipc_df.drop(columns=['instructions'])\n",
    "                merged_df = pd.concat([merged_df, ipc_df], axis=1)\n",
    "    \n",
    "            #print('merged_df')\n",
    "            #print(merged_df)\n",
    "            merged_df.drop(merged_df.tail(10).index,inplace=True)\n",
    "                \n",
    "        merged_df.to_csv('../../Data/traces/inst_aligned_traces/' + job_list[10:] + '_' + str(chunk_size) + '.csv', sep=',', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dae58ef-e53b-4cc1-b745-81d1626c8998",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Arm InO\n",
    "import pandas as pd\n",
    "\n",
    "job_lists = [\n",
    "        'job_lists/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_500',\n",
    "        'job_lists/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_502', \n",
    "        'job_lists/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_505', \n",
    "        'job_lists/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_520', \n",
    "        'job_lists/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_523', \n",
    "        'job_lists/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_525', \n",
    "        'job_lists/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_531', \n",
    "        'job_lists/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_541', \n",
    "        'job_lists/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_548', \n",
    "        'job_lists/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_557', \n",
    "        'job_lists/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_503', \n",
    "        'job_lists/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_507', \n",
    "        'job_lists/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_508', \n",
    "        'job_lists/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_510', \n",
    "        'job_lists/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_511', \n",
    "        'job_lists/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_519', \n",
    "        'job_lists/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_521', \n",
    "        'job_lists/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_527', \n",
    "        'job_lists/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_538', \n",
    "        'job_lists/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_544', \n",
    "        'job_lists/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_549', \n",
    "        'job_lists/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_554' \n",
    "]\n",
    "\n",
    "num_counters = 2\n",
    "\n",
    "for job_list in job_lists:\n",
    "    chunks = [1000000000, 2000000000, 5000000000, 10000000000, 20000000000]\n",
    "    \n",
    "    for chunk_size in chunks:\n",
    "        input_file = open(job_list, 'r')\n",
    "        csv_files = input_file.readlines()\n",
    "    \n",
    "        merged_df = pd.read_csv('blank.csv')\n",
    "        initial_read = 0    \n",
    "        \n",
    "        for csv in csv_files:\n",
    "            csv = csv.strip()\n",
    "            df1 = pd.read_csv(csv)\n",
    "            df1['time'] = df1['time'].astype(float).round(2)\n",
    "            df1.drop(df1.columns[4:], axis=1, inplace=True)\n",
    "            df1.drop(df1.columns[2], axis=1, inplace=True)\n",
    "    \n",
    "            # Get counter 1\n",
    "            df2 = df1.iloc[num_counters-0::2, :]\n",
    "            counter_name = df2.iloc[num_counters-0][2]\n",
    "            df2 = df2.rename(columns={'value': counter_name})\n",
    "            df2.drop(df2.columns[2], axis=1, inplace = True)\n",
    "            df2 = df2.round({'time':2})\n",
    "            \n",
    "            # Get Counter 2\n",
    "            df3 = df1.iloc[num_counters-1::2, :]\n",
    "            counter_name = df3.iloc[num_counters-1][2]\n",
    "            df3 = df3.rename(columns={'value': counter_name})\n",
    "            df3.drop(df3.columns[2], axis=1, inplace = True)\n",
    "            df3 = df3.round({'time':2})\n",
    "            \n",
    "            # Merge DFs\n",
    "            df4 = pd.merge(df2, df3, on = 'time', how='inner')\n",
    "            df4 = df4.round({'time':2})\n",
    "    \n",
    "            # Replace all non-numeric values with 0\n",
    "            df4 = df4.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "            inst_per_chunk = chunk_size\n",
    "    \n",
    "            chunk_inst_value = 0\n",
    "            temp_chunk_inst_value = 0\n",
    "            ipc_values = []\n",
    "            row_start = 0\n",
    "                \n",
    "            for start_row in range(0, len(df4)):\n",
    "                temp_chunk_inst_value = df4.iloc[start_row]['instructions:u']\n",
    "                \n",
    "                if chunk_inst_value >= inst_per_chunk: #or (chunk_inst_value + temp_chunk_inst_value) >= inst_per_chunk:\n",
    "                    chunk_data = df4.iloc[row_start:start_row].sum()\n",
    "                    #chunk_data['instructions:u'] = inst_per_chunk\n",
    "                    ipc_values.append(chunk_data)\n",
    "                    chunk_inst_value = 0\n",
    "                    row_start = start_row\n",
    "                chunk_inst_value += temp_chunk_inst_value\n",
    "                \n",
    "            # Handle last chunk\n",
    "            chunk_data = df4.iloc[row_start:len(df4)].sum()\n",
    "            #chunk_data['instructions:u'] = inst_per_chunk\n",
    "            ipc_values.append(chunk_data)\n",
    "        \n",
    "            # Create CSV\n",
    "            ipc_df = pd.DataFrame(ipc_values)\n",
    "            \n",
    "            if initial_read == 0:\n",
    "                merged_df = ipc_df\n",
    "                initial_read = 1\n",
    "            else:\n",
    "                ipc_df = ipc_df.drop(columns=['time'])\n",
    "                ipc_df = ipc_df.drop(columns=['instructions:u'])\n",
    "                merged_df = pd.concat([merged_df, ipc_df], axis=1)\n",
    "    \n",
    "            merged_df.drop(merged_df.tail(10).index,inplace=True)\n",
    "                \n",
    "        merged_df.to_csv('../../Data/traces/inst_aligned_traces/' + job_list[10:] + '_' + str(chunk_size) + '.csv', sep=',', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d97f342-2ddf-418c-932c-29fc3d7e481d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Arm OOO\n",
    "import pandas as pd\n",
    "\n",
    "job_lists = [\n",
    "        'job_lists/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_500',\n",
    "        'job_lists/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_502', \n",
    "        'job_lists/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_505', \n",
    "        'job_lists/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_520', \n",
    "        'job_lists/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_523', \n",
    "        'job_lists/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_525', \n",
    "        'job_lists/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_531', \n",
    "        'job_lists/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_541', \n",
    "        'job_lists/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_548', \n",
    "        'job_lists/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_557', \n",
    "        'job_lists/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_503', \n",
    "        'job_lists/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_507', \n",
    "        'job_lists/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_508', \n",
    "        'job_lists/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_510', \n",
    "        'job_lists/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_511', \n",
    "        'job_lists/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_519', \n",
    "        'job_lists/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_521', \n",
    "        'job_lists/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_527', \n",
    "        'job_lists/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_538', \n",
    "        'job_lists/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_544', \n",
    "        'job_lists/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_549', \n",
    "        'job_lists/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_554' \n",
    "]\n",
    "\n",
    "num_counters = 2\n",
    "\n",
    "for job_list in job_lists:\n",
    "    chunks = [1000000000, 2000000000, 5000000000, 10000000000, 20000000000]\n",
    "    \n",
    "    for chunk_size in chunks:\n",
    "        input_file = open(job_list, 'r')\n",
    "        csv_files = input_file.readlines()\n",
    "    \n",
    "        merged_df = pd.read_csv('blank.csv')\n",
    "        initial_read = 0    \n",
    "    \n",
    "        for csv in csv_files:\n",
    "            csv = csv.strip()\n",
    "            df1 = pd.read_csv(csv)\n",
    "            df1['time'] = df1['time'].astype(float).round(2)\n",
    "            df1.drop(df1.columns[4:], axis=1, inplace=True)\n",
    "            df1.drop(df1.columns[2], axis=1, inplace=True)\n",
    "    \n",
    "            # Get counter 1\n",
    "            df2 = df1.iloc[num_counters-0::2, :]\n",
    "            counter_name = df2.iloc[num_counters-0][2]\n",
    "            df2 = df2.rename(columns={'value': counter_name})\n",
    "            df2.drop(df2.columns[2], axis=1, inplace = True)\n",
    "            df2 = df2.round({'time':2})\n",
    "            \n",
    "            # Get Counter 2\n",
    "            df3 = df1.iloc[num_counters-1::2, :]\n",
    "            counter_name = df3.iloc[num_counters-1][2]\n",
    "            df3 = df3.rename(columns={'value': counter_name})\n",
    "            df3.drop(df3.columns[2], axis=1, inplace = True)\n",
    "            df3 = df3.round({'time':2})\n",
    "            \n",
    "            # Merge DFs\n",
    "            df4 = pd.merge(df2, df3, on = 'time', how='inner')\n",
    "            df4 = df4.round({'time':2})\n",
    "            \n",
    "            # Replace all non-numeric values with 0\n",
    "            df4 = df4.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "            inst_per_chunk = chunk_size\n",
    "    \n",
    "            chunk_inst_value = 0\n",
    "            temp_chunk_inst_value = 0\n",
    "            ipc_values = []\n",
    "            row_start = 0\n",
    "                \n",
    "            for start_row in range(0, len(df4)):\n",
    "                temp_chunk_inst_value = df4.iloc[start_row]['instructions:u']\n",
    "                \n",
    "                if chunk_inst_value >= inst_per_chunk: #or (chunk_inst_value + temp_chunk_inst_value) >= inst_per_chunk:\n",
    "                    chunk_data = df4.iloc[row_start:start_row].sum()\n",
    "                    #chunk_data['instructions:u'] = inst_per_chunk\n",
    "                    ipc_values.append(chunk_data)\n",
    "                    chunk_inst_value = 0\n",
    "                    row_start = start_row\n",
    "                chunk_inst_value += temp_chunk_inst_value\n",
    "                \n",
    "            # Handle last chunk\n",
    "            chunk_data = df4.iloc[row_start:len(df4)].sum()\n",
    "            #chunk_data['instructions:u'] = inst_per_chunk\n",
    "            ipc_values.append(chunk_data)\n",
    "        \n",
    "            # Create CSV\n",
    "            ipc_df = pd.DataFrame(ipc_values)\n",
    "            \n",
    "            if initial_read == 0:\n",
    "                merged_df = ipc_df\n",
    "                initial_read = 1\n",
    "            else:\n",
    "                ipc_df = ipc_df.drop(columns=['time'])\n",
    "                ipc_df = ipc_df.drop(columns=['instructions:u'])\n",
    "                merged_df = pd.concat([merged_df, ipc_df], axis=1)\n",
    "    \n",
    "            merged_df.drop(merged_df.tail(10).index,inplace=True)\n",
    "                \n",
    "        merged_df.to_csv('../../Data/traces/inst_aligned_traces/' + job_list[10:] + '_' + str(chunk_size) + '.csv', sep=',', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fab58a-674f-4e22-b265-0956589d6d5a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Arm OOO Cache\n",
    "import pandas as pd\n",
    "\n",
    "job_lists = [\n",
    "        'job_lists/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_cache_500',\n",
    "        'job_lists/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_cache_502', \n",
    "        'job_lists/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_cache_505', \n",
    "        'job_lists/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_cache_520', \n",
    "        'job_lists/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_cache_523', \n",
    "        'job_lists/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_cache_525', \n",
    "        'job_lists/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_cache_531', \n",
    "        'job_lists/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_cache_541', \n",
    "        'job_lists/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_cache_548', \n",
    "        'job_lists/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_cache_557', \n",
    "        'job_lists/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_cache_503', \n",
    "        'job_lists/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_cache_507', \n",
    "        'job_lists/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_cache_508', \n",
    "        'job_lists/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_cache_510', \n",
    "        'job_lists/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_cache_511', \n",
    "        'job_lists/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_cache_519', \n",
    "        'job_lists/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_cache_521', \n",
    "        'job_lists/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_cache_527', \n",
    "        'job_lists/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_cache_538', \n",
    "        'job_lists/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_cache_544', \n",
    "        'job_lists/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_cache_549', \n",
    "        'job_lists/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_cache_554' \n",
    "]\n",
    "\n",
    "num_counters = 2\n",
    "\n",
    "for job_list in job_lists:\n",
    "    chunks = [1000000000, 2000000000, 5000000000, 10000000000, 20000000000]\n",
    "    \n",
    "    for chunk_size in chunks:\n",
    "        input_file = open(job_list, 'r')\n",
    "        csv_files = input_file.readlines()\n",
    "    \n",
    "        merged_df = pd.read_csv('blank.csv')\n",
    "        initial_read = 0    \n",
    "    \n",
    "        for csv in csv_files:\n",
    "            csv = csv.strip()\n",
    "            df1 = pd.read_csv(csv)\n",
    "            df1['time'] = df1['time'].astype(float).round(2)\n",
    "            df1.drop(df1.columns[4:], axis=1, inplace=True)\n",
    "            df1.drop(df1.columns[2], axis=1, inplace=True)\n",
    "    \n",
    "            # Get counter 1\n",
    "            df2 = df1.iloc[num_counters-0::2, :]\n",
    "            counter_name = df2.iloc[num_counters-0][2]\n",
    "            df2 = df2.rename(columns={'value': counter_name})\n",
    "            df2.drop(df2.columns[2], axis=1, inplace = True)\n",
    "            df2 = df2.round({'time':2})\n",
    "            \n",
    "            # Get Counter 2\n",
    "            df3 = df1.iloc[num_counters-1::2, :]\n",
    "            counter_name = df3.iloc[num_counters-1][2]\n",
    "            df3 = df3.rename(columns={'value': counter_name})\n",
    "            df3.drop(df3.columns[2], axis=1, inplace = True)\n",
    "            df3 = df3.round({'time':2})\n",
    "            \n",
    "            # Merge DFs\n",
    "            df4 = pd.merge(df2, df3, on = 'time', how='inner')\n",
    "            df4 = df4.round({'time':2})\n",
    "            \n",
    "            # Replace all non-numeric values with 0\n",
    "            df4 = df4.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "            inst_per_chunk = chunk_size\n",
    "    \n",
    "            chunk_inst_value = 0\n",
    "            temp_chunk_inst_value = 0\n",
    "            ipc_values = []\n",
    "            row_start = 0\n",
    "                \n",
    "            for start_row in range(0, len(df4)):\n",
    "                temp_chunk_inst_value = df4.iloc[start_row]['instructions:u']\n",
    "                \n",
    "                if chunk_inst_value >= inst_per_chunk: #or (chunk_inst_value + temp_chunk_inst_value) >= inst_per_chunk:\n",
    "                    chunk_data = df4.iloc[row_start:start_row].sum()\n",
    "                    #chunk_data['instructions:u'] = inst_per_chunk\n",
    "                    ipc_values.append(chunk_data)\n",
    "                    chunk_inst_value = 0\n",
    "                    row_start = start_row\n",
    "                chunk_inst_value += temp_chunk_inst_value\n",
    "                \n",
    "            # Handle last chunk\n",
    "            chunk_data = df4.iloc[row_start:len(df4)].sum()\n",
    "            #chunk_data['instructions:u'] = inst_per_chunk\n",
    "            ipc_values.append(chunk_data)\n",
    "        \n",
    "            # Create CSV\n",
    "            ipc_df = pd.DataFrame(ipc_values)\n",
    "            \n",
    "            if initial_read == 0:\n",
    "                merged_df = ipc_df\n",
    "                initial_read = 1\n",
    "            else:\n",
    "                ipc_df = ipc_df.drop(columns=['time'])\n",
    "                ipc_df = ipc_df.drop(columns=['instructions:u'])\n",
    "                merged_df = pd.concat([merged_df, ipc_df], axis=1)\n",
    "    \n",
    "            merged_df.drop(merged_df.tail(10).index,inplace=True)\n",
    "                       \n",
    "        merged_df.to_csv('../../Data/traces/inst_aligned_traces/' + job_list[10:] + '_' + str(chunk_size) + '.csv', sep=',', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

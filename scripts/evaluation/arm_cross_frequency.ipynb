{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "93e518bb-30f5-41ce-afb6-2ef8e8d90ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_size: 1000000000\n",
      "0.054016097046296035\n",
      "chunk_size: 2000000000\n",
      "0.042022473436048854\n",
      "chunk_size: 3000000000\n",
      "0.035203562606348764\n",
      "chunk_size: 4000000000\n",
      "0.03180148137758148\n",
      "chunk_size: 5000000000\n",
      "0.029960384780602586\n",
      "chunk_size: 10000000000\n",
      "0.02607161952791216\n",
      "chunk_size: 20000000000\n",
      "0.02267863451567231\n",
      "chunk_size: 25000000000\n",
      "0.01971618905884204\n"
     ]
    }
   ],
   "source": [
    "# Arm Server 1.5GHz -> 3.0GHz per benchmark\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "def process_csv_feautures(file_path, chunk_size):\n",
    "    all_features = []\n",
    "#    for file_path in file_paths:\n",
    "    df = pd.read_csv(file_path + '_' + str(chunk_size))\n",
    "    df = df.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "    features = []\n",
    "    for index, row in df.iterrows():\n",
    "        inst = row['instructions']\n",
    "        cyc = row['cpu-cycles']\n",
    "        ipc = inst/cyc\n",
    "        features.append([\n",
    "            row['instructions'],\n",
    "            row['cpu-cycles'], \n",
    "            row['br_pred'], \n",
    "            row['br_mis_pred'], \n",
    "            row['l1d_cache'], \n",
    "            row['l1d_tlb'], \n",
    "            row['l1d_tlb_rd'], \n",
    "            row['l1d_tlb_wr'], \n",
    "            row['l2d_cache_rd'], \n",
    "            row['l2d_cache_wr'], \n",
    "            row['l2d_cache'], \n",
    "            row['l1i_cache'], \n",
    "            row['l1i_cache_refill'], \n",
    "            row['remote_access'], \n",
    "            row['dtlb_walk'], \n",
    "            row['itlb_walk'], \n",
    "            row['l1i_tlb'], \n",
    "            row['l2d_tlb'], \n",
    "            row['l2d_tlb_rd'], \n",
    "            row['l2d_tlb_wr'], \n",
    "            row['vfp_spec'], \n",
    "            row['inst_spec'], \n",
    "            row['ase_spec'], \n",
    "            row['stall_backend'], \n",
    "            row['stall_frontend'], \n",
    "            row['ll_cache_miss_rd'], \n",
    "            row['mem_access'], \n",
    "            row['mem_access_rd'], \n",
    "            row['mem_access_wr'],\n",
    "            ipc])\n",
    "    all_features.extend(features)\n",
    "    return all_features\n",
    "\n",
    "\n",
    "def process_csv_ipc(file_path, chunk_size):\n",
    "    all_ipc_values = []\n",
    "#    for file_path in file_paths:\n",
    "    df = pd.read_csv(file_path + '_' + str(chunk_size))\n",
    "    df = df.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "    for _, row in df.iterrows():\n",
    "        all_ipc_values.append(row['instructions'] / row['cpu-cycles'])  \n",
    "    return all_ipc_values\n",
    "\n",
    "one_ghz_files = [\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_500',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_502',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_505',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_520',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_523',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_525',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_531',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_541',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_548',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_557',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_503',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_507',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_508',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_510',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_511',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_519',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_521',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_527',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_538',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_544',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_549',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_554',\n",
    "]\n",
    "    \n",
    "three_ghz_files = [\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_500',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_502',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_505',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_520',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_523',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_525',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_531',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_541',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_548',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_557',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_503',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_507',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_508',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_510',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_511',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_519',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_521',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_527',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_538',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_544',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_549',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_554',\n",
    "]\n",
    "\n",
    "chunks = [1000000000,2000000000,3000000000,4000000000,5000000000,10000000000,20000000000,25000000000]\n",
    "for chunk_size in chunks:\n",
    "    print('chunk_size: ' + str(chunk_size))\n",
    "    average_test_mape = 0\n",
    "    for file in range(0,len(one_ghz_files)):\n",
    "        #print(one_ghz_files[file])\n",
    "        in_order_hardware_counters_features = process_csv_feautures(one_ghz_files[file], chunk_size)\n",
    "        out_of_order_ipc = process_csv_ipc(three_ghz_files[file], chunk_size)\n",
    "\n",
    "        data = {\n",
    "            'In Order': in_order_hardware_counters_features,\n",
    "            'Out of Order': out_of_order_ipc\n",
    "        }\n",
    "        \n",
    "        length = min(len(in_order_hardware_counters_features), len(out_of_order_ipc))\n",
    "        \n",
    "        # Shorten the longen array\n",
    "        in_order_hardware_counters_features = in_order_hardware_counters_features[:length]\n",
    "        out_of_order_ipc = out_of_order_ipc[:length]\n",
    "        \n",
    "        # separate features and target\n",
    "        X = pd.DataFrame(\n",
    "            in_order_hardware_counters_features, \n",
    "            columns=[\n",
    "                'instructions',\n",
    "                'cpu-cycles', \n",
    "                'br_pred', \n",
    "                'br_mis_pred', \n",
    "                'l1d_cache', \n",
    "                'l1d_tlb', \n",
    "                'l1d_tlb_rd', \n",
    "                'l1d_tlb_wr', \n",
    "                'l2d_cache_rd', \n",
    "                'l2d_cache_wr', \n",
    "                'l2d_cache', \n",
    "                'l1i_cache', \n",
    "                'l1i_cache_refill', \n",
    "                'remote_access', \n",
    "                'dtlb_walk', \n",
    "                'itlb_walk', \n",
    "                'l1i_tlb', \n",
    "                'l2d_tlb', \n",
    "                'l2d_tlb_rd', \n",
    "                'l2d_tlb_wr', \n",
    "                'vfp_spec', \n",
    "                'inst_spec', \n",
    "                'ase_spec', \n",
    "                'stall_backend', \n",
    "                'stall_frontend', \n",
    "                'll_cache_miss_rd', \n",
    "                'mem_access', \n",
    "                'mem_access_rd', \n",
    "                'mem_access_wr',\n",
    "                'ipc'])  # Features\n",
    "        y = pd.Series(out_of_order_ipc) \n",
    "        \n",
    "        # split \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "        \n",
    "        rf_model = RandomForestRegressor(n_estimators=100)\n",
    "        gb_model = GradientBoostingRegressor(n_estimators=100)\n",
    "        ensemble_model = gb_model\n",
    "        #ensemble_model = VotingRegressor([('rf', rf_model), ('gb', gb_model)])\n",
    "        ensemble_model.fit(X_train, y_train)\n",
    "        \n",
    "        ensemble_predictions = ensemble_model.predict(X_test)\n",
    "        # Calculate R²\n",
    "        ensemble_r2 = r2_score(y_test, ensemble_predictions)\n",
    "        \n",
    "        #bruh wut\n",
    "        train_predictions = ensemble_model.predict(X_train)\n",
    "        test_predictions = ensemble_model.predict(X_test)\n",
    "        \n",
    "        # Calculate R² for training and testing sets\n",
    "        train_r2 = r2_score(y_train, train_predictions)\n",
    "        train_mape = mean_absolute_percentage_error(y_train, train_predictions)\n",
    "        test_r2 = r2_score(y_test, test_predictions)\n",
    "        test_mape = mean_absolute_percentage_error(y_test, test_predictions)\n",
    "        \n",
    "        \n",
    "        #print(f\"Training R²: {train_r2}\")\n",
    "        #print(f\"Testing R²: {test_r2}\")\n",
    "        #print(f\"Training Mape: {train_mape}\")\n",
    "        #print(f\"Testing Mape: {test_mape}\")\n",
    "        average_test_mape += test_mape\n",
    "    print(average_test_mape/22)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ae40e296-fae4-4451-af06-98a1d53c1a04",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000000\n",
      "Training R²: 0.7747877073327094\n",
      "Testing R²: 0.7472157830145929\n",
      "Training Mape: 0.1039863957056471\n",
      "Testing Mape: 0.11136237057289103\n",
      "2000000000\n",
      "Training R²: 0.8109653201695137\n",
      "Testing R²: 0.7767789528880593\n",
      "Training Mape: 0.09381697221870643\n",
      "Testing Mape: 0.09827588352592231\n",
      "3000000000\n",
      "Training R²: 0.8311308170597065\n",
      "Testing R²: 0.805730221008394\n",
      "Training Mape: 0.08710157391202547\n",
      "Testing Mape: 0.09448803416671143\n",
      "4000000000\n",
      "Training R²: 0.8377153104035223\n",
      "Testing R²: 0.808661129428478\n",
      "Training Mape: 0.08425843482954519\n",
      "Testing Mape: 0.09142003956629374\n",
      "5000000000\n",
      "Training R²: 0.8642824776349226\n",
      "Testing R²: 0.8070512045441024\n",
      "Training Mape: 0.07625909266411036\n",
      "Testing Mape: 0.09044310528338524\n",
      "10000000000\n",
      "Training R²: 0.8932674435473521\n",
      "Testing R²: 0.7949576633091914\n",
      "Training Mape: 0.0651854964618677\n",
      "Testing Mape: 0.0944287380754397\n",
      "20000000000\n",
      "Training R²: 0.939110982637152\n",
      "Testing R²: 0.791477704406731\n",
      "Training Mape: 0.05063605390244098\n",
      "Testing Mape: 0.0762146717370401\n",
      "25000000000\n",
      "Training R²: 0.937631613871161\n",
      "Testing R²: 0.7880346170530778\n",
      "Training Mape: 0.052462268860198356\n",
      "Testing Mape: 0.0899558162995586\n"
     ]
    }
   ],
   "source": [
    "# Arm Server 1.5GHz -> 3.0GHz\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "def process_csv_feautures(file_paths, chunk_size):\n",
    "    all_features = []\n",
    "    for file_path in file_paths:\n",
    "        df = pd.read_csv(file_path + '_' + str(chunk_size))\n",
    "        df = df.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "        features = []\n",
    "        for index, row in df.iterrows():\n",
    "            inst = row['instructions']\n",
    "            cyc = row['cpu-cycles']\n",
    "            ipc = inst/cyc\n",
    "            features.append([\n",
    "                row['instructions'],\n",
    "                row['cpu-cycles'], \n",
    "                row['br_pred'], \n",
    "                row['br_mis_pred'], \n",
    "                row['l1d_cache'], \n",
    "                row['l1d_tlb'], \n",
    "                row['l1d_tlb_rd'], \n",
    "                row['l1d_tlb_wr'], \n",
    "                row['l2d_cache_rd'], \n",
    "                row['l2d_cache_wr'], \n",
    "                row['l2d_cache'], \n",
    "                row['l1i_cache'], \n",
    "                row['l1i_cache_refill'], \n",
    "                row['remote_access'], \n",
    "                row['dtlb_walk'], \n",
    "                row['itlb_walk'], \n",
    "                row['l1i_tlb'], \n",
    "                row['l2d_tlb'], \n",
    "                row['l2d_tlb_rd'], \n",
    "                row['l2d_tlb_wr'], \n",
    "                row['vfp_spec'], \n",
    "                row['inst_spec'], \n",
    "                row['ase_spec'], \n",
    "                row['stall_backend'], \n",
    "                row['stall_frontend'], \n",
    "                row['ll_cache_miss_rd'], \n",
    "                row['mem_access'], \n",
    "                row['mem_access_rd'], \n",
    "                row['mem_access_wr'],\n",
    "                ipc])\n",
    "        all_features.extend(features)\n",
    "    return all_features\n",
    "\n",
    "\n",
    "def process_csv_ipc(file_paths, chunk_size):\n",
    "    all_ipc_values = []\n",
    "    for file_path in file_paths:\n",
    "        df = pd.read_csv(file_path + '_' + str(chunk_size))\n",
    "        df = df.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "        for _, row in df.iterrows():\n",
    "            all_ipc_values.append(row['instructions'] / row['cpu-cycles'])  \n",
    "    return all_ipc_values\n",
    "\n",
    "one_ghz_files = [\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_500',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_502',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_505',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_520',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_523',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_525',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_531',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_541',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_548',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_557',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_503',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_507',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_508',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_510',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_511',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_519',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_521',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_527',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_538',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_544',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_549',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_554',\n",
    "]\n",
    "    \n",
    "three_ghz_files = [\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_500',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_502',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_505',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_520',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_523',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_525',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_531',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_541',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_548',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_557',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_503',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_507',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_508',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_510',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_511',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_519',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_521',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_527',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_538',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_544',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_549',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_554',\n",
    "]\n",
    "\n",
    "chunks = [1000000000,2000000000,3000000000,4000000000,5000000000,10000000000,20000000000,25000000000]\n",
    "for chunk_size in chunks:\n",
    "    print(chunk_size)\n",
    "    in_order_hardware_counters_features = process_csv_feautures(one_ghz_files, chunk_size)\n",
    "    out_of_order_ipc = process_csv_ipc(three_ghz_files, chunk_size)\n",
    "\n",
    "    data = {\n",
    "        'In Order': in_order_hardware_counters_features,\n",
    "        'Out of Order': out_of_order_ipc\n",
    "    }\n",
    "    \n",
    "    length = min(len(in_order_hardware_counters_features), len(out_of_order_ipc))\n",
    "    \n",
    "    # Shorten the longen array\n",
    "    in_order_hardware_counters_features = in_order_hardware_counters_features[:length]\n",
    "    out_of_order_ipc = out_of_order_ipc[:length]\n",
    "    \n",
    "    # separate features and target\n",
    "    X = pd.DataFrame(\n",
    "        in_order_hardware_counters_features, \n",
    "        columns=[\n",
    "            'instructions',\n",
    "            'cpu-cycles', \n",
    "            'br_pred', \n",
    "            'br_mis_pred', \n",
    "            'l1d_cache', \n",
    "            'l1d_tlb', \n",
    "            'l1d_tlb_rd', \n",
    "            'l1d_tlb_wr', \n",
    "            'l2d_cache_rd', \n",
    "            'l2d_cache_wr', \n",
    "            'l2d_cache', \n",
    "            'l1i_cache', \n",
    "            'l1i_cache_refill', \n",
    "            'remote_access', \n",
    "            'dtlb_walk', \n",
    "            'itlb_walk', \n",
    "            'l1i_tlb', \n",
    "            'l2d_tlb', \n",
    "            'l2d_tlb_rd', \n",
    "            'l2d_tlb_wr', \n",
    "            'vfp_spec', \n",
    "            'inst_spec', \n",
    "            'ase_spec', \n",
    "            'stall_backend', \n",
    "            'stall_frontend', \n",
    "            'll_cache_miss_rd', \n",
    "            'mem_access', \n",
    "            'mem_access_rd', \n",
    "            'mem_access_wr',\n",
    "            'ipc'])  # Features\n",
    "    y = pd.Series(out_of_order_ipc) \n",
    "    \n",
    "    # split \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    rf_model = RandomForestRegressor(n_estimators=100)\n",
    "    gb_model = GradientBoostingRegressor(n_estimators=100)\n",
    "    ensemble_model = gb_model\n",
    "    #ensemble_model = VotingRegressor([('rf', rf_model), ('gb', gb_model)])\n",
    "    ensemble_model.fit(X_train, y_train)\n",
    "    \n",
    "    ensemble_predictions = ensemble_model.predict(X_test)\n",
    "    # Calculate R²\n",
    "    ensemble_r2 = r2_score(y_test, ensemble_predictions)\n",
    "    \n",
    "    #bruh wut\n",
    "    train_predictions = ensemble_model.predict(X_train)\n",
    "    test_predictions = ensemble_model.predict(X_test)\n",
    "    \n",
    "    # Calculate R² for training and testing sets\n",
    "    train_r2 = r2_score(y_train, train_predictions)\n",
    "    train_mape = mean_absolute_percentage_error(y_train, train_predictions)\n",
    "    test_r2 = r2_score(y_test, test_predictions)\n",
    "    test_mape = mean_absolute_percentage_error(y_test, test_predictions)\n",
    "    \n",
    "    \n",
    "    print(f\"Training R²: {train_r2}\")\n",
    "    print(f\"Testing R²: {test_r2}\")\n",
    "    print(f\"Training Mape: {train_mape}\")\n",
    "    print(f\"Testing Mape: {test_mape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6c606f18-9515-4a2f-9682-81de479d3bf3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_size: 1000000000\n",
      "Training R²: 0.7311560030566375\n",
      "Testing R²: 0.7028916944445027\n",
      "Training Mape: 0.09774397581938267\n",
      "Testing Mape: 0.1024575832700276\n",
      "chunk_size: 2000000000\n",
      "Training R²: 0.7900761482636315\n",
      "Testing R²: 0.744561578089336\n",
      "Training Mape: 0.08220037940062935\n",
      "Testing Mape: 0.08824452078183354\n",
      "chunk_size: 3000000000\n",
      "Training R²: 0.8197591441478957\n",
      "Testing R²: 0.7873532849628531\n",
      "Training Mape: 0.07510611073088662\n",
      "Testing Mape: 0.07983893616280623\n",
      "chunk_size: 4000000000\n",
      "Training R²: 0.8333670968511047\n",
      "Testing R²: 0.7885050921260413\n",
      "Training Mape: 0.07231974606692589\n",
      "Testing Mape: 0.07754665926295644\n",
      "chunk_size: 5000000000\n",
      "Training R²: 0.8493743098156987\n",
      "Testing R²: 0.7745311842739575\n",
      "Training Mape: 0.06723786128004236\n",
      "Testing Mape: 0.08111047688834389\n",
      "chunk_size: 10000000000\n",
      "Training R²: 0.8894770624282083\n",
      "Testing R²: 0.7714752064011249\n",
      "Training Mape: 0.05672170912872436\n",
      "Testing Mape: 0.07982599626127655\n",
      "chunk_size: 20000000000\n",
      "Training R²: 0.9182411699348124\n",
      "Testing R²: 0.8034287606399693\n",
      "Training Mape: 0.0497276618614989\n",
      "Testing Mape: 0.06560339348976503\n",
      "chunk_size: 25000000000\n",
      "Training R²: 0.9266304897986495\n",
      "Testing R²: 0.7506896429341762\n",
      "Training Mape: 0.0485443113453042\n",
      "Testing Mape: 0.07800522333914812\n"
     ]
    }
   ],
   "source": [
    "# Arm Server 3.0GHz -> 1.5GHz\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "def process_csv_feautures(file_paths, chunk_size):\n",
    "    all_features = []\n",
    "    for file_path in file_paths:\n",
    "        df = pd.read_csv(file_path + '_' + str(chunk_size))\n",
    "        df = df.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "        features = []\n",
    "        for index, row in df.iterrows():\n",
    "            inst = row['instructions']\n",
    "            cyc = row['cpu-cycles']\n",
    "            ipc = inst/cyc\n",
    "            features.append([\n",
    "                row['instructions'],\n",
    "                row['cpu-cycles'], \n",
    "                row['br_pred'], \n",
    "                row['br_mis_pred'], \n",
    "                row['l1d_cache'], \n",
    "                row['l1d_tlb'], \n",
    "                row['l1d_tlb_rd'], \n",
    "                row['l1d_tlb_wr'], \n",
    "                row['l2d_cache_rd'], \n",
    "                row['l2d_cache_wr'], \n",
    "                row['l2d_cache'], \n",
    "                row['l1i_cache'], \n",
    "                row['l1i_cache_refill'], \n",
    "                row['remote_access'], \n",
    "                row['dtlb_walk'], \n",
    "                row['itlb_walk'], \n",
    "                row['l1i_tlb'], \n",
    "                row['l2d_tlb'], \n",
    "                row['l2d_tlb_rd'], \n",
    "                row['l2d_tlb_wr'], \n",
    "                row['vfp_spec'], \n",
    "                row['inst_spec'], \n",
    "                row['ase_spec'], \n",
    "                row['stall_backend'], \n",
    "                row['stall_frontend'], \n",
    "                row['ll_cache_miss_rd'], \n",
    "                row['mem_access'], \n",
    "                row['mem_access_rd'], \n",
    "                row['mem_access_wr'],\n",
    "                ipc])\n",
    "        all_features.extend(features)\n",
    "    return all_features\n",
    "\n",
    "\n",
    "def process_csv_ipc(file_paths, chunk_size):\n",
    "    all_ipc_values = []\n",
    "    for file_path in file_paths:\n",
    "        df = pd.read_csv(file_path + '_' + str(chunk_size))\n",
    "        df = df.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "        for _, row in df.iterrows():\n",
    "            all_ipc_values.append(row['instructions'] / row['cpu-cycles'])  \n",
    "    return all_ipc_values\n",
    "\n",
    "one_ghz_files = [\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_500',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_502',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_505',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_520',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_523',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_525',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_531',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_541',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_548',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_557',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_503',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_507',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_508',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_510',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_511',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_519',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_521',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_527',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_538',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_544',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_549',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_554',\n",
    "]\n",
    "    \n",
    "three_ghz_files = [\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_500',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_502',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_505',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_520',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_523',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_525',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_531',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_541',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_548',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_557',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_503',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_507',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_508',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_510',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_511',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_519',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_521',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_527',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_538',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_544',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_549',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_554',\n",
    "]\n",
    "\n",
    "chunks = [1000000000,2000000000,3000000000,4000000000,5000000000,10000000000,20000000000,25000000000]\n",
    "for chunk_size in chunks:\n",
    "    in_order_hardware_counters_features = process_csv_feautures(three_ghz_files, chunk_size)\n",
    "    out_of_order_ipc = process_csv_ipc(one_ghz_files, chunk_size)\n",
    "    print('chunk_size: ' + str(chunk_size))\n",
    "    \n",
    "    data = {\n",
    "        'In Order': in_order_hardware_counters_features,\n",
    "        'Out of Order': out_of_order_ipc\n",
    "    }\n",
    "    \n",
    "    length = min(len(in_order_hardware_counters_features), len(out_of_order_ipc))\n",
    "    \n",
    "    # Shorten the longen array\n",
    "    in_order_hardware_counters_features = in_order_hardware_counters_features[:length]\n",
    "    out_of_order_ipc = out_of_order_ipc[:length]\n",
    "    \n",
    "    # separate features and target\n",
    "    X = pd.DataFrame(\n",
    "        in_order_hardware_counters_features, \n",
    "        columns=[\n",
    "            'instructions',\n",
    "            'cpu-cycles', \n",
    "            'br_pred', \n",
    "            'br_mis_pred', \n",
    "            'l1d_cache', \n",
    "            'l1d_tlb', \n",
    "            'l1d_tlb_rd', \n",
    "            'l1d_tlb_wr', \n",
    "            'l2d_cache_rd', \n",
    "            'l2d_cache_wr', \n",
    "            'l2d_cache', \n",
    "            'l1i_cache', \n",
    "            'l1i_cache_refill', \n",
    "            'remote_access', \n",
    "            'dtlb_walk', \n",
    "            'itlb_walk', \n",
    "            'l1i_tlb', \n",
    "            'l2d_tlb', \n",
    "            'l2d_tlb_rd', \n",
    "            'l2d_tlb_wr', \n",
    "            'vfp_spec', \n",
    "            'inst_spec', \n",
    "            'ase_spec', \n",
    "            'stall_backend', \n",
    "            'stall_frontend', \n",
    "            'll_cache_miss_rd', \n",
    "            'mem_access', \n",
    "            'mem_access_rd', \n",
    "            'mem_access_wr',\n",
    "            'ipc'])  # Features\n",
    "    y = pd.Series(out_of_order_ipc) \n",
    "    \n",
    "    # split \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    rf_model = RandomForestRegressor(n_estimators=100)\n",
    "    gb_model = GradientBoostingRegressor(n_estimators=100)\n",
    "    ensemble_model = gb_model\n",
    "    #ensemble_model = VotingRegressor([('rf', rf_model), ('gb', gb_model)])\n",
    "    ensemble_model.fit(X_train, y_train)\n",
    "    \n",
    "    ensemble_predictions = ensemble_model.predict(X_test)\n",
    "    # Calculate R²\n",
    "    ensemble_r2 = r2_score(y_test, ensemble_predictions)\n",
    "    \n",
    "    #bruh wut\n",
    "    train_predictions = ensemble_model.predict(X_train)\n",
    "    test_predictions = ensemble_model.predict(X_test)\n",
    "    \n",
    "    # Calculate R² for training and testing sets\n",
    "    train_r2 = r2_score(y_train, train_predictions)\n",
    "    train_mape = mean_absolute_percentage_error(y_train, train_predictions)\n",
    "    test_r2 = r2_score(y_test, test_predictions)\n",
    "    test_mape = mean_absolute_percentage_error(y_test, test_predictions)\n",
    "    \n",
    "    \n",
    "    print(f\"Training R²: {train_r2}\")\n",
    "    print(f\"Testing R²: {test_r2}\")\n",
    "    print(f\"Training Mape: {train_mape}\")\n",
    "    print(f\"Testing Mape: {test_mape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1ef61e63-ac41-435d-b360-0691bd3ba1d3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_size: 1000000000\n",
      "Training R²: 0.7311560030566375\n",
      "Testing R²: 0.7029870708237731\n",
      "Training Mape: 0.0977439758193827\n",
      "Testing Mape: 0.1024574291235517\n",
      "chunk_size: 2000000000\n",
      "Training R²: 0.7900761482636316\n",
      "Testing R²: 0.7446185582976441\n",
      "Training Mape: 0.08220037940062928\n",
      "Testing Mape: 0.08823537183681124\n",
      "chunk_size: 3000000000\n",
      "Training R²: 0.8197591441478957\n",
      "Testing R²: 0.7869624352046728\n",
      "Training Mape: 0.07510611073088667\n",
      "Testing Mape: 0.0799111066215638\n",
      "chunk_size: 4000000000\n",
      "Training R²: 0.8333670968511047\n",
      "Testing R²: 0.7885123742482926\n",
      "Training Mape: 0.07231974606692591\n",
      "Testing Mape: 0.07752149843585548\n",
      "chunk_size: 5000000000\n",
      "Training R²: 0.8493743098156987\n",
      "Testing R²: 0.7758279924618484\n",
      "Training Mape: 0.06723786128004239\n",
      "Testing Mape: 0.08095809917840616\n",
      "chunk_size: 10000000000\n",
      "Training R²: 0.8894770624282083\n",
      "Testing R²: 0.772366989177234\n",
      "Training Mape: 0.05672170912872436\n",
      "Testing Mape: 0.07980980792176051\n",
      "chunk_size: 20000000000\n",
      "Training R²: 0.9182411699348124\n",
      "Testing R²: 0.7996715628378452\n",
      "Training Mape: 0.049727661861498906\n",
      "Testing Mape: 0.06621468369897501\n",
      "chunk_size: 25000000000\n",
      "Training R²: 0.9266304897986495\n",
      "Testing R²: 0.7589296742268975\n",
      "Training Mape: 0.048544311345304215\n",
      "Testing Mape: 0.07639994920356397\n"
     ]
    }
   ],
   "source": [
    "# Arm Server 3.0GHz -> 1.5GHz per benchmark\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "def process_csv_feautures(file_paths, chunk_size):\n",
    "    all_features = []\n",
    "    for file_path in file_paths:\n",
    "        df = pd.read_csv(file_path + '_' + str(chunk_size))\n",
    "        df = df.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "        features = []\n",
    "        for index, row in df.iterrows():\n",
    "            inst = row['instructions']\n",
    "            cyc = row['cpu-cycles']\n",
    "            ipc = inst/cyc\n",
    "            features.append([\n",
    "                row['instructions'],\n",
    "                row['cpu-cycles'], \n",
    "                row['br_pred'], \n",
    "                row['br_mis_pred'], \n",
    "                row['l1d_cache'], \n",
    "                row['l1d_tlb'], \n",
    "                row['l1d_tlb_rd'], \n",
    "                row['l1d_tlb_wr'], \n",
    "                row['l2d_cache_rd'], \n",
    "                row['l2d_cache_wr'], \n",
    "                row['l2d_cache'], \n",
    "                row['l1i_cache'], \n",
    "                row['l1i_cache_refill'], \n",
    "                row['remote_access'], \n",
    "                row['dtlb_walk'], \n",
    "                row['itlb_walk'], \n",
    "                row['l1i_tlb'], \n",
    "                row['l2d_tlb'], \n",
    "                row['l2d_tlb_rd'], \n",
    "                row['l2d_tlb_wr'], \n",
    "                row['vfp_spec'], \n",
    "                row['inst_spec'], \n",
    "                row['ase_spec'], \n",
    "                row['stall_backend'], \n",
    "                row['stall_frontend'], \n",
    "                row['ll_cache_miss_rd'], \n",
    "                row['mem_access'], \n",
    "                row['mem_access_rd'], \n",
    "                row['mem_access_wr'],\n",
    "                ipc])\n",
    "        all_features.extend(features)\n",
    "    return all_features\n",
    "\n",
    "\n",
    "def process_csv_ipc(file_paths, chunk_size):\n",
    "    all_ipc_values = []\n",
    "    for file_path in file_paths:\n",
    "        df = pd.read_csv(file_path + '_' + str(chunk_size))\n",
    "        df = df.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "        for _, row in df.iterrows():\n",
    "            all_ipc_values.append(row['instructions'] / row['cpu-cycles'])  \n",
    "    return all_ipc_values\n",
    "\n",
    "one_ghz_files = [\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_500',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_502',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_505',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_520',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_523',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_525',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_531',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_541',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_548',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_557',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_503',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_507',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_508',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_510',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_511',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_519',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_521',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_527',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_538',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_544',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_549',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_554',\n",
    "]\n",
    "    \n",
    "three_ghz_files = [\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_500',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_502',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_505',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_520',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_523',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_525',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_531',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_541',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_548',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_557',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_503',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_507',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_508',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_510',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_511',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_519',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_521',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_527',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_538',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_544',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_549',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_554',\n",
    "]\n",
    "\n",
    "chunks = [1000000000,2000000000,3000000000,4000000000,5000000000,10000000000,20000000000,25000000000]\n",
    "for chunk_size in chunks:\n",
    "    in_order_hardware_counters_features = process_csv_feautures(three_ghz_files, chunk_size)\n",
    "    out_of_order_ipc = process_csv_ipc(one_ghz_files, chunk_size)\n",
    "    print('chunk_size: ' + str(chunk_size))\n",
    "    \n",
    "    data = {\n",
    "        'In Order': in_order_hardware_counters_features,\n",
    "        'Out of Order': out_of_order_ipc\n",
    "    }\n",
    "    \n",
    "    length = min(len(in_order_hardware_counters_features), len(out_of_order_ipc))\n",
    "    \n",
    "    # Shorten the longen array\n",
    "    in_order_hardware_counters_features = in_order_hardware_counters_features[:length]\n",
    "    out_of_order_ipc = out_of_order_ipc[:length]\n",
    "    \n",
    "    # separate features and target\n",
    "    X = pd.DataFrame(\n",
    "        in_order_hardware_counters_features, \n",
    "        columns=[\n",
    "            'instructions',\n",
    "            'cpu-cycles', \n",
    "            'br_pred', \n",
    "            'br_mis_pred', \n",
    "            'l1d_cache', \n",
    "            'l1d_tlb', \n",
    "            'l1d_tlb_rd', \n",
    "            'l1d_tlb_wr', \n",
    "            'l2d_cache_rd', \n",
    "            'l2d_cache_wr', \n",
    "            'l2d_cache', \n",
    "            'l1i_cache', \n",
    "            'l1i_cache_refill', \n",
    "            'remote_access', \n",
    "            'dtlb_walk', \n",
    "            'itlb_walk', \n",
    "            'l1i_tlb', \n",
    "            'l2d_tlb', \n",
    "            'l2d_tlb_rd', \n",
    "            'l2d_tlb_wr', \n",
    "            'vfp_spec', \n",
    "            'inst_spec', \n",
    "            'ase_spec', \n",
    "            'stall_backend', \n",
    "            'stall_frontend', \n",
    "            'll_cache_miss_rd', \n",
    "            'mem_access', \n",
    "            'mem_access_rd', \n",
    "            'mem_access_wr',\n",
    "            'ipc'])  # Features\n",
    "    y = pd.Series(out_of_order_ipc) \n",
    "    \n",
    "    # split \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    rf_model = RandomForestRegressor(n_estimators=100)\n",
    "    gb_model = GradientBoostingRegressor(n_estimators=100)\n",
    "    ensemble_model = gb_model\n",
    "    #ensemble_model = VotingRegressor([('rf', rf_model), ('gb', gb_model)])\n",
    "    ensemble_model.fit(X_train, y_train)\n",
    "    \n",
    "    ensemble_predictions = ensemble_model.predict(X_test)\n",
    "    # Calculate R²\n",
    "    ensemble_r2 = r2_score(y_test, ensemble_predictions)\n",
    "    \n",
    "    #bruh wut\n",
    "    train_predictions = ensemble_model.predict(X_train)\n",
    "    test_predictions = ensemble_model.predict(X_test)\n",
    "    \n",
    "    # Calculate R² for training and testing sets\n",
    "    train_r2 = r2_score(y_train, train_predictions)\n",
    "    train_mape = mean_absolute_percentage_error(y_train, train_predictions)\n",
    "    test_r2 = r2_score(y_test, test_predictions)\n",
    "    test_mape = mean_absolute_percentage_error(y_test, test_predictions)\n",
    "    \n",
    "    \n",
    "    print(f\"Training R²: {train_r2}\")\n",
    "    print(f\"Testing R²: {test_r2}\")\n",
    "    print(f\"Training Mape: {train_mape}\")\n",
    "    print(f\"Testing Mape: {test_mape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5c5b2d94-ab24-47ec-aa04-03748625d704",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_size: 1000000000\n",
      "Training R²: 0.8532276877653872\n",
      "Testing R²: 0.844318412270775\n",
      "Training Mape: 0.09935631138618829\n",
      "Testing Mape: 0.1007511398407588\n",
      "chunk_size: 2000000000\n",
      "Training R²: 0.8899783175783558\n",
      "Testing R²: 0.889313381207904\n",
      "Training Mape: 0.08143747139366687\n",
      "Testing Mape: 0.08443697544419648\n",
      "chunk_size: 3000000000\n",
      "Training R²: 0.9185044215241572\n",
      "Testing R²: 0.9025287922223153\n",
      "Training Mape: 0.06706969244587695\n",
      "Testing Mape: 0.06940173197812953\n",
      "chunk_size: 4000000000\n",
      "Training R²: 0.9377085972213356\n",
      "Testing R²: 0.8914171919011225\n",
      "Training Mape: 0.05638423977706261\n",
      "Testing Mape: 0.06430734894050914\n",
      "chunk_size: 5000000000\n",
      "Training R²: 0.9403029758492388\n",
      "Testing R²: 0.9137048810087119\n",
      "Training Mape: 0.054554105810011556\n",
      "Testing Mape: 0.05930108703743655\n",
      "chunk_size: 10000000000\n",
      "Training R²: 0.9723883168216384\n",
      "Testing R²: 0.9100976789148273\n",
      "Training Mape: 0.036396091882553466\n",
      "Testing Mape: 0.05161951297424254\n",
      "chunk_size: 20000000000\n",
      "Training R²: 0.9811263711044121\n",
      "Testing R²: 0.895523526747049\n",
      "Training Mape: 0.031329514752393685\n",
      "Testing Mape: 0.053334616128476425\n",
      "chunk_size: 25000000000\n",
      "Training R²: 0.9889779472323519\n",
      "Testing R²: 0.9486666433704503\n",
      "Training Mape: 0.022038381758704054\n",
      "Testing Mape: 0.04160220879759998\n"
     ]
    }
   ],
   "source": [
    "# Arm Desktop 3.0GHz -> 1.5GHz\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "def process_csv_feautures(file_paths, chunk_size):\n",
    "    all_features = []\n",
    "    for file_path in file_paths:\n",
    "        df = pd.read_csv(file_path + '_' + str(chunk_size))\n",
    "        df = df.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "        features = []\n",
    "        for index, row in df.iterrows():\n",
    "            inst = row['instructions']\n",
    "            cyc = row['cpu-cycles']\n",
    "            ipc = inst/cyc\n",
    "            features.append([\n",
    "            row['instructions'],\n",
    "            row['cpu-cycles'],\n",
    "            row['br_pred'],\n",
    "            row['br_mis_pred'],\n",
    "            row['l1d_cache_rd'],\n",
    "            row['l1d_cache_wr'],\n",
    "            row['l1d_cache'],\n",
    "            row['l1i_cache'],\n",
    "            row['l1i_cache_refill'],\n",
    "            row['context-switches'],\n",
    "            row['l2d_cache_rd'],\n",
    "            row['l2d_cache_wr'],\n",
    "            row['l2d_cache'],\n",
    "            row['l1d_tlb'],\n",
    "            row['l1d_tlb_refill_rd'],\n",
    "            row['l1d_tlb_refill_wr'],\n",
    "            row['dtlb_walk'],\n",
    "            row['itlb_walk'],\n",
    "            row['page-faults'],\n",
    "            row['l2d_tlb_access'],\n",
    "            row['l2i_tlb_access'],\n",
    "            row['l1i_tlb_refill'],\n",
    "            row['iTLB-loads'],\n",
    "            row['iTLB-load-misses'],\n",
    "            row['branch-loads'],\n",
    "            row['dTLB-loads'],\n",
    "            row['dTLB-load-misses'],\n",
    "            row['branch-load-misses'],\n",
    "            row['vfp_spec'],\n",
    "            row['inst_spec'],\n",
    "            row['ase_spec'],\n",
    "            row['bx_stall'],\n",
    "            row['decode_stall'],\n",
    "            row['dispatch_stall'],\n",
    "            row['fx_stall'],\n",
    "            row['ixa_stall'],\n",
    "            row['ixb_stall'],\n",
    "            row['lx_stall'],\n",
    "            row['sx_stall'],\n",
    "            row['bus_access'],\n",
    "            row['mem_access'],\n",
    "            row['mem_access_rd'],\n",
    "            row['mem_access_wr'],\n",
    "                ipc])\n",
    "        all_features.extend(features)\n",
    "    return all_features\n",
    "\n",
    "\n",
    "def process_csv_ipc(file_paths, chunk_size):\n",
    "    all_ipc_values = []\n",
    "    for file_path in file_paths:\n",
    "        df = pd.read_csv(file_path + '_' + str(chunk_size))\n",
    "        df = df.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "        for _, row in df.iterrows():\n",
    "            all_ipc_values.append(row['instructions'] / row['cpu-cycles'])  \n",
    "    return all_ipc_values\n",
    "\n",
    "one_ghz_files = [\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_500',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_502',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_505',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_520',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_523',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_525',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_531',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_541',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_548',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_557',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_503',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_507',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_508',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_510',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_511',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_519',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_521',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_527',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_538',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_544',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_549',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_554',\n",
    "]\n",
    "    \n",
    "three_ghz_files = [\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_500',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_502',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_505',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_520',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_523',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_525',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_531',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_541',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_548',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_557',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_503',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_507',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_508',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_510',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_511',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_519',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_521',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_527',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_538',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_544',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_549',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_554',\n",
    "]\n",
    "\n",
    "chunks = [1000000000,2000000000,3000000000,4000000000,5000000000,10000000000,20000000000,25000000000]\n",
    "for chunk_size in chunks:\n",
    "    in_order_hardware_counters_features = process_csv_feautures(three_ghz_files, chunk_size)\n",
    "    out_of_order_ipc = process_csv_ipc(one_ghz_files, chunk_size)\n",
    "    print('chunk_size: ' + str(chunk_size))\n",
    "    \n",
    "    data = {\n",
    "        'In Order': in_order_hardware_counters_features,\n",
    "        'Out of Order': out_of_order_ipc\n",
    "    }\n",
    "    \n",
    "    length = min(len(in_order_hardware_counters_features), len(out_of_order_ipc))\n",
    "    \n",
    "    # Shorten the longen array\n",
    "    in_order_hardware_counters_features = in_order_hardware_counters_features[:length]\n",
    "    out_of_order_ipc = out_of_order_ipc[:length]\n",
    "    \n",
    "    # separate features and target\n",
    "    X = pd.DataFrame(\n",
    "        in_order_hardware_counters_features, \n",
    "        columns=[\n",
    "            'instructions',\n",
    "            'cpu-cycles',\n",
    "            'br_pred',\n",
    "            'br_mis_pred',\n",
    "            'l1d_cache_rd',\n",
    "            'l1d_cache_wr',\n",
    "            'l1d_cache',\n",
    "            'l1i_cache',\n",
    "            'l1i_cache_refill',\n",
    "            'context-switches',\n",
    "            'l2d_cache_rd',\n",
    "            'l2d_cache_wr',\n",
    "            'l2d_cache',\n",
    "            'l1d_tlb',\n",
    "            'l1d_tlb_refill_rd',\n",
    "            'l1d_tlb_refill_wr',\n",
    "            'dtlb_walk',\n",
    "            'itlb_walk',\n",
    "            'page-faults',\n",
    "            'l2d_tlb_access',\n",
    "            'l2i_tlb_access',\n",
    "            'l1i_tlb_refill',\n",
    "            'iTLB-loads',\n",
    "            'iTLB-load-misses',\n",
    "            'branch-loads',\n",
    "            'dTLB-loads',\n",
    "            'dTLB-load-misses',\n",
    "            'branch-load-misses',\n",
    "            'vfp_spec',\n",
    "            'inst_spec',\n",
    "            'ase_spec',\n",
    "            'bx_stall',\n",
    "            'decode_stall',\n",
    "            'dispatch_stall',\n",
    "            'fx_stall',\n",
    "            'ixa_stall',\n",
    "            'ixb_stall',\n",
    "            'lx_stall',\n",
    "            'sx_stall',\n",
    "            'bus_access',\n",
    "            'mem_access',\n",
    "            'mem_access_rd',\n",
    "            'mem_access_wr',\n",
    "            'ipc'])  # Features\n",
    "    y = pd.Series(out_of_order_ipc) \n",
    "    \n",
    "    # split \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    rf_model = RandomForestRegressor(n_estimators=100)\n",
    "    gb_model = GradientBoostingRegressor(n_estimators=100)\n",
    "    ensemble_model = gb_model\n",
    "    #ensemble_model = VotingRegressor([('rf', rf_model), ('gb', gb_model)])\n",
    "    ensemble_model.fit(X_train, y_train)\n",
    "    \n",
    "    ensemble_predictions = ensemble_model.predict(X_test)\n",
    "    # Calculate R²\n",
    "    ensemble_r2 = r2_score(y_test, ensemble_predictions)\n",
    "    \n",
    "    #bruh wut\n",
    "    train_predictions = ensemble_model.predict(X_train)\n",
    "    test_predictions = ensemble_model.predict(X_test)\n",
    "    \n",
    "    # Calculate R² for training and testing sets\n",
    "    train_r2 = r2_score(y_train, train_predictions)\n",
    "    train_mape = mean_absolute_percentage_error(y_train, train_predictions)\n",
    "    test_r2 = r2_score(y_test, test_predictions)\n",
    "    test_mape = mean_absolute_percentage_error(y_test, test_predictions)\n",
    "    \n",
    "    \n",
    "    print(f\"Training R²: {train_r2}\")\n",
    "    print(f\"Testing R²: {test_r2}\")\n",
    "    print(f\"Training Mape: {train_mape}\")\n",
    "    print(f\"Testing Mape: {test_mape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a1d81b56-819d-4cfb-9303-11355bb7cb90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_size: 1000000000\n",
      "Training R²: 0.8283052279828783\n",
      "Testing R²: 0.8177545994935815\n",
      "Training Mape: 0.12103921459377222\n",
      "Testing Mape: 0.12545535924815254\n",
      "chunk_size: 2000000000\n",
      "Training R²: 0.874917221001391\n",
      "Testing R²: 0.8629028173303805\n",
      "Training Mape: 0.09790852433401365\n",
      "Testing Mape: 0.10100449401843004\n",
      "chunk_size: 3000000000\n",
      "Training R²: 0.9051597246149836\n",
      "Testing R²: 0.8794591197612545\n",
      "Training Mape: 0.08200566121183779\n",
      "Testing Mape: 0.08614042715750131\n",
      "chunk_size: 4000000000\n",
      "Training R²: 0.924190165956555\n",
      "Testing R²: 0.8795751400698442\n",
      "Training Mape: 0.06977294016244627\n",
      "Testing Mape: 0.08009100763294169\n",
      "chunk_size: 5000000000\n",
      "Training R²: 0.9283121916882733\n",
      "Testing R²: 0.9113834792669789\n",
      "Training Mape: 0.06510301501682672\n",
      "Testing Mape: 0.06963097549833205\n",
      "chunk_size: 10000000000\n",
      "Training R²: 0.9660712717427882\n",
      "Testing R²: 0.8947134989285109\n",
      "Training Mape: 0.04626375912346107\n",
      "Testing Mape: 0.07190622347069539\n",
      "chunk_size: 20000000000\n",
      "Training R²: 0.9721911809131245\n",
      "Testing R²: 0.8969185165135706\n",
      "Training Mape: 0.04052537655482748\n",
      "Testing Mape: 0.06903123668391864\n",
      "chunk_size: 25000000000\n",
      "Training R²: 0.9908362916018694\n",
      "Testing R²: 0.949307926930168\n",
      "Training Mape: 0.022768624980990946\n",
      "Testing Mape: 0.039399870422961\n"
     ]
    }
   ],
   "source": [
    "# Arm Desktop 1.5GHz -> 3.0GHz\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "def process_csv_feautures(file_paths, chunk_size):\n",
    "    all_features = []\n",
    "    for file_path in file_paths:\n",
    "        df = pd.read_csv(file_path + '_' + str(chunk_size))\n",
    "        df = df.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "        features = []\n",
    "        for index, row in df.iterrows():\n",
    "            inst = row['instructions']\n",
    "            cyc = row['cpu-cycles']\n",
    "            ipc = inst/cyc\n",
    "            features.append([\n",
    "            row['instructions'],\n",
    "            row['cpu-cycles'],\n",
    "            row['br_pred'],\n",
    "            row['br_mis_pred'],\n",
    "            row['l1d_cache_rd'],\n",
    "            row['l1d_cache_wr'],\n",
    "            row['l1d_cache'],\n",
    "            row['l1i_cache'],\n",
    "            row['l1i_cache_refill'],\n",
    "            row['context-switches'],\n",
    "            row['l2d_cache_rd'],\n",
    "            row['l2d_cache_wr'],\n",
    "            row['l2d_cache'],\n",
    "            row['l1d_tlb'],\n",
    "            row['l1d_tlb_refill_rd'],\n",
    "            row['l1d_tlb_refill_wr'],\n",
    "            row['dtlb_walk'],\n",
    "            row['itlb_walk'],\n",
    "            row['page-faults'],\n",
    "            row['l2d_tlb_access'],\n",
    "            row['l2i_tlb_access'],\n",
    "            row['l1i_tlb_refill'],\n",
    "            row['iTLB-loads'],\n",
    "            row['iTLB-load-misses'],\n",
    "            row['branch-loads'],\n",
    "            row['dTLB-loads'],\n",
    "            row['dTLB-load-misses'],\n",
    "            row['branch-load-misses'],\n",
    "            row['vfp_spec'],\n",
    "            row['inst_spec'],\n",
    "            row['ase_spec'],\n",
    "            row['bx_stall'],\n",
    "            row['decode_stall'],\n",
    "            row['dispatch_stall'],\n",
    "            row['fx_stall'],\n",
    "            row['ixa_stall'],\n",
    "            row['ixb_stall'],\n",
    "            row['lx_stall'],\n",
    "            row['sx_stall'],\n",
    "            row['bus_access'],\n",
    "            row['mem_access'],\n",
    "            row['mem_access_rd'],\n",
    "            row['mem_access_wr'],\n",
    "                ipc])\n",
    "        all_features.extend(features)\n",
    "    return all_features\n",
    "\n",
    "\n",
    "def process_csv_ipc(file_paths, chunk_size):\n",
    "    all_ipc_values = []\n",
    "    for file_path in file_paths:\n",
    "        df = pd.read_csv(file_path + '_' + str(chunk_size))\n",
    "        df = df.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "        for _, row in df.iterrows():\n",
    "            all_ipc_values.append(row['instructions'] / row['cpu-cycles'])  \n",
    "    return all_ipc_values\n",
    "\n",
    "one_ghz_files = [\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_500',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_502',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_505',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_520',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_523',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_525',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_531',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_541',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_548',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_557',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_503',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_507',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_508',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_510',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_511',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_519',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_521',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_527',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_538',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_544',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_549',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_554',\n",
    "]\n",
    "    \n",
    "three_ghz_files = [\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_500',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_502',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_505',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_520',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_523',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_525',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_531',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_541',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_548',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_557',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_503',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_507',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_508',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_510',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_511',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_519',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_521',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_527',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_538',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_544',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_549',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_554',\n",
    "]\n",
    "\n",
    "chunks = [1000000000,2000000000,3000000000,4000000000,5000000000,10000000000,20000000000,25000000000]\n",
    "for chunk_size in chunks:\n",
    "    in_order_hardware_counters_features = process_csv_feautures(one_ghz_files, chunk_size)\n",
    "    out_of_order_ipc = process_csv_ipc(three_ghz_files, chunk_size)\n",
    "    print('chunk_size: ' + str(chunk_size))\n",
    "    \n",
    "    data = {\n",
    "        'In Order': in_order_hardware_counters_features,\n",
    "        'Out of Order': out_of_order_ipc\n",
    "    }\n",
    "    \n",
    "    length = min(len(in_order_hardware_counters_features), len(out_of_order_ipc))\n",
    "    \n",
    "    # Shorten the longen array\n",
    "    in_order_hardware_counters_features = in_order_hardware_counters_features[:length]\n",
    "    out_of_order_ipc = out_of_order_ipc[:length]\n",
    "    \n",
    "    # separate features and target\n",
    "    X = pd.DataFrame(\n",
    "        in_order_hardware_counters_features, \n",
    "        columns=[\n",
    "            'instructions',\n",
    "            'cpu-cycles',\n",
    "            'br_pred',\n",
    "            'br_mis_pred',\n",
    "            'l1d_cache_rd',\n",
    "            'l1d_cache_wr',\n",
    "            'l1d_cache',\n",
    "            'l1i_cache',\n",
    "            'l1i_cache_refill',\n",
    "            'context-switches',\n",
    "            'l2d_cache_rd',\n",
    "            'l2d_cache_wr',\n",
    "            'l2d_cache',\n",
    "            'l1d_tlb',\n",
    "            'l1d_tlb_refill_rd',\n",
    "            'l1d_tlb_refill_wr',\n",
    "            'dtlb_walk',\n",
    "            'itlb_walk',\n",
    "            'page-faults',\n",
    "            'l2d_tlb_access',\n",
    "            'l2i_tlb_access',\n",
    "            'l1i_tlb_refill',\n",
    "            'iTLB-loads',\n",
    "            'iTLB-load-misses',\n",
    "            'branch-loads',\n",
    "            'dTLB-loads',\n",
    "            'dTLB-load-misses',\n",
    "            'branch-load-misses',\n",
    "            'vfp_spec',\n",
    "            'inst_spec',\n",
    "            'ase_spec',\n",
    "            'bx_stall',\n",
    "            'decode_stall',\n",
    "            'dispatch_stall',\n",
    "            'fx_stall',\n",
    "            'ixa_stall',\n",
    "            'ixb_stall',\n",
    "            'lx_stall',\n",
    "            'sx_stall',\n",
    "            'bus_access',\n",
    "            'mem_access',\n",
    "            'mem_access_rd',\n",
    "            'mem_access_wr',\n",
    "            'ipc'])  # Features\n",
    "    y = pd.Series(out_of_order_ipc) \n",
    "    \n",
    "    # split \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    rf_model = RandomForestRegressor(n_estimators=100)\n",
    "    gb_model = GradientBoostingRegressor(n_estimators=100)\n",
    "    ensemble_model = gb_model\n",
    "    #ensemble_model = VotingRegressor([('rf', rf_model), ('gb', gb_model)])\n",
    "    ensemble_model.fit(X_train, y_train)\n",
    "    \n",
    "    ensemble_predictions = ensemble_model.predict(X_test)\n",
    "    # Calculate R²\n",
    "    ensemble_r2 = r2_score(y_test, ensemble_predictions)\n",
    "    \n",
    "    #bruh wut\n",
    "    train_predictions = ensemble_model.predict(X_train)\n",
    "    test_predictions = ensemble_model.predict(X_test)\n",
    "    \n",
    "    # Calculate R² for training and testing sets\n",
    "    train_r2 = r2_score(y_train, train_predictions)\n",
    "    train_mape = mean_absolute_percentage_error(y_train, train_predictions)\n",
    "    test_r2 = r2_score(y_test, test_predictions)\n",
    "    test_mape = mean_absolute_percentage_error(y_test, test_predictions)\n",
    "    \n",
    "    \n",
    "    print(f\"Training R²: {train_r2}\")\n",
    "    print(f\"Testing R²: {test_r2}\")\n",
    "    print(f\"Training Mape: {train_mape}\")\n",
    "    print(f\"Testing Mape: {test_mape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f469f5-6b2c-4abc-9e4e-b11967a1b731",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

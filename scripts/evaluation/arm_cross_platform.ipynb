{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93e518bb-30f5-41ce-afb6-2ef8e8d90ad2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_size: 1000000000\n",
      "Training R²: 0.7533227482609632\n",
      "Testing R²: 0.7242447776604461\n",
      "Training Mape: 0.12840343356897382\n",
      "Testing Mape: 0.13617713282189672\n",
      "chunk_size: 2000000000\n",
      "Training R²: 0.8252557268611814\n",
      "Testing R²: 0.8095724195358404\n",
      "Training Mape: 0.09877155831189298\n",
      "Testing Mape: 0.10628183676494485\n",
      "chunk_size: 3000000000\n",
      "Training R²: 0.8823023641368511\n",
      "Testing R²: 0.8568119820578944\n",
      "Training Mape: 0.07654288500558365\n",
      "Testing Mape: 0.08110601996105317\n",
      "chunk_size: 4000000000\n",
      "Training R²: 0.8975520179746324\n",
      "Testing R²: 0.8800112370079545\n",
      "Training Mape: 0.06991489881555085\n",
      "Testing Mape: 0.07383344574194714\n",
      "chunk_size: 5000000000\n",
      "Training R²: 0.9174818388537153\n",
      "Testing R²: 0.8821351920877867\n",
      "Training Mape: 0.061673152853304554\n",
      "Testing Mape: 0.07389000230555437\n",
      "chunk_size: 10000000000\n",
      "Training R²: 0.9554409641385191\n",
      "Testing R²: 0.9235300306123675\n",
      "Training Mape: 0.043799851855081874\n",
      "Testing Mape: 0.052739482219395645\n",
      "chunk_size: 20000000000\n",
      "Training R²: 0.9778157303280457\n",
      "Testing R²: 0.9369782645742039\n",
      "Training Mape: 0.03204747044599781\n",
      "Testing Mape: 0.04177421425704161\n",
      "chunk_size: 25000000000\n",
      "Training R²: 0.9809817465749389\n",
      "Testing R²: 0.8989535455911418\n",
      "Training Mape: 0.02952897412017369\n",
      "Testing Mape: 0.05089298054125931\n"
     ]
    }
   ],
   "source": [
    "# InO to OOO\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "def process_csv_feautures(file_paths, chunk_size):\n",
    "    all_features = []\n",
    "    for file_path in file_paths:\n",
    "        df = pd.read_csv(file_path + '_' + str(chunk_size))\n",
    "        df = df.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "        features = []\n",
    "        for index, row in df.iterrows():\n",
    "            inst = row['instructions:u']\n",
    "            cyc = row['cpu-cycles:u']\n",
    "            ipc = inst/cyc\n",
    "            features.append([\n",
    "                row['instructions:u'],\n",
    "                row['cpu-cycles:u'],\n",
    "                row['armv8_pmuv3/stall_backend/u'],\n",
    "                row['armv8_pmuv3/stall_frontend/u'],\n",
    "                row['armv8_pmuv3/br_mis_pred/u'],\n",
    "                row['armv8_pmuv3/br_pred/u'],\n",
    "                row['armv8_pmuv3/br_retired/u'],\n",
    "                row['armv8_pmuv3/inst_spec/u'],\n",
    "                row['armv8_pmuv3/inst_retired/u'],\n",
    "                row['armv8_pmuv3/l1d_cache/u'],\n",
    "                row['armv8_pmuv3/l1d_cache_refill/u'],\n",
    "                row['armv8_pmuv3/l1d_cache_wb/u'],\n",
    "                row['armv8_pmuv3/l1d_tlb/u'],\n",
    "                row['armv8_pmuv3/l1d_tlb_refill/u'],\n",
    "                row['armv8_pmuv3/l1i_cache/u'],\n",
    "                row['armv8_pmuv3/l1i_cache_refill/u'],\n",
    "                row['armv8_pmuv3/l1i_tlb/u'],\n",
    "                row['armv8_pmuv3/l1i_tlb_refill/u'],\n",
    "                row['armv8_pmuv3/l2d_cache/u'],\n",
    "                row['armv8_pmuv3/l2d_cache_allocate/u'],\n",
    "                row['armv8_pmuv3/l2d_cache_refill/u'],\n",
    "                row['armv8_pmuv3/l2d_cache_wb/u'],\n",
    "                row['armv8_pmuv3/l2d_tlb/u'],\n",
    "                row['armv8_pmuv3/l2d_tlb_refill/u'],\n",
    "                row['armv8_pmuv3/l3d_cache/u'],\n",
    "                row['armv8_pmuv3/l3d_cache_allocate/u'],\n",
    "                row['armv8_pmuv3/l3d_cache_refill/u'],\n",
    "                row['armv8_pmuv3/l3d_cache_wb/u'],\n",
    "                row['armv8_pmuv3/mem_access/u'],\n",
    "                ipc])\n",
    "        all_features.extend(features)\n",
    "    return all_features\n",
    "\n",
    "\n",
    "def process_csv_ipc(file_paths, chunk_size):\n",
    "    all_ipc_values = []\n",
    "    for file_path in file_paths:\n",
    "        df = pd.read_csv(file_path + '_' + str(chunk_size))\n",
    "        df = df.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "        for _, row in df.iterrows():\n",
    "            all_ipc_values.append(row['instructions:u'] / row['cpu-cycles:u'])  \n",
    "    return all_ipc_values\n",
    "\n",
    "source = [\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_500',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_502',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_505',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_520',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_523',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_525',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_531',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_541',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_548',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_557',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_503',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_507',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_508',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_510',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_511',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_519',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_521',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_527',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_538',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_544',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_549',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_554',\n",
    "]\n",
    "    \n",
    "target = [\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_500',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_502',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_505',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_520',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_523',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_525',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_531',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_541',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_548',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_557',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_503',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_507',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_508',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_510',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_511',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_519',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_521',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_527',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_538',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_544',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_549',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_554',\n",
    "]\n",
    "\n",
    "chunks = [1000000000,2000000000,3000000000,4000000000,5000000000,10000000000,20000000000,25000000000]\n",
    "for chunk_size in chunks:\n",
    "    in_order_hardware_counters_features = process_csv_feautures(source, chunk_size)\n",
    "    out_of_order_ipc = process_csv_ipc(target, chunk_size)\n",
    "    print('chunk_size: ' + str(chunk_size))\n",
    "    \n",
    "    data = {\n",
    "        'In Order': in_order_hardware_counters_features,\n",
    "        'Out of Order': out_of_order_ipc\n",
    "    }\n",
    "    \n",
    "    length = min(len(in_order_hardware_counters_features), len(out_of_order_ipc))\n",
    "    \n",
    "    # Shorten the longen array\n",
    "    in_order_hardware_counters_features = in_order_hardware_counters_features[:length]\n",
    "    out_of_order_ipc = out_of_order_ipc[:length]\n",
    "    \n",
    "    # separate features and target\n",
    "    X = pd.DataFrame(\n",
    "        in_order_hardware_counters_features, \n",
    "        columns=[\n",
    "            'instructions:u',\n",
    "            'cpu-cycles:u',\n",
    "            'armv8_pmuv3/stall_backend/u',\n",
    "            'armv8_pmuv3/stall_frontend/u',\n",
    "            'armv8_pmuv3/br_mis_pred/u',\n",
    "            'armv8_pmuv3/br_pred/u',\n",
    "            'armv8_pmuv3/br_retired/u',\n",
    "            'armv8_pmuv3/inst_spec/u',\n",
    "            'armv8_pmuv3/inst_retired/u',\n",
    "            'armv8_pmuv3/l1d_cache/u',\n",
    "            'armv8_pmuv3/l1d_cache_refill/u',\n",
    "            'armv8_pmuv3/l1d_cache_wb/u',\n",
    "            'armv8_pmuv3/l1d_tlb/u',\n",
    "            'armv8_pmuv3/l1d_tlb_refill/u',\n",
    "            'armv8_pmuv3/l1i_cache/u',\n",
    "            'armv8_pmuv3/l1i_cache_refill/u',\n",
    "            'armv8_pmuv3/l1i_tlb/u',\n",
    "            'armv8_pmuv3/l1i_tlb_refill/u',\n",
    "            'armv8_pmuv3/l2d_cache/u',\n",
    "            'armv8_pmuv3/l2d_cache_allocate/u',\n",
    "            'armv8_pmuv3/l2d_cache_refill/u',\n",
    "            'armv8_pmuv3/l2d_cache_wb/u',\n",
    "            'armv8_pmuv3/l2d_tlb/u',\n",
    "            'armv8_pmuv3/l2d_tlb_refill/u',\n",
    "            'armv8_pmuv3/l3d_cache/u',\n",
    "            'armv8_pmuv3/l3d_cache_allocate/u',\n",
    "            'armv8_pmuv3/l3d_cache_refill/u',\n",
    "            'armv8_pmuv3/l3d_cache_wb/u',\n",
    "            'armv8_pmuv3/mem_access/u',\n",
    "            'ipc'])  # Features\n",
    "    y = pd.Series(out_of_order_ipc) \n",
    "    \n",
    "    # split \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    rf_model = RandomForestRegressor(n_estimators=100)\n",
    "    gb_model = GradientBoostingRegressor(n_estimators=100)\n",
    "    ensemble_model = gb_model\n",
    "    #ensemble_model = VotingRegressor([('rf', rf_model), ('gb', gb_model)])\n",
    "    ensemble_model.fit(X_train, y_train)\n",
    "    \n",
    "    ensemble_predictions = ensemble_model.predict(X_test)\n",
    "    # Calculate R²\n",
    "    ensemble_r2 = r2_score(y_test, ensemble_predictions)\n",
    "    \n",
    "    #bruh wut\n",
    "    train_predictions = ensemble_model.predict(X_train)\n",
    "    test_predictions = ensemble_model.predict(X_test)\n",
    "    \n",
    "    # Calculate R² for training and testing sets\n",
    "    train_r2 = r2_score(y_train, train_predictions)\n",
    "    train_mape = mean_absolute_percentage_error(y_train, train_predictions)\n",
    "    test_r2 = r2_score(y_test, test_predictions)\n",
    "    test_mape = mean_absolute_percentage_error(y_test, test_predictions)\n",
    "    \n",
    "    \n",
    "    print(f\"Training R²: {train_r2}\")\n",
    "    print(f\"Testing R²: {test_r2}\")\n",
    "    print(f\"Training Mape: {train_mape}\")\n",
    "    print(f\"Testing Mape: {test_mape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00f469f5-6b2c-4abc-9e4e-b11967a1b731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_size: 1000000000\n",
      "Training R²: 0.6910936776497234\n",
      "Testing R²: 0.676544032437079\n",
      "Training Mape: 0.18409765343390178\n",
      "Testing Mape: 0.18827471751145444\n",
      "chunk_size: 2000000000\n",
      "Training R²: 0.7334777551503983\n",
      "Testing R²: 0.7116491806041958\n",
      "Training Mape: 0.16956938055464565\n",
      "Testing Mape: 0.17574670110841292\n",
      "chunk_size: 3000000000\n",
      "Training R²: 0.7691899196414684\n",
      "Testing R²: 0.7253385632903826\n",
      "Training Mape: 0.15693529380391408\n",
      "Testing Mape: 0.16579425716497895\n",
      "chunk_size: 4000000000\n",
      "Training R²: 0.7911504404260685\n",
      "Testing R²: 0.7373325624273959\n",
      "Training Mape: 0.1496092085845062\n",
      "Testing Mape: 0.15852020278239326\n",
      "chunk_size: 5000000000\n",
      "Training R²: 0.8104705144392689\n",
      "Testing R²: 0.7423268527749864\n",
      "Training Mape: 0.14310593532922053\n",
      "Testing Mape: 0.15626885326538387\n",
      "chunk_size: 10000000000\n",
      "Training R²: 0.8552418596469199\n",
      "Testing R²: 0.7412774828287119\n",
      "Training Mape: 0.12337849397383734\n",
      "Testing Mape: 0.1524415356366756\n",
      "chunk_size: 20000000000\n",
      "Training R²: 0.9066239559301295\n",
      "Testing R²: 0.7538846788101959\n",
      "Training Mape: 0.10456723096463597\n",
      "Testing Mape: 0.1525651047045593\n",
      "chunk_size: 25000000000\n",
      "Training R²: 0.9030995583888826\n",
      "Testing R²: 0.7602415956007161\n",
      "Training Mape: 0.10517175600460585\n",
      "Testing Mape: 0.16092605716996347\n"
     ]
    }
   ],
   "source": [
    "# InO to desktop\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "def process_csv_feautures(file_paths, chunk_size):\n",
    "    all_features = []\n",
    "    for file_path in file_paths:\n",
    "        df = pd.read_csv(file_path + '_' + str(chunk_size))\n",
    "        df = df.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "        features = []\n",
    "        for index, row in df.iterrows():\n",
    "            inst = row['instructions:u']\n",
    "            cyc = row['cpu-cycles:u']\n",
    "            ipc = inst/cyc\n",
    "            features.append([\n",
    "                row['instructions:u'],\n",
    "                row['cpu-cycles:u'],\n",
    "                row['armv8_pmuv3/stall_backend/u'],\n",
    "                row['armv8_pmuv3/stall_frontend/u'],\n",
    "                row['armv8_pmuv3/br_mis_pred/u'],\n",
    "                row['armv8_pmuv3/br_pred/u'],\n",
    "                row['armv8_pmuv3/br_retired/u'],\n",
    "                row['armv8_pmuv3/inst_spec/u'],\n",
    "                row['armv8_pmuv3/inst_retired/u'],\n",
    "                row['armv8_pmuv3/l1d_cache/u'],\n",
    "                row['armv8_pmuv3/l1d_cache_refill/u'],\n",
    "                row['armv8_pmuv3/l1d_cache_wb/u'],\n",
    "                row['armv8_pmuv3/l1d_tlb/u'],\n",
    "                row['armv8_pmuv3/l1d_tlb_refill/u'],\n",
    "                row['armv8_pmuv3/l1i_cache/u'],\n",
    "                row['armv8_pmuv3/l1i_cache_refill/u'],\n",
    "                row['armv8_pmuv3/l1i_tlb/u'],\n",
    "                row['armv8_pmuv3/l1i_tlb_refill/u'],\n",
    "                row['armv8_pmuv3/l2d_cache/u'],\n",
    "                row['armv8_pmuv3/l2d_cache_allocate/u'],\n",
    "                row['armv8_pmuv3/l2d_cache_refill/u'],\n",
    "                row['armv8_pmuv3/l2d_cache_wb/u'],\n",
    "                row['armv8_pmuv3/l2d_tlb/u'],\n",
    "                row['armv8_pmuv3/l2d_tlb_refill/u'],\n",
    "                row['armv8_pmuv3/l3d_cache/u'],\n",
    "                row['armv8_pmuv3/l3d_cache_allocate/u'],\n",
    "                row['armv8_pmuv3/l3d_cache_refill/u'],\n",
    "                row['armv8_pmuv3/l3d_cache_wb/u'],\n",
    "                row['armv8_pmuv3/mem_access/u'],\n",
    "                ipc])\n",
    "        all_features.extend(features)\n",
    "    return all_features\n",
    "\n",
    "\n",
    "def process_csv_ipc(file_paths, chunk_size):\n",
    "    all_ipc_values = []\n",
    "    for file_path in file_paths:\n",
    "        df = pd.read_csv(file_path + '_' + str(chunk_size))\n",
    "        df = df.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "        for _, row in df.iterrows():\n",
    "            all_ipc_values.append(row['instructions'] / row['cpu-cycles'])  \n",
    "    return all_ipc_values\n",
    "\n",
    "source = [\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_500',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_502',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_505',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_520',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_523',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_525',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_531',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_541',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_548',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_557',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_503',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_507',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_508',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_510',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_511',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_519',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_521',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_527',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_538',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_544',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_549',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_554',\n",
    "]\n",
    "    \n",
    "target = [\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_500',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_502',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_505',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_520',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_523',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_525',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_531',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_541',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_548',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_557',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_503',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_507',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_508',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_510',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_511',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_519',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_521',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_527',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_538',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_544',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_549',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_554',\n",
    "]\n",
    "\n",
    "chunks = [1000000000,2000000000,3000000000,4000000000,5000000000,10000000000,20000000000,25000000000]\n",
    "for chunk_size in chunks:\n",
    "    in_order_hardware_counters_features = process_csv_feautures(source, chunk_size)\n",
    "    out_of_order_ipc = process_csv_ipc(target, chunk_size)\n",
    "    print('chunk_size: ' + str(chunk_size))\n",
    "    \n",
    "    data = {\n",
    "        'In Order': in_order_hardware_counters_features,\n",
    "        'Out of Order': out_of_order_ipc\n",
    "    }\n",
    "    \n",
    "    length = min(len(in_order_hardware_counters_features), len(out_of_order_ipc))\n",
    "    \n",
    "    # Shorten the longen array\n",
    "    in_order_hardware_counters_features = in_order_hardware_counters_features[:length]\n",
    "    out_of_order_ipc = out_of_order_ipc[:length]\n",
    "    \n",
    "    # separate features and target\n",
    "    X = pd.DataFrame(\n",
    "        in_order_hardware_counters_features, \n",
    "        columns=[\n",
    "            'instructions:u',\n",
    "            'cpu-cycles:u',\n",
    "            'armv8_pmuv3/stall_backend/u',\n",
    "            'armv8_pmuv3/stall_frontend/u',\n",
    "            'armv8_pmuv3/br_mis_pred/u',\n",
    "            'armv8_pmuv3/br_pred/u',\n",
    "            'armv8_pmuv3/br_retired/u',\n",
    "            'armv8_pmuv3/inst_spec/u',\n",
    "            'armv8_pmuv3/inst_retired/u',\n",
    "            'armv8_pmuv3/l1d_cache/u',\n",
    "            'armv8_pmuv3/l1d_cache_refill/u',\n",
    "            'armv8_pmuv3/l1d_cache_wb/u',\n",
    "            'armv8_pmuv3/l1d_tlb/u',\n",
    "            'armv8_pmuv3/l1d_tlb_refill/u',\n",
    "            'armv8_pmuv3/l1i_cache/u',\n",
    "            'armv8_pmuv3/l1i_cache_refill/u',\n",
    "            'armv8_pmuv3/l1i_tlb/u',\n",
    "            'armv8_pmuv3/l1i_tlb_refill/u',\n",
    "            'armv8_pmuv3/l2d_cache/u',\n",
    "            'armv8_pmuv3/l2d_cache_allocate/u',\n",
    "            'armv8_pmuv3/l2d_cache_refill/u',\n",
    "            'armv8_pmuv3/l2d_cache_wb/u',\n",
    "            'armv8_pmuv3/l2d_tlb/u',\n",
    "            'armv8_pmuv3/l2d_tlb_refill/u',\n",
    "            'armv8_pmuv3/l3d_cache/u',\n",
    "            'armv8_pmuv3/l3d_cache_allocate/u',\n",
    "            'armv8_pmuv3/l3d_cache_refill/u',\n",
    "            'armv8_pmuv3/l3d_cache_wb/u',\n",
    "            'armv8_pmuv3/mem_access/u',\n",
    "            'ipc'])  # Features\n",
    "    y = pd.Series(out_of_order_ipc) \n",
    "    \n",
    "    # split \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    rf_model = RandomForestRegressor(n_estimators=100)\n",
    "    gb_model = GradientBoostingRegressor(n_estimators=100)\n",
    "    ensemble_model = gb_model\n",
    "    #ensemble_model = VotingRegressor([('rf', rf_model), ('gb', gb_model)])\n",
    "    ensemble_model.fit(X_train, y_train)\n",
    "    \n",
    "    ensemble_predictions = ensemble_model.predict(X_test)\n",
    "    # Calculate R²\n",
    "    ensemble_r2 = r2_score(y_test, ensemble_predictions)\n",
    "    \n",
    "    #bruh wut\n",
    "    train_predictions = ensemble_model.predict(X_train)\n",
    "    test_predictions = ensemble_model.predict(X_test)\n",
    "    \n",
    "    # Calculate R² for training and testing sets\n",
    "    train_r2 = r2_score(y_train, train_predictions)\n",
    "    train_mape = mean_absolute_percentage_error(y_train, train_predictions)\n",
    "    test_r2 = r2_score(y_test, test_predictions)\n",
    "    test_mape = mean_absolute_percentage_error(y_test, test_predictions)\n",
    "    \n",
    "    \n",
    "    print(f\"Training R²: {train_r2}\")\n",
    "    print(f\"Testing R²: {test_r2}\")\n",
    "    print(f\"Training Mape: {train_mape}\")\n",
    "    print(f\"Testing Mape: {test_mape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cf61275-6147-4e9f-819b-2b49da8d4f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_size: 1000000000\n",
      "Training R²: 0.718348189899501\n",
      "Testing R²: 0.6952376476099033\n",
      "Training Mape: 0.12606824581706225\n",
      "Testing Mape: 0.13188237630504226\n",
      "chunk_size: 2000000000\n",
      "Training R²: 0.7357130924170849\n",
      "Testing R²: 0.6806544568376339\n",
      "Training Mape: 0.1229320566653013\n",
      "Testing Mape: 0.13156668151536197\n",
      "chunk_size: 3000000000\n",
      "Training R²: 0.7361964015335656\n",
      "Testing R²: 0.6778301537053854\n",
      "Training Mape: 0.12182921244344638\n",
      "Testing Mape: 0.13986451496236743\n",
      "chunk_size: 4000000000\n",
      "Training R²: 0.7390141743447367\n",
      "Testing R²: 0.6901461316561361\n",
      "Training Mape: 0.12234282914609654\n",
      "Testing Mape: 0.12995282897021443\n",
      "chunk_size: 5000000000\n",
      "Training R²: 0.7363375137614889\n",
      "Testing R²: 0.6773145948932816\n",
      "Training Mape: 0.12431023543714013\n",
      "Testing Mape: 0.1377892985960329\n",
      "chunk_size: 10000000000\n",
      "Training R²: 0.7784855385652182\n",
      "Testing R²: 0.6868007364548734\n",
      "Training Mape: 0.1105535353260399\n",
      "Testing Mape: 0.1424460180321974\n",
      "chunk_size: 20000000000\n",
      "Training R²: 0.8519210514286037\n",
      "Testing R²: 0.6412983653690292\n",
      "Training Mape: 0.09484103753372335\n",
      "Testing Mape: 0.11806510684322231\n",
      "chunk_size: 25000000000\n",
      "Training R²: 0.8714599494682606\n",
      "Testing R²: 0.6669226206617316\n",
      "Training Mape: 0.08525427840449841\n",
      "Testing Mape: 0.13020269081974037\n"
     ]
    }
   ],
   "source": [
    "# InO to server\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "def process_csv_feautures(file_paths, chunk_size):\n",
    "    all_features = []\n",
    "    for file_path in file_paths:\n",
    "        df = pd.read_csv(file_path + '_' + str(chunk_size))\n",
    "        df = df.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "        features = []\n",
    "        for index, row in df.iterrows():\n",
    "            inst = row['instructions:u']\n",
    "            cyc = row['cpu-cycles:u']\n",
    "            ipc = inst/cyc\n",
    "            features.append([\n",
    "                row['instructions:u'],\n",
    "                row['cpu-cycles:u'],\n",
    "                row['armv8_pmuv3/stall_backend/u'],\n",
    "                row['armv8_pmuv3/stall_frontend/u'],\n",
    "                row['armv8_pmuv3/br_mis_pred/u'],\n",
    "                row['armv8_pmuv3/br_pred/u'],\n",
    "                row['armv8_pmuv3/br_retired/u'],\n",
    "                row['armv8_pmuv3/inst_spec/u'],\n",
    "                row['armv8_pmuv3/inst_retired/u'],\n",
    "                row['armv8_pmuv3/l1d_cache/u'],\n",
    "                row['armv8_pmuv3/l1d_cache_refill/u'],\n",
    "                row['armv8_pmuv3/l1d_cache_wb/u'],\n",
    "                row['armv8_pmuv3/l1d_tlb/u'],\n",
    "                row['armv8_pmuv3/l1d_tlb_refill/u'],\n",
    "                row['armv8_pmuv3/l1i_cache/u'],\n",
    "                row['armv8_pmuv3/l1i_cache_refill/u'],\n",
    "                row['armv8_pmuv3/l1i_tlb/u'],\n",
    "                row['armv8_pmuv3/l1i_tlb_refill/u'],\n",
    "                row['armv8_pmuv3/l2d_cache/u'],\n",
    "                row['armv8_pmuv3/l2d_cache_allocate/u'],\n",
    "                row['armv8_pmuv3/l2d_cache_refill/u'],\n",
    "                row['armv8_pmuv3/l2d_cache_wb/u'],\n",
    "                row['armv8_pmuv3/l2d_tlb/u'],\n",
    "                row['armv8_pmuv3/l2d_tlb_refill/u'],\n",
    "                row['armv8_pmuv3/l3d_cache/u'],\n",
    "                row['armv8_pmuv3/l3d_cache_allocate/u'],\n",
    "                row['armv8_pmuv3/l3d_cache_refill/u'],\n",
    "                row['armv8_pmuv3/l3d_cache_wb/u'],\n",
    "                row['armv8_pmuv3/mem_access/u'],\n",
    "                ipc])\n",
    "        all_features.extend(features)\n",
    "    return all_features\n",
    "\n",
    "\n",
    "def process_csv_ipc(file_paths, chunk_size):\n",
    "    all_ipc_values = []\n",
    "    for file_path in file_paths:\n",
    "        df = pd.read_csv(file_path + '_' + str(chunk_size))\n",
    "        df = df.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "        for _, row in df.iterrows():\n",
    "            all_ipc_values.append(row['instructions'] / row['cpu-cycles'])  \n",
    "    return all_ipc_values\n",
    "\n",
    "source = [\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_500',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_502',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_505',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_520',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_523',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_525',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_531',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_541',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_548',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_557',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_503',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_507',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_508',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_510',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_511',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_519',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_521',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_527',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_538',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_544',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_549',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_554',\n",
    "]\n",
    "    \n",
    "target = [\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_500',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_502',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_505',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_520',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_523',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_525',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_531',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_541',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_548',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_557',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_503',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_507',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_508',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_510',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_511',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_519',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_521',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_527',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_538',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_544',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_549',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_554',\n",
    "]\n",
    "\n",
    "chunks = [1000000000,2000000000,3000000000,4000000000,5000000000,10000000000,20000000000,25000000000]\n",
    "for chunk_size in chunks:\n",
    "    in_order_hardware_counters_features = process_csv_feautures(source, chunk_size)\n",
    "    out_of_order_ipc = process_csv_ipc(target, chunk_size)\n",
    "    print('chunk_size: ' + str(chunk_size))\n",
    "    \n",
    "    data = {\n",
    "        'In Order': in_order_hardware_counters_features,\n",
    "        'Out of Order': out_of_order_ipc\n",
    "    }\n",
    "    \n",
    "    length = min(len(in_order_hardware_counters_features), len(out_of_order_ipc))\n",
    "    \n",
    "    # Shorten the longen array\n",
    "    in_order_hardware_counters_features = in_order_hardware_counters_features[:length]\n",
    "    out_of_order_ipc = out_of_order_ipc[:length]\n",
    "    \n",
    "    # separate features and target\n",
    "    X = pd.DataFrame(\n",
    "        in_order_hardware_counters_features, \n",
    "        columns=[\n",
    "            'instructions:u',\n",
    "            'cpu-cycles:u',\n",
    "            'armv8_pmuv3/stall_backend/u',\n",
    "            'armv8_pmuv3/stall_frontend/u',\n",
    "            'armv8_pmuv3/br_mis_pred/u',\n",
    "            'armv8_pmuv3/br_pred/u',\n",
    "            'armv8_pmuv3/br_retired/u',\n",
    "            'armv8_pmuv3/inst_spec/u',\n",
    "            'armv8_pmuv3/inst_retired/u',\n",
    "            'armv8_pmuv3/l1d_cache/u',\n",
    "            'armv8_pmuv3/l1d_cache_refill/u',\n",
    "            'armv8_pmuv3/l1d_cache_wb/u',\n",
    "            'armv8_pmuv3/l1d_tlb/u',\n",
    "            'armv8_pmuv3/l1d_tlb_refill/u',\n",
    "            'armv8_pmuv3/l1i_cache/u',\n",
    "            'armv8_pmuv3/l1i_cache_refill/u',\n",
    "            'armv8_pmuv3/l1i_tlb/u',\n",
    "            'armv8_pmuv3/l1i_tlb_refill/u',\n",
    "            'armv8_pmuv3/l2d_cache/u',\n",
    "            'armv8_pmuv3/l2d_cache_allocate/u',\n",
    "            'armv8_pmuv3/l2d_cache_refill/u',\n",
    "            'armv8_pmuv3/l2d_cache_wb/u',\n",
    "            'armv8_pmuv3/l2d_tlb/u',\n",
    "            'armv8_pmuv3/l2d_tlb_refill/u',\n",
    "            'armv8_pmuv3/l3d_cache/u',\n",
    "            'armv8_pmuv3/l3d_cache_allocate/u',\n",
    "            'armv8_pmuv3/l3d_cache_refill/u',\n",
    "            'armv8_pmuv3/l3d_cache_wb/u',\n",
    "            'armv8_pmuv3/mem_access/u',\n",
    "            'ipc'])  # Features\n",
    "    y = pd.Series(out_of_order_ipc) \n",
    "    \n",
    "    # split \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    rf_model = RandomForestRegressor(n_estimators=100)\n",
    "    gb_model = GradientBoostingRegressor(n_estimators=100)\n",
    "    ensemble_model = gb_model\n",
    "    #ensemble_model = VotingRegressor([('rf', rf_model), ('gb', gb_model)])\n",
    "    ensemble_model.fit(X_train, y_train)\n",
    "    \n",
    "    ensemble_predictions = ensemble_model.predict(X_test)\n",
    "    # Calculate R²\n",
    "    ensemble_r2 = r2_score(y_test, ensemble_predictions)\n",
    "    \n",
    "    #bruh wut\n",
    "    train_predictions = ensemble_model.predict(X_train)\n",
    "    test_predictions = ensemble_model.predict(X_test)\n",
    "    \n",
    "    # Calculate R² for training and testing sets\n",
    "    train_r2 = r2_score(y_train, train_predictions)\n",
    "    train_mape = mean_absolute_percentage_error(y_train, train_predictions)\n",
    "    test_r2 = r2_score(y_test, test_predictions)\n",
    "    test_mape = mean_absolute_percentage_error(y_test, test_predictions)\n",
    "    \n",
    "    \n",
    "    print(f\"Training R²: {train_r2}\")\n",
    "    print(f\"Testing R²: {test_r2}\")\n",
    "    print(f\"Training Mape: {train_mape}\")\n",
    "    print(f\"Testing Mape: {test_mape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7568794-df2e-4edf-9fe3-074ff0c5bf84",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_size: 1000000000\n",
      "Training R²: 0.8006280152860468\n",
      "Testing R²: 0.7861053859888825\n",
      "Training Mape: 0.19220372172185204\n",
      "Testing Mape: 0.20181693872517956\n",
      "chunk_size: 2000000000\n",
      "Training R²: 0.8535109133588694\n",
      "Testing R²: 0.8448451249776255\n",
      "Training Mape: 0.15115970345113244\n",
      "Testing Mape: 0.1547619079385732\n",
      "chunk_size: 3000000000\n",
      "Training R²: 0.8880825728773998\n",
      "Testing R²: 0.8552866822524878\n",
      "Training Mape: 0.12230895660980581\n",
      "Testing Mape: 0.13276253904181154\n",
      "chunk_size: 4000000000\n",
      "Training R²: 0.8980063422679573\n",
      "Testing R²: 0.8819279820671895\n",
      "Training Mape: 0.1096437852427383\n",
      "Testing Mape: 0.1188755959967791\n",
      "chunk_size: 5000000000\n",
      "Training R²: 0.9135959978131359\n",
      "Testing R²: 0.8719190949672951\n",
      "Training Mape: 0.09604390456621083\n",
      "Testing Mape: 0.11719668842036472\n",
      "chunk_size: 10000000000\n",
      "Training R²: 0.9459728554942582\n",
      "Testing R²: 0.9006623760963973\n",
      "Training Mape: 0.07689046527230946\n",
      "Testing Mape: 0.09851635437968805\n",
      "chunk_size: 20000000000\n",
      "Training R²: 0.9747531219605597\n",
      "Testing R²: 0.9271674643483876\n",
      "Training Mape: 0.0511845447580029\n",
      "Testing Mape: 0.07672237250705896\n",
      "chunk_size: 25000000000\n",
      "Training R²: 0.9771080069150087\n",
      "Testing R²: 0.8997606360039259\n",
      "Training Mape: 0.04668823777962018\n",
      "Testing Mape: 0.08808814812492531\n"
     ]
    }
   ],
   "source": [
    "# OOO to InO\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "def process_csv_feautures(file_paths, chunk_size):\n",
    "    all_features = []\n",
    "    for file_path in file_paths:\n",
    "        df = pd.read_csv(file_path + '_' + str(chunk_size))\n",
    "        df = df.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "        features = []\n",
    "        for index, row in df.iterrows():\n",
    "            inst = row['instructions:u']\n",
    "            cyc = row['cpu-cycles:u']\n",
    "            ipc = inst/cyc\n",
    "            features.append([\n",
    "                row['instructions:u'],\n",
    "                row['cpu-cycles:u'],\n",
    "                row['armv8_pmuv3/stall_backend/u'],\n",
    "                row['armv8_pmuv3/stall_frontend/u'],\n",
    "                row['armv8_pmuv3/br_mis_pred/u'],\n",
    "                row['armv8_pmuv3/br_pred/u'],\n",
    "                row['armv8_pmuv3/br_retired/u'],\n",
    "                row['armv8_pmuv3/inst_spec/u'],\n",
    "                row['armv8_pmuv3/inst_retired/u'],\n",
    "                row['armv8_pmuv3/l1d_cache/u'],\n",
    "                row['armv8_pmuv3/l1d_cache_refill/u'],\n",
    "                row['armv8_pmuv3/l1d_cache_wb/u'],\n",
    "                row['armv8_pmuv3/l1d_tlb/u'],\n",
    "                row['armv8_pmuv3/l1d_tlb_refill/u'],\n",
    "                row['armv8_pmuv3/l1i_cache/u'],\n",
    "                row['armv8_pmuv3/l1i_cache_refill/u'],\n",
    "                row['armv8_pmuv3/l1i_tlb/u'],\n",
    "                row['armv8_pmuv3/l1i_tlb_refill/u'],\n",
    "                row['armv8_pmuv3/l2d_cache/u'],\n",
    "                row['armv8_pmuv3/l2d_cache_allocate/u'],\n",
    "                row['armv8_pmuv3/l2d_cache_refill/u'],\n",
    "                row['armv8_pmuv3/l2d_cache_wb/u'],\n",
    "                row['armv8_pmuv3/l2d_tlb/u'],\n",
    "                row['armv8_pmuv3/l2d_tlb_refill/u'],\n",
    "                row['armv8_pmuv3/l3d_cache/u'],\n",
    "                row['armv8_pmuv3/l3d_cache_allocate/u'],\n",
    "                row['armv8_pmuv3/l3d_cache_refill/u'],\n",
    "                row['armv8_pmuv3/l3d_cache_wb/u'],\n",
    "                row['armv8_pmuv3/mem_access/u'],\n",
    "                ipc])\n",
    "        all_features.extend(features)\n",
    "    return all_features\n",
    "\n",
    "\n",
    "def process_csv_ipc(file_paths, chunk_size):\n",
    "    all_ipc_values = []\n",
    "    for file_path in file_paths:\n",
    "        df = pd.read_csv(file_path + '_' + str(chunk_size))\n",
    "        df = df.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "        for _, row in df.iterrows():\n",
    "            all_ipc_values.append(row['instructions:u'] / row['cpu-cycles:u'])  \n",
    "    return all_ipc_values\n",
    "\n",
    "source = [\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_500',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_502',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_505',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_520',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_523',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_525',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_531',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_541',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_548',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_557',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_503',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_507',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_508',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_510',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_511',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_519',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_521',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_527',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_538',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_544',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_549',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_554',\n",
    "]\n",
    "    \n",
    "target = [\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_500',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_502',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_505',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_520',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_523',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_525',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_531',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_541',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_548',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_557',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_503',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_507',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_508',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_510',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_511',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_519',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_521',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_527',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_538',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_544',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_549',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_554',\n",
    "]\n",
    "\n",
    "chunks = [1000000000,2000000000,3000000000,4000000000,5000000000,10000000000,20000000000,25000000000]\n",
    "for chunk_size in chunks:\n",
    "    in_order_hardware_counters_features = process_csv_feautures(source, chunk_size)\n",
    "    out_of_order_ipc = process_csv_ipc(target, chunk_size)\n",
    "    print('chunk_size: ' + str(chunk_size))\n",
    "    \n",
    "    data = {\n",
    "        'In Order': in_order_hardware_counters_features,\n",
    "        'Out of Order': out_of_order_ipc\n",
    "    }\n",
    "    \n",
    "    length = min(len(in_order_hardware_counters_features), len(out_of_order_ipc))\n",
    "    \n",
    "    # Shorten the longen array\n",
    "    in_order_hardware_counters_features = in_order_hardware_counters_features[:length]\n",
    "    out_of_order_ipc = out_of_order_ipc[:length]\n",
    "    \n",
    "    # separate features and target\n",
    "    X = pd.DataFrame(\n",
    "        in_order_hardware_counters_features, \n",
    "        columns=[\n",
    "            'instructions:u',\n",
    "            'cpu-cycles:u',\n",
    "            'armv8_pmuv3/stall_backend/u',\n",
    "            'armv8_pmuv3/stall_frontend/u',\n",
    "            'armv8_pmuv3/br_mis_pred/u',\n",
    "            'armv8_pmuv3/br_pred/u',\n",
    "            'armv8_pmuv3/br_retired/u',\n",
    "            'armv8_pmuv3/inst_spec/u',\n",
    "            'armv8_pmuv3/inst_retired/u',\n",
    "            'armv8_pmuv3/l1d_cache/u',\n",
    "            'armv8_pmuv3/l1d_cache_refill/u',\n",
    "            'armv8_pmuv3/l1d_cache_wb/u',\n",
    "            'armv8_pmuv3/l1d_tlb/u',\n",
    "            'armv8_pmuv3/l1d_tlb_refill/u',\n",
    "            'armv8_pmuv3/l1i_cache/u',\n",
    "            'armv8_pmuv3/l1i_cache_refill/u',\n",
    "            'armv8_pmuv3/l1i_tlb/u',\n",
    "            'armv8_pmuv3/l1i_tlb_refill/u',\n",
    "            'armv8_pmuv3/l2d_cache/u',\n",
    "            'armv8_pmuv3/l2d_cache_allocate/u',\n",
    "            'armv8_pmuv3/l2d_cache_refill/u',\n",
    "            'armv8_pmuv3/l2d_cache_wb/u',\n",
    "            'armv8_pmuv3/l2d_tlb/u',\n",
    "            'armv8_pmuv3/l2d_tlb_refill/u',\n",
    "            'armv8_pmuv3/l3d_cache/u',\n",
    "            'armv8_pmuv3/l3d_cache_allocate/u',\n",
    "            'armv8_pmuv3/l3d_cache_refill/u',\n",
    "            'armv8_pmuv3/l3d_cache_wb/u',\n",
    "            'armv8_pmuv3/mem_access/u',\n",
    "            'ipc'])  # Features\n",
    "    y = pd.Series(out_of_order_ipc) \n",
    "    \n",
    "    # split \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    rf_model = RandomForestRegressor(n_estimators=100)\n",
    "    gb_model = GradientBoostingRegressor(n_estimators=100)\n",
    "    ensemble_model = gb_model\n",
    "    #ensemble_model = VotingRegressor([('rf', rf_model), ('gb', gb_model)])\n",
    "    ensemble_model.fit(X_train, y_train)\n",
    "    \n",
    "    ensemble_predictions = ensemble_model.predict(X_test)\n",
    "    # Calculate R²\n",
    "    ensemble_r2 = r2_score(y_test, ensemble_predictions)\n",
    "    \n",
    "    #bruh wut\n",
    "    train_predictions = ensemble_model.predict(X_train)\n",
    "    test_predictions = ensemble_model.predict(X_test)\n",
    "    \n",
    "    # Calculate R² for training and testing sets\n",
    "    train_r2 = r2_score(y_train, train_predictions)\n",
    "    train_mape = mean_absolute_percentage_error(y_train, train_predictions)\n",
    "    test_r2 = r2_score(y_test, test_predictions)\n",
    "    test_mape = mean_absolute_percentage_error(y_test, test_predictions)\n",
    "    \n",
    "    \n",
    "    print(f\"Training R²: {train_r2}\")\n",
    "    print(f\"Testing R²: {test_r2}\")\n",
    "    print(f\"Training Mape: {train_mape}\")\n",
    "    print(f\"Testing Mape: {test_mape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f01428c-6bb4-456a-bfef-8d37b8b7b34c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_size: 1000000000\n",
      "Training R²: 0.7214008833550598\n",
      "Testing R²: 0.7328732928901014\n",
      "Training Mape: 0.17854106878791678\n",
      "Testing Mape: 0.17830975596372572\n",
      "chunk_size: 2000000000\n",
      "Training R²: 0.7701954160503407\n",
      "Testing R²: 0.7437475706780319\n",
      "Training Mape: 0.15844706034257766\n",
      "Testing Mape: 0.16394299009503543\n",
      "chunk_size: 3000000000\n",
      "Training R²: 0.8021068497289406\n",
      "Testing R²: 0.7532001761736778\n",
      "Training Mape: 0.14646108257078322\n",
      "Testing Mape: 0.15759741845277786\n",
      "chunk_size: 4000000000\n",
      "Training R²: 0.8242457775012503\n",
      "Testing R²: 0.7596349908409283\n",
      "Training Mape: 0.1389264012628872\n",
      "Testing Mape: 0.1489718772541412\n",
      "chunk_size: 5000000000\n",
      "Training R²: 0.8357844428213137\n",
      "Testing R²: 0.7998400168332092\n",
      "Training Mape: 0.12870021713516358\n",
      "Testing Mape: 0.13869140341093383\n",
      "chunk_size: 10000000000\n",
      "Training R²: 0.8755763658350714\n",
      "Testing R²: 0.7833353961641173\n",
      "Training Mape: 0.11182008281002652\n",
      "Testing Mape: 0.1410213072499318\n",
      "chunk_size: 20000000000\n",
      "Training R²: 0.9231134877520144\n",
      "Testing R²: 0.7687010499186417\n",
      "Training Mape: 0.09302167743330134\n",
      "Testing Mape: 0.14412279156636573\n",
      "chunk_size: 25000000000\n",
      "Training R²: 0.9134025241278643\n",
      "Testing R²: 0.8025741566704745\n",
      "Training Mape: 0.09340158013656198\n",
      "Testing Mape: 0.14424435422191464\n"
     ]
    }
   ],
   "source": [
    "# OOO to desktop\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "def process_csv_feautures(file_paths, chunk_size):\n",
    "    all_features = []\n",
    "    for file_path in file_paths:\n",
    "        df = pd.read_csv(file_path + '_' + str(chunk_size))\n",
    "        df = df.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "        features = []\n",
    "        for index, row in df.iterrows():\n",
    "            inst = row['instructions:u']\n",
    "            cyc = row['cpu-cycles:u']\n",
    "            ipc = inst/cyc\n",
    "            features.append([\n",
    "                row['instructions:u'],\n",
    "                row['cpu-cycles:u'],\n",
    "                row['armv8_pmuv3/stall_backend/u'],\n",
    "                row['armv8_pmuv3/stall_frontend/u'],\n",
    "                row['armv8_pmuv3/br_mis_pred/u'],\n",
    "                row['armv8_pmuv3/br_pred/u'],\n",
    "                row['armv8_pmuv3/br_retired/u'],\n",
    "                row['armv8_pmuv3/inst_spec/u'],\n",
    "                row['armv8_pmuv3/inst_retired/u'],\n",
    "                row['armv8_pmuv3/l1d_cache/u'],\n",
    "                row['armv8_pmuv3/l1d_cache_refill/u'],\n",
    "                row['armv8_pmuv3/l1d_cache_wb/u'],\n",
    "                row['armv8_pmuv3/l1d_tlb/u'],\n",
    "                row['armv8_pmuv3/l1d_tlb_refill/u'],\n",
    "                row['armv8_pmuv3/l1i_cache/u'],\n",
    "                row['armv8_pmuv3/l1i_cache_refill/u'],\n",
    "                row['armv8_pmuv3/l1i_tlb/u'],\n",
    "                row['armv8_pmuv3/l1i_tlb_refill/u'],\n",
    "                row['armv8_pmuv3/l2d_cache/u'],\n",
    "                row['armv8_pmuv3/l2d_cache_allocate/u'],\n",
    "                row['armv8_pmuv3/l2d_cache_refill/u'],\n",
    "                row['armv8_pmuv3/l2d_cache_wb/u'],\n",
    "                row['armv8_pmuv3/l2d_tlb/u'],\n",
    "                row['armv8_pmuv3/l2d_tlb_refill/u'],\n",
    "                row['armv8_pmuv3/l3d_cache/u'],\n",
    "                row['armv8_pmuv3/l3d_cache_allocate/u'],\n",
    "                row['armv8_pmuv3/l3d_cache_refill/u'],\n",
    "                row['armv8_pmuv3/l3d_cache_wb/u'],\n",
    "                row['armv8_pmuv3/mem_access/u'],\n",
    "                ipc])\n",
    "        all_features.extend(features)\n",
    "    return all_features\n",
    "\n",
    "\n",
    "def process_csv_ipc(file_paths, chunk_size):\n",
    "    all_ipc_values = []\n",
    "    for file_path in file_paths:\n",
    "        df = pd.read_csv(file_path + '_' + str(chunk_size))\n",
    "        df = df.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "        for _, row in df.iterrows():\n",
    "            all_ipc_values.append(row['instructions'] / row['cpu-cycles'])  \n",
    "    return all_ipc_values\n",
    "\n",
    "source = [\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_500',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_502',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_505',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_520',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_523',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_525',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_531',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_541',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_548',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_557',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_503',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_507',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_508',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_510',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_511',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_519',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_521',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_527',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_538',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_544',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_549',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_554',\n",
    "]\n",
    "    \n",
    "target = [\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_500',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_502',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_505',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_520',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_523',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_525',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_531',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_541',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_548',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_557',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_503',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_507',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_508',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_510',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_511',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_519',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_521',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_527',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_538',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_544',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_549',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_554',\n",
    "]\n",
    "chunks = [1000000000,2000000000,3000000000,4000000000,5000000000,10000000000,20000000000,25000000000]\n",
    "for chunk_size in chunks:\n",
    "    in_order_hardware_counters_features = process_csv_feautures(source, chunk_size)\n",
    "    out_of_order_ipc = process_csv_ipc(target, chunk_size)\n",
    "    print('chunk_size: ' + str(chunk_size))\n",
    "    \n",
    "    data = {\n",
    "        'In Order': in_order_hardware_counters_features,\n",
    "        'Out of Order': out_of_order_ipc\n",
    "    }\n",
    "    \n",
    "    length = min(len(in_order_hardware_counters_features), len(out_of_order_ipc))\n",
    "    \n",
    "    # Shorten the longen array\n",
    "    in_order_hardware_counters_features = in_order_hardware_counters_features[:length]\n",
    "    out_of_order_ipc = out_of_order_ipc[:length]\n",
    "    \n",
    "    # separate features and target\n",
    "    X = pd.DataFrame(\n",
    "        in_order_hardware_counters_features, \n",
    "        columns=[\n",
    "            'instructions:u',\n",
    "            'cpu-cycles:u',\n",
    "            'armv8_pmuv3/stall_backend/u',\n",
    "            'armv8_pmuv3/stall_frontend/u',\n",
    "            'armv8_pmuv3/br_mis_pred/u',\n",
    "            'armv8_pmuv3/br_pred/u',\n",
    "            'armv8_pmuv3/br_retired/u',\n",
    "            'armv8_pmuv3/inst_spec/u',\n",
    "            'armv8_pmuv3/inst_retired/u',\n",
    "            'armv8_pmuv3/l1d_cache/u',\n",
    "            'armv8_pmuv3/l1d_cache_refill/u',\n",
    "            'armv8_pmuv3/l1d_cache_wb/u',\n",
    "            'armv8_pmuv3/l1d_tlb/u',\n",
    "            'armv8_pmuv3/l1d_tlb_refill/u',\n",
    "            'armv8_pmuv3/l1i_cache/u',\n",
    "            'armv8_pmuv3/l1i_cache_refill/u',\n",
    "            'armv8_pmuv3/l1i_tlb/u',\n",
    "            'armv8_pmuv3/l1i_tlb_refill/u',\n",
    "            'armv8_pmuv3/l2d_cache/u',\n",
    "            'armv8_pmuv3/l2d_cache_allocate/u',\n",
    "            'armv8_pmuv3/l2d_cache_refill/u',\n",
    "            'armv8_pmuv3/l2d_cache_wb/u',\n",
    "            'armv8_pmuv3/l2d_tlb/u',\n",
    "            'armv8_pmuv3/l2d_tlb_refill/u',\n",
    "            'armv8_pmuv3/l3d_cache/u',\n",
    "            'armv8_pmuv3/l3d_cache_allocate/u',\n",
    "            'armv8_pmuv3/l3d_cache_refill/u',\n",
    "            'armv8_pmuv3/l3d_cache_wb/u',\n",
    "            'armv8_pmuv3/mem_access/u',\n",
    "            'ipc'])  # Features\n",
    "    y = pd.Series(out_of_order_ipc) \n",
    "    \n",
    "    # split \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    rf_model = RandomForestRegressor(n_estimators=100)\n",
    "    gb_model = GradientBoostingRegressor(n_estimators=100)\n",
    "    ensemble_model = gb_model\n",
    "    #ensemble_model = VotingRegressor([('rf', rf_model), ('gb', gb_model)])\n",
    "    ensemble_model.fit(X_train, y_train)\n",
    "    \n",
    "    ensemble_predictions = ensemble_model.predict(X_test)\n",
    "    # Calculate R²\n",
    "    ensemble_r2 = r2_score(y_test, ensemble_predictions)\n",
    "    \n",
    "    #bruh wut\n",
    "    train_predictions = ensemble_model.predict(X_train)\n",
    "    test_predictions = ensemble_model.predict(X_test)\n",
    "    \n",
    "    # Calculate R² for training and testing sets\n",
    "    train_r2 = r2_score(y_train, train_predictions)\n",
    "    train_mape = mean_absolute_percentage_error(y_train, train_predictions)\n",
    "    test_r2 = r2_score(y_test, test_predictions)\n",
    "    test_mape = mean_absolute_percentage_error(y_test, test_predictions)\n",
    "    \n",
    "    \n",
    "    print(f\"Training R²: {train_r2}\")\n",
    "    print(f\"Testing R²: {test_r2}\")\n",
    "    print(f\"Training Mape: {train_mape}\")\n",
    "    print(f\"Testing Mape: {test_mape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06a70f2c-cf59-42c3-b5a5-4314d11da410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_size: 1000000000\n",
      "Training R²: 0.6955937142674178\n",
      "Testing R²: 0.6718011023200388\n",
      "Training Mape: 0.13741153325712407\n",
      "Testing Mape: 0.14357817964034783\n",
      "chunk_size: 2000000000\n",
      "Training R²: 0.7697282953422072\n",
      "Testing R²: 0.7209491874015428\n",
      "Training Mape: 0.1137195587583317\n",
      "Testing Mape: 0.12128566001722259\n",
      "chunk_size: 3000000000\n",
      "Training R²: 0.7693562229308479\n",
      "Testing R²: 0.7014448850013517\n",
      "Training Mape: 0.1146418944869485\n",
      "Testing Mape: 0.13213681808710911\n",
      "chunk_size: 4000000000\n",
      "Training R²: 0.7696004086013086\n",
      "Testing R²: 0.7182434413499015\n",
      "Training Mape: 0.11526412749772606\n",
      "Testing Mape: 0.12392094508375646\n",
      "chunk_size: 5000000000\n",
      "Training R²: 0.789719711473349\n",
      "Testing R²: 0.7159114257101846\n",
      "Training Mape: 0.10865194665569117\n",
      "Testing Mape: 0.12983627773987552\n",
      "chunk_size: 10000000000\n",
      "Training R²: 0.8126481418715242\n",
      "Testing R²: 0.7172885031113654\n",
      "Training Mape: 0.10085424032092787\n",
      "Testing Mape: 0.13321944684710885\n",
      "chunk_size: 20000000000\n",
      "Training R²: 0.8542135415482579\n",
      "Testing R²: 0.6629147949431679\n",
      "Training Mape: 0.09324398288874226\n",
      "Testing Mape: 0.11851927704069326\n",
      "chunk_size: 25000000000\n",
      "Training R²: 0.8868956833337546\n",
      "Testing R²: 0.6915150970749289\n",
      "Training Mape: 0.0787535289721235\n",
      "Testing Mape: 0.12199234447362142\n"
     ]
    }
   ],
   "source": [
    "# OOO to server\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "def process_csv_feautures(file_paths, chunk_size):\n",
    "    all_features = []\n",
    "    for file_path in file_paths:\n",
    "        df = pd.read_csv(file_path + '_' + str(chunk_size))\n",
    "        df = df.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "        features = []\n",
    "        for index, row in df.iterrows():\n",
    "            inst = row['instructions:u']\n",
    "            cyc = row['cpu-cycles:u']\n",
    "            ipc = inst/cyc\n",
    "            features.append([\n",
    "                row['instructions:u'],\n",
    "                row['cpu-cycles:u'],\n",
    "                row['armv8_pmuv3/stall_backend/u'],\n",
    "                row['armv8_pmuv3/stall_frontend/u'],\n",
    "                row['armv8_pmuv3/br_mis_pred/u'],\n",
    "                row['armv8_pmuv3/br_pred/u'],\n",
    "                row['armv8_pmuv3/br_retired/u'],\n",
    "                row['armv8_pmuv3/inst_spec/u'],\n",
    "                row['armv8_pmuv3/inst_retired/u'],\n",
    "                row['armv8_pmuv3/l1d_cache/u'],\n",
    "                row['armv8_pmuv3/l1d_cache_refill/u'],\n",
    "                row['armv8_pmuv3/l1d_cache_wb/u'],\n",
    "                row['armv8_pmuv3/l1d_tlb/u'],\n",
    "                row['armv8_pmuv3/l1d_tlb_refill/u'],\n",
    "                row['armv8_pmuv3/l1i_cache/u'],\n",
    "                row['armv8_pmuv3/l1i_cache_refill/u'],\n",
    "                row['armv8_pmuv3/l1i_tlb/u'],\n",
    "                row['armv8_pmuv3/l1i_tlb_refill/u'],\n",
    "                row['armv8_pmuv3/l2d_cache/u'],\n",
    "                row['armv8_pmuv3/l2d_cache_allocate/u'],\n",
    "                row['armv8_pmuv3/l2d_cache_refill/u'],\n",
    "                row['armv8_pmuv3/l2d_cache_wb/u'],\n",
    "                row['armv8_pmuv3/l2d_tlb/u'],\n",
    "                row['armv8_pmuv3/l2d_tlb_refill/u'],\n",
    "                row['armv8_pmuv3/l3d_cache/u'],\n",
    "                row['armv8_pmuv3/l3d_cache_allocate/u'],\n",
    "                row['armv8_pmuv3/l3d_cache_refill/u'],\n",
    "                row['armv8_pmuv3/l3d_cache_wb/u'],\n",
    "                row['armv8_pmuv3/mem_access/u'],\n",
    "                ipc])\n",
    "        all_features.extend(features)\n",
    "    return all_features\n",
    "\n",
    "\n",
    "def process_csv_ipc(file_paths, chunk_size):\n",
    "    all_ipc_values = []\n",
    "    for file_path in file_paths:\n",
    "        df = pd.read_csv(file_path + '_' + str(chunk_size))\n",
    "        df = df.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "        for _, row in df.iterrows():\n",
    "            all_ipc_values.append(row['instructions'] / row['cpu-cycles'])  \n",
    "    return all_ipc_values\n",
    "\n",
    "source = [\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_500',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_502',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_505',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_520',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_523',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_525',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_531',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_541',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_548',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_557',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_503',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_507',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_508',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_510',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_511',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_519',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_521',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_527',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_538',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_544',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_549',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_554',\n",
    "]\n",
    "    \n",
    "target = [\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_500',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_502',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_505',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_520',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_523',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_525',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_531',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_541',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_548',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_557',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_503',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_507',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_508',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_510',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_511',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_519',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_521',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_527',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_538',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_544',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_549',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_554',\n",
    "]\n",
    "\n",
    "chunks = [1000000000,2000000000,3000000000,4000000000,5000000000,10000000000,20000000000,25000000000]\n",
    "for chunk_size in chunks:\n",
    "    in_order_hardware_counters_features = process_csv_feautures(source, chunk_size)\n",
    "    out_of_order_ipc = process_csv_ipc(target, chunk_size)\n",
    "    print('chunk_size: ' + str(chunk_size))\n",
    "    \n",
    "    data = {\n",
    "        'In Order': in_order_hardware_counters_features,\n",
    "        'Out of Order': out_of_order_ipc\n",
    "    }\n",
    "    \n",
    "    length = min(len(in_order_hardware_counters_features), len(out_of_order_ipc))\n",
    "    \n",
    "    # Shorten the longen array\n",
    "    in_order_hardware_counters_features = in_order_hardware_counters_features[:length]\n",
    "    out_of_order_ipc = out_of_order_ipc[:length]\n",
    "    \n",
    "    # separate features and target\n",
    "    X = pd.DataFrame(\n",
    "        in_order_hardware_counters_features, \n",
    "        columns=[\n",
    "            'instructions:u',\n",
    "            'cpu-cycles:u',\n",
    "            'armv8_pmuv3/stall_backend/u',\n",
    "            'armv8_pmuv3/stall_frontend/u',\n",
    "            'armv8_pmuv3/br_mis_pred/u',\n",
    "            'armv8_pmuv3/br_pred/u',\n",
    "            'armv8_pmuv3/br_retired/u',\n",
    "            'armv8_pmuv3/inst_spec/u',\n",
    "            'armv8_pmuv3/inst_retired/u',\n",
    "            'armv8_pmuv3/l1d_cache/u',\n",
    "            'armv8_pmuv3/l1d_cache_refill/u',\n",
    "            'armv8_pmuv3/l1d_cache_wb/u',\n",
    "            'armv8_pmuv3/l1d_tlb/u',\n",
    "            'armv8_pmuv3/l1d_tlb_refill/u',\n",
    "            'armv8_pmuv3/l1i_cache/u',\n",
    "            'armv8_pmuv3/l1i_cache_refill/u',\n",
    "            'armv8_pmuv3/l1i_tlb/u',\n",
    "            'armv8_pmuv3/l1i_tlb_refill/u',\n",
    "            'armv8_pmuv3/l2d_cache/u',\n",
    "            'armv8_pmuv3/l2d_cache_allocate/u',\n",
    "            'armv8_pmuv3/l2d_cache_refill/u',\n",
    "            'armv8_pmuv3/l2d_cache_wb/u',\n",
    "            'armv8_pmuv3/l2d_tlb/u',\n",
    "            'armv8_pmuv3/l2d_tlb_refill/u',\n",
    "            'armv8_pmuv3/l3d_cache/u',\n",
    "            'armv8_pmuv3/l3d_cache_allocate/u',\n",
    "            'armv8_pmuv3/l3d_cache_refill/u',\n",
    "            'armv8_pmuv3/l3d_cache_wb/u',\n",
    "            'armv8_pmuv3/mem_access/u',\n",
    "            'ipc'])  # Features\n",
    "    y = pd.Series(out_of_order_ipc) \n",
    "    \n",
    "    # split \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    rf_model = RandomForestRegressor(n_estimators=100)\n",
    "    gb_model = GradientBoostingRegressor(n_estimators=100)\n",
    "    ensemble_model = gb_model\n",
    "    #ensemble_model = VotingRegressor([('rf', rf_model), ('gb', gb_model)])\n",
    "    ensemble_model.fit(X_train, y_train)\n",
    "    \n",
    "    ensemble_predictions = ensemble_model.predict(X_test)\n",
    "    # Calculate R²\n",
    "    ensemble_r2 = r2_score(y_test, ensemble_predictions)\n",
    "    \n",
    "    #bruh wut\n",
    "    train_predictions = ensemble_model.predict(X_train)\n",
    "    test_predictions = ensemble_model.predict(X_test)\n",
    "    \n",
    "    # Calculate R² for training and testing sets\n",
    "    train_r2 = r2_score(y_train, train_predictions)\n",
    "    train_mape = mean_absolute_percentage_error(y_train, train_predictions)\n",
    "    test_r2 = r2_score(y_test, test_predictions)\n",
    "    test_mape = mean_absolute_percentage_error(y_test, test_predictions)\n",
    "    \n",
    "    \n",
    "    print(f\"Training R²: {train_r2}\")\n",
    "    print(f\"Testing R²: {test_r2}\")\n",
    "    print(f\"Training Mape: {train_mape}\")\n",
    "    print(f\"Testing Mape: {test_mape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6dc43da5-edad-4f8b-b5f3-ccd64577191b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_size: 1000000000\n",
      "Training R²: 0.727211322573146\n",
      "Testing R²: 0.6999231595733997\n",
      "Training Mape: 0.13120868513327424\n",
      "Testing Mape: 0.1370728327353493\n",
      "chunk_size: 2000000000\n",
      "Training R²: 0.7770220044037062\n",
      "Testing R²: 0.733048992802261\n",
      "Training Mape: 0.11345889235866469\n",
      "Testing Mape: 0.11963723569265175\n",
      "chunk_size: 3000000000\n",
      "Training R²: 0.7774327327198404\n",
      "Testing R²: 0.7226523084476628\n",
      "Training Mape: 0.11270820178765605\n",
      "Testing Mape: 0.12593479122187268\n",
      "chunk_size: 4000000000\n",
      "Training R²: 0.7968335389950065\n",
      "Testing R²: 0.7308521732641571\n",
      "Training Mape: 0.10687598395707912\n",
      "Testing Mape: 0.12147807819305823\n",
      "chunk_size: 5000000000\n",
      "Training R²: 0.8229883119623735\n",
      "Testing R²: 0.7134723894246384\n",
      "Training Mape: 0.09761511752713292\n",
      "Testing Mape: 0.12303076278313574\n",
      "chunk_size: 10000000000\n",
      "Training R²: 0.8497857971252774\n",
      "Testing R²: 0.7573345798235489\n",
      "Training Mape: 0.09068323801040805\n",
      "Testing Mape: 0.12498022156061978\n",
      "chunk_size: 20000000000\n",
      "Training R²: 0.8931701337902705\n",
      "Testing R²: 0.7117183235052581\n",
      "Training Mape: 0.07887001706461579\n",
      "Testing Mape: 0.1093441969242858\n",
      "chunk_size: 25000000000\n",
      "Training R²: 0.9138664439636662\n",
      "Testing R²: 0.723451614909468\n",
      "Training Mape: 0.06766682498009435\n",
      "Testing Mape: 0.11741885043581678\n"
     ]
    }
   ],
   "source": [
    "# desktop to server\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "def process_csv_feautures(file_paths, chunk_size):\n",
    "    all_features = []\n",
    "    for file_path in file_paths:\n",
    "        df = pd.read_csv(file_path + '_' + str(chunk_size))\n",
    "        df = df.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "        features = []\n",
    "        for index, row in df.iterrows():\n",
    "            inst = row['instructions']\n",
    "            cyc = row['cpu-cycles']\n",
    "            ipc = inst/cyc\n",
    "            features.append([\n",
    "                row['instructions'],\n",
    "                row['cpu-cycles'],\n",
    "                row['br_pred'],\n",
    "                row['br_mis_pred'],\n",
    "                row['l1d_cache_rd'],\n",
    "                row['l1d_cache_wr'],\n",
    "                row['l1d_cache'],\n",
    "                row['l1i_cache'],\n",
    "                row['l1i_cache_refill'],\n",
    "                row['context-switches'],\n",
    "                row['l2d_cache_rd'],\n",
    "                row['l2d_cache_wr'],\n",
    "                row['l2d_cache'],\n",
    "                row['l1d_tlb'],\n",
    "                row['l1d_tlb_refill_rd'],\n",
    "                row['l1d_tlb_refill_wr'],\n",
    "                row['dtlb_walk'],\n",
    "                row['itlb_walk'],\n",
    "                row['page-faults'],\n",
    "                row['l2d_tlb_access'],\n",
    "                row['l2i_tlb_access'],\n",
    "                row['l1i_tlb_refill'],\n",
    "                row['iTLB-loads'],\n",
    "                row['iTLB-load-misses'],\n",
    "                row['branch-loads'],\n",
    "                row['dTLB-loads'],\n",
    "                row['dTLB-load-misses'],\n",
    "                row['branch-load-misses'],\n",
    "                row['vfp_spec'],\n",
    "                row['inst_spec'],\n",
    "                row['ase_spec'],\n",
    "                row['bx_stall'],\n",
    "                row['decode_stall'],\n",
    "                row['dispatch_stall'],\n",
    "                row['fx_stall'],\n",
    "                row['ixa_stall'],\n",
    "                row['ixb_stall'],\n",
    "                row['lx_stall'],\n",
    "                row['sx_stall'],\n",
    "                row['bus_access'],\n",
    "                row['mem_access'],\n",
    "                row['mem_access_rd'],\n",
    "                row['mem_access_wr'],\n",
    "                ipc])\n",
    "        all_features.extend(features)\n",
    "    return all_features\n",
    "\n",
    "\n",
    "def process_csv_ipc(file_paths, chunk_size):\n",
    "    all_ipc_values = []\n",
    "    for file_path in file_paths:\n",
    "        df = pd.read_csv(file_path + '_' + str(chunk_size))\n",
    "        df = df.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "        for _, row in df.iterrows():\n",
    "            all_ipc_values.append(row['instructions'] / row['cpu-cycles'])  \n",
    "    return all_ipc_values\n",
    "\n",
    "source = [\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_500',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_502',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_505',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_520',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_523',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_525',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_531',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_541',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_548',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_557',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_503',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_507',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_508',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_510',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_511',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_519',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_521',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_527',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_538',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_544',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_549',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_554',\n",
    "]\n",
    "    \n",
    "target = [\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_500',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_502',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_505',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_520',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_523',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_525',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_531',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_541',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_548',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_557',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_503',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_507',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_508',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_510',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_511',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_519',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_521',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_527',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_538',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_544',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_549',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_554',\n",
    "]\n",
    "\n",
    "chunks = [1000000000,2000000000,3000000000,4000000000,5000000000,10000000000,20000000000,25000000000]\n",
    "for chunk_size in chunks:\n",
    "    in_order_hardware_counters_features = process_csv_feautures(source, chunk_size)\n",
    "    out_of_order_ipc = process_csv_ipc(target, chunk_size)\n",
    "    print('chunk_size: ' + str(chunk_size))\n",
    "    \n",
    "    data = {\n",
    "        'In Order': in_order_hardware_counters_features,\n",
    "        'Out of Order': out_of_order_ipc\n",
    "    }\n",
    "    \n",
    "    length = min(len(in_order_hardware_counters_features), len(out_of_order_ipc))\n",
    "    \n",
    "    # Shorten the longen array\n",
    "    in_order_hardware_counters_features = in_order_hardware_counters_features[:length]\n",
    "    out_of_order_ipc = out_of_order_ipc[:length]\n",
    "    \n",
    "    # separate features and target\n",
    "    X = pd.DataFrame(\n",
    "        in_order_hardware_counters_features, \n",
    "        columns=[\n",
    "            'instructions',\n",
    "            'cpu-cycles',\n",
    "            'br_pred',\n",
    "            'br_mis_pred',\n",
    "            'l1d_cache_rd',\n",
    "            'l1d_cache_wr',\n",
    "            'l1d_cache',\n",
    "            'l1i_cache',\n",
    "            'l1i_cache_refill',\n",
    "            'context-switches',\n",
    "            'l2d_cache_rd',\n",
    "            'l2d_cache_wr',\n",
    "            'l2d_cache',\n",
    "            'l1d_tlb',\n",
    "            'l1d_tlb_refill_rd',\n",
    "            'l1d_tlb_refill_wr',\n",
    "            'dtlb_walk',\n",
    "            'itlb_walk',\n",
    "            'page-faults',\n",
    "            'l2d_tlb_access',\n",
    "            'l2i_tlb_access',\n",
    "            'l1i_tlb_refill',\n",
    "            'iTLB-loads',\n",
    "            'iTLB-load-misses',\n",
    "            'branch-loads',\n",
    "            'dTLB-loads',\n",
    "            'dTLB-load-misses',\n",
    "            'branch-load-misses',\n",
    "            'vfp_spec',\n",
    "            'inst_spec',\n",
    "            'ase_spec',\n",
    "            'bx_stall',\n",
    "            'decode_stall',\n",
    "            'dispatch_stall',\n",
    "            'fx_stall',\n",
    "            'ixa_stall',\n",
    "            'ixb_stall',\n",
    "            'lx_stall',\n",
    "            'sx_stall',\n",
    "            'bus_access',\n",
    "            'mem_access',\n",
    "            'mem_access_rd',\n",
    "            'mem_access_wr',\n",
    "            'ipc'])  # Features\n",
    "    y = pd.Series(out_of_order_ipc) \n",
    "    \n",
    "    # split \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    rf_model = RandomForestRegressor(n_estimators=100)\n",
    "    gb_model = GradientBoostingRegressor(n_estimators=100)\n",
    "    ensemble_model = gb_model\n",
    "    #ensemble_model = VotingRegressor([('rf', rf_model), ('gb', gb_model)])\n",
    "    ensemble_model.fit(X_train, y_train)\n",
    "    \n",
    "    ensemble_predictions = ensemble_model.predict(X_test)\n",
    "    # Calculate R²\n",
    "    ensemble_r2 = r2_score(y_test, ensemble_predictions)\n",
    "    \n",
    "    #bruh wut\n",
    "    train_predictions = ensemble_model.predict(X_train)\n",
    "    test_predictions = ensemble_model.predict(X_test)\n",
    "    \n",
    "    # Calculate R² for training and testing sets\n",
    "    train_r2 = r2_score(y_train, train_predictions)\n",
    "    train_mape = mean_absolute_percentage_error(y_train, train_predictions)\n",
    "    test_r2 = r2_score(y_test, test_predictions)\n",
    "    test_mape = mean_absolute_percentage_error(y_test, test_predictions)\n",
    "    \n",
    "    \n",
    "    print(f\"Training R²: {train_r2}\")\n",
    "    print(f\"Testing R²: {test_r2}\")\n",
    "    print(f\"Training Mape: {train_mape}\")\n",
    "    print(f\"Testing Mape: {test_mape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2983595c-dd6d-4752-835a-596ca83e0da6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_size: 1000000000\n",
      "Training R²: 0.7608788396226028\n",
      "Testing R²: 0.7365513401287896\n",
      "Training Mape: 0.21193932739485133\n",
      "Testing Mape: 0.2197005709292848\n",
      "chunk_size: 2000000000\n",
      "Training R²: 0.7806624681250065\n",
      "Testing R²: 0.77073735960906\n",
      "Training Mape: 0.19344178324315714\n",
      "Testing Mape: 0.1946951174135152\n",
      "chunk_size: 3000000000\n",
      "Training R²: 0.7981602063586978\n",
      "Testing R²: 0.7662139439027111\n",
      "Training Mape: 0.1830137713215602\n",
      "Testing Mape: 0.18765029761497082\n",
      "chunk_size: 4000000000\n",
      "Training R²: 0.815577660408685\n",
      "Testing R²: 0.7662222180778089\n",
      "Training Mape: 0.17183815368736854\n",
      "Testing Mape: 0.18169142067503358\n",
      "chunk_size: 5000000000\n",
      "Training R²: 0.8227272799192334\n",
      "Testing R²: 0.7762812050201559\n",
      "Training Mape: 0.1636308958193071\n",
      "Testing Mape: 0.18631622684601223\n",
      "chunk_size: 10000000000\n",
      "Training R²: 0.8528954455977872\n",
      "Testing R²: 0.7860679234089263\n",
      "Training Mape: 0.14609409886289138\n",
      "Testing Mape: 0.17852729906919917\n",
      "chunk_size: 20000000000\n",
      "Training R²: 0.8949484145943186\n",
      "Testing R²: 0.7845558074032386\n",
      "Training Mape: 0.1271322684846998\n",
      "Testing Mape: 0.16530402094123592\n",
      "chunk_size: 25000000000\n",
      "Training R²: 0.8983546275105134\n",
      "Testing R²: 0.7882538613860737\n",
      "Training Mape: 0.1265306707533097\n",
      "Testing Mape: 0.17957015080953595\n"
     ]
    }
   ],
   "source": [
    "# desktop to InO\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "def process_csv_feautures(file_paths, chunk_size):\n",
    "    all_features = []\n",
    "    for file_path in file_paths:\n",
    "        df = pd.read_csv(file_path + '_' + str(chunk_size))\n",
    "        df = df.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "        features = []\n",
    "        for index, row in df.iterrows():\n",
    "            inst = row['instructions']\n",
    "            cyc = row['cpu-cycles']\n",
    "            ipc = inst/cyc\n",
    "            features.append([\n",
    "                row['instructions'],\n",
    "                row['cpu-cycles'],\n",
    "                row['br_pred'],\n",
    "                row['br_mis_pred'],\n",
    "                row['l1d_cache_rd'],\n",
    "                row['l1d_cache_wr'],\n",
    "                row['l1d_cache'],\n",
    "                row['l1i_cache'],\n",
    "                row['l1i_cache_refill'],\n",
    "                row['context-switches'],\n",
    "                row['l2d_cache_rd'],\n",
    "                row['l2d_cache_wr'],\n",
    "                row['l2d_cache'],\n",
    "                row['l1d_tlb'],\n",
    "                row['l1d_tlb_refill_rd'],\n",
    "                row['l1d_tlb_refill_wr'],\n",
    "                row['dtlb_walk'],\n",
    "                row['itlb_walk'],\n",
    "                row['page-faults'],\n",
    "                row['l2d_tlb_access'],\n",
    "                row['l2i_tlb_access'],\n",
    "                row['l1i_tlb_refill'],\n",
    "                row['iTLB-loads'],\n",
    "                row['iTLB-load-misses'],\n",
    "                row['branch-loads'],\n",
    "                row['dTLB-loads'],\n",
    "                row['dTLB-load-misses'],\n",
    "                row['branch-load-misses'],\n",
    "                row['vfp_spec'],\n",
    "                row['inst_spec'],\n",
    "                row['ase_spec'],\n",
    "                row['bx_stall'],\n",
    "                row['decode_stall'],\n",
    "                row['dispatch_stall'],\n",
    "                row['fx_stall'],\n",
    "                row['ixa_stall'],\n",
    "                row['ixb_stall'],\n",
    "                row['lx_stall'],\n",
    "                row['sx_stall'],\n",
    "                row['bus_access'],\n",
    "                row['mem_access'],\n",
    "                row['mem_access_rd'],\n",
    "                row['mem_access_wr'],\n",
    "                ipc])\n",
    "        all_features.extend(features)\n",
    "    return all_features\n",
    "\n",
    "\n",
    "def process_csv_ipc(file_paths, chunk_size):\n",
    "    all_ipc_values = []\n",
    "    for file_path in file_paths:\n",
    "        df = pd.read_csv(file_path + '_' + str(chunk_size))\n",
    "        df = df.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "        for _, row in df.iterrows():\n",
    "            all_ipc_values.append(row['instructions:u'] / row['cpu-cycles:u'])  \n",
    "    return all_ipc_values\n",
    "\n",
    "source = [\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_500',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_502',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_505',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_520',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_523',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_525',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_531',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_541',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_548',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_557',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_503',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_507',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_508',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_510',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_511',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_519',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_521',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_527',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_538',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_544',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_549',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_554',\n",
    "]\n",
    "    \n",
    "target = [\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_500',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_502',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_505',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_520',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_523',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_525',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_531',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_541',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_548',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_557',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_503',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_507',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_508',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_510',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_511',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_519',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_521',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_527',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_538',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_544',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_549',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_554',\n",
    "]\n",
    "\n",
    "chunks = [1000000000,2000000000,3000000000,4000000000,5000000000,10000000000,20000000000,25000000000]\n",
    "for chunk_size in chunks:\n",
    "    in_order_hardware_counters_features = process_csv_feautures(source, chunk_size)\n",
    "    out_of_order_ipc = process_csv_ipc(target, chunk_size)\n",
    "    print('chunk_size: ' + str(chunk_size))\n",
    "    \n",
    "    data = {\n",
    "        'In Order': in_order_hardware_counters_features,\n",
    "        'Out of Order': out_of_order_ipc\n",
    "    }\n",
    "    \n",
    "    length = min(len(in_order_hardware_counters_features), len(out_of_order_ipc))\n",
    "    \n",
    "    # Shorten the longen array\n",
    "    in_order_hardware_counters_features = in_order_hardware_counters_features[:length]\n",
    "    out_of_order_ipc = out_of_order_ipc[:length]\n",
    "    \n",
    "    # separate features and target\n",
    "    X = pd.DataFrame(\n",
    "        in_order_hardware_counters_features, \n",
    "        columns=[\n",
    "            'instructions',\n",
    "            'cpu-cycles',\n",
    "            'br_pred',\n",
    "            'br_mis_pred',\n",
    "            'l1d_cache_rd',\n",
    "            'l1d_cache_wr',\n",
    "            'l1d_cache',\n",
    "            'l1i_cache',\n",
    "            'l1i_cache_refill',\n",
    "            'context-switches',\n",
    "            'l2d_cache_rd',\n",
    "            'l2d_cache_wr',\n",
    "            'l2d_cache',\n",
    "            'l1d_tlb',\n",
    "            'l1d_tlb_refill_rd',\n",
    "            'l1d_tlb_refill_wr',\n",
    "            'dtlb_walk',\n",
    "            'itlb_walk',\n",
    "            'page-faults',\n",
    "            'l2d_tlb_access',\n",
    "            'l2i_tlb_access',\n",
    "            'l1i_tlb_refill',\n",
    "            'iTLB-loads',\n",
    "            'iTLB-load-misses',\n",
    "            'branch-loads',\n",
    "            'dTLB-loads',\n",
    "            'dTLB-load-misses',\n",
    "            'branch-load-misses',\n",
    "            'vfp_spec',\n",
    "            'inst_spec',\n",
    "            'ase_spec',\n",
    "            'bx_stall',\n",
    "            'decode_stall',\n",
    "            'dispatch_stall',\n",
    "            'fx_stall',\n",
    "            'ixa_stall',\n",
    "            'ixb_stall',\n",
    "            'lx_stall',\n",
    "            'sx_stall',\n",
    "            'bus_access',\n",
    "            'mem_access',\n",
    "            'mem_access_rd',\n",
    "            'mem_access_wr',\n",
    "            'ipc'])  # Features\n",
    "    y = pd.Series(out_of_order_ipc) \n",
    "    \n",
    "    # split \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    rf_model = RandomForestRegressor(n_estimators=100)\n",
    "    gb_model = GradientBoostingRegressor(n_estimators=100)\n",
    "    ensemble_model = gb_model\n",
    "    #ensemble_model = VotingRegressor([('rf', rf_model), ('gb', gb_model)])\n",
    "    ensemble_model.fit(X_train, y_train)\n",
    "    \n",
    "    ensemble_predictions = ensemble_model.predict(X_test)\n",
    "    # Calculate R²\n",
    "    ensemble_r2 = r2_score(y_test, ensemble_predictions)\n",
    "    \n",
    "    #bruh wut\n",
    "    train_predictions = ensemble_model.predict(X_train)\n",
    "    test_predictions = ensemble_model.predict(X_test)\n",
    "    \n",
    "    # Calculate R² for training and testing sets\n",
    "    train_r2 = r2_score(y_train, train_predictions)\n",
    "    train_mape = mean_absolute_percentage_error(y_train, train_predictions)\n",
    "    test_r2 = r2_score(y_test, test_predictions)\n",
    "    test_mape = mean_absolute_percentage_error(y_test, test_predictions)\n",
    "    \n",
    "    \n",
    "    print(f\"Training R²: {train_r2}\")\n",
    "    print(f\"Testing R²: {test_r2}\")\n",
    "    print(f\"Training Mape: {train_mape}\")\n",
    "    print(f\"Testing Mape: {test_mape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2bb2831e-56ab-49c6-b9d7-6a047be1c524",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_size: 1000000000\n",
      "Training R²: 0.7222113355467252\n",
      "Testing R²: 0.7048806074814544\n",
      "Training Mape: 0.13721243699680027\n",
      "Testing Mape: 0.14006001208682833\n",
      "chunk_size: 2000000000\n",
      "Training R²: 0.7366127629665724\n",
      "Testing R²: 0.73249279823532\n",
      "Training Mape: 0.1313362912851658\n",
      "Testing Mape: 0.13515558913547593\n",
      "chunk_size: 3000000000\n",
      "Training R²: 0.7656155604593871\n",
      "Testing R²: 0.7453264004335969\n",
      "Training Mape: 0.12348016404866816\n",
      "Testing Mape: 0.12156360212145055\n",
      "chunk_size: 4000000000\n",
      "Training R²: 0.7909600909021419\n",
      "Testing R²: 0.7352717198430467\n",
      "Training Mape: 0.11644727834817763\n",
      "Testing Mape: 0.12268356630038195\n",
      "chunk_size: 5000000000\n",
      "Training R²: 0.797462450340034\n",
      "Testing R²: 0.7559239161792449\n",
      "Training Mape: 0.11211626560469953\n",
      "Testing Mape: 0.12021924311504427\n",
      "chunk_size: 10000000000\n",
      "Training R²: 0.8576312998388194\n",
      "Testing R²: 0.7073427337254444\n",
      "Training Mape: 0.09339203433142906\n",
      "Testing Mape: 0.1265684355501373\n",
      "chunk_size: 20000000000\n",
      "Training R²: 0.8879247398330529\n",
      "Testing R²: 0.7681150218126945\n",
      "Training Mape: 0.08191349592614326\n",
      "Testing Mape: 0.12440217591834422\n",
      "chunk_size: 25000000000\n",
      "Training R²: 0.9092127154595018\n",
      "Testing R²: 0.7560151080332655\n",
      "Training Mape: 0.07572971355491842\n",
      "Testing Mape: 0.11752227973609385\n"
     ]
    }
   ],
   "source": [
    "# desktop to OOO\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "def process_csv_feautures(file_paths, chunk_size):\n",
    "    all_features = []\n",
    "    for file_path in file_paths:\n",
    "        df = pd.read_csv(file_path + '_' + str(chunk_size))\n",
    "        df = df.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "        features = []\n",
    "        for index, row in df.iterrows():\n",
    "            inst = row['instructions']\n",
    "            cyc = row['cpu-cycles']\n",
    "            ipc = inst/cyc\n",
    "            features.append([\n",
    "                row['instructions'],\n",
    "                row['cpu-cycles'],\n",
    "                row['br_pred'],\n",
    "                row['br_mis_pred'],\n",
    "                row['l1d_cache_rd'],\n",
    "                row['l1d_cache_wr'],\n",
    "                row['l1d_cache'],\n",
    "                row['l1i_cache'],\n",
    "                row['l1i_cache_refill'],\n",
    "                row['context-switches'],\n",
    "                row['l2d_cache_rd'],\n",
    "                row['l2d_cache_wr'],\n",
    "                row['l2d_cache'],\n",
    "                row['l1d_tlb'],\n",
    "                row['l1d_tlb_refill_rd'],\n",
    "                row['l1d_tlb_refill_wr'],\n",
    "                row['dtlb_walk'],\n",
    "                row['itlb_walk'],\n",
    "                row['page-faults'],\n",
    "                row['l2d_tlb_access'],\n",
    "                row['l2i_tlb_access'],\n",
    "                row['l1i_tlb_refill'],\n",
    "                row['iTLB-loads'],\n",
    "                row['iTLB-load-misses'],\n",
    "                row['branch-loads'],\n",
    "                row['dTLB-loads'],\n",
    "                row['dTLB-load-misses'],\n",
    "                row['branch-load-misses'],\n",
    "                row['vfp_spec'],\n",
    "                row['inst_spec'],\n",
    "                row['ase_spec'],\n",
    "                row['bx_stall'],\n",
    "                row['decode_stall'],\n",
    "                row['dispatch_stall'],\n",
    "                row['fx_stall'],\n",
    "                row['ixa_stall'],\n",
    "                row['ixb_stall'],\n",
    "                row['lx_stall'],\n",
    "                row['sx_stall'],\n",
    "                row['bus_access'],\n",
    "                row['mem_access'],\n",
    "                row['mem_access_rd'],\n",
    "                row['mem_access_wr'],\n",
    "                ipc])\n",
    "        all_features.extend(features)\n",
    "    return all_features\n",
    "\n",
    "\n",
    "def process_csv_ipc(file_paths, chunk_size):\n",
    "    all_ipc_values = []\n",
    "    for file_path in file_paths:\n",
    "        df = pd.read_csv(file_path + '_' + str(chunk_size))\n",
    "        df = df.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "        for _, row in df.iterrows():\n",
    "            all_ipc_values.append(row['instructions:u'] / row['cpu-cycles:u'])  \n",
    "    return all_ipc_values\n",
    "\n",
    "source = [\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_500',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_502',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_505',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_520',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_523',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_525',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_531',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_541',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_548',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_557',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_503',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_507',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_508',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_510',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_511',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_519',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_521',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_527',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_538',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_544',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_549',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_554',\n",
    "]\n",
    "    \n",
    "target = [\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_500',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_502',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_505',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_520',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_523',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_525',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_531',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_541',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_548',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_557',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_503',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_507',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_508',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_510',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_511',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_519',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_521',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_527',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_538',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_544',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_549',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_554',\n",
    "]\n",
    "\n",
    "chunks = [1000000000,2000000000,3000000000,4000000000,5000000000,10000000000,20000000000,25000000000]\n",
    "for chunk_size in chunks:\n",
    "    in_order_hardware_counters_features = process_csv_feautures(source, chunk_size)\n",
    "    out_of_order_ipc = process_csv_ipc(target, chunk_size)\n",
    "    print('chunk_size: ' + str(chunk_size))\n",
    "    \n",
    "    data = {\n",
    "        'In Order': in_order_hardware_counters_features,\n",
    "        'Out of Order': out_of_order_ipc\n",
    "    }\n",
    "    \n",
    "    length = min(len(in_order_hardware_counters_features), len(out_of_order_ipc))\n",
    "    \n",
    "    # Shorten the longen array\n",
    "    in_order_hardware_counters_features = in_order_hardware_counters_features[:length]\n",
    "    out_of_order_ipc = out_of_order_ipc[:length]\n",
    "    \n",
    "    # separate features and target\n",
    "    X = pd.DataFrame(\n",
    "        in_order_hardware_counters_features, \n",
    "        columns=[\n",
    "            'instructions',\n",
    "            'cpu-cycles',\n",
    "            'br_pred',\n",
    "            'br_mis_pred',\n",
    "            'l1d_cache_rd',\n",
    "            'l1d_cache_wr',\n",
    "            'l1d_cache',\n",
    "            'l1i_cache',\n",
    "            'l1i_cache_refill',\n",
    "            'context-switches',\n",
    "            'l2d_cache_rd',\n",
    "            'l2d_cache_wr',\n",
    "            'l2d_cache',\n",
    "            'l1d_tlb',\n",
    "            'l1d_tlb_refill_rd',\n",
    "            'l1d_tlb_refill_wr',\n",
    "            'dtlb_walk',\n",
    "            'itlb_walk',\n",
    "            'page-faults',\n",
    "            'l2d_tlb_access',\n",
    "            'l2i_tlb_access',\n",
    "            'l1i_tlb_refill',\n",
    "            'iTLB-loads',\n",
    "            'iTLB-load-misses',\n",
    "            'branch-loads',\n",
    "            'dTLB-loads',\n",
    "            'dTLB-load-misses',\n",
    "            'branch-load-misses',\n",
    "            'vfp_spec',\n",
    "            'inst_spec',\n",
    "            'ase_spec',\n",
    "            'bx_stall',\n",
    "            'decode_stall',\n",
    "            'dispatch_stall',\n",
    "            'fx_stall',\n",
    "            'ixa_stall',\n",
    "            'ixb_stall',\n",
    "            'lx_stall',\n",
    "            'sx_stall',\n",
    "            'bus_access',\n",
    "            'mem_access',\n",
    "            'mem_access_rd',\n",
    "            'mem_access_wr',\n",
    "            'ipc'])  # Features\n",
    "    y = pd.Series(out_of_order_ipc) \n",
    "    \n",
    "    # split \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    rf_model = RandomForestRegressor(n_estimators=100)\n",
    "    gb_model = GradientBoostingRegressor(n_estimators=100)\n",
    "    ensemble_model = gb_model\n",
    "    #ensemble_model = VotingRegressor([('rf', rf_model), ('gb', gb_model)])\n",
    "    ensemble_model.fit(X_train, y_train)\n",
    "    \n",
    "    ensemble_predictions = ensemble_model.predict(X_test)\n",
    "    # Calculate R²\n",
    "    ensemble_r2 = r2_score(y_test, ensemble_predictions)\n",
    "    \n",
    "    #bruh wut\n",
    "    train_predictions = ensemble_model.predict(X_train)\n",
    "    test_predictions = ensemble_model.predict(X_test)\n",
    "    \n",
    "    # Calculate R² for training and testing sets\n",
    "    train_r2 = r2_score(y_train, train_predictions)\n",
    "    train_mape = mean_absolute_percentage_error(y_train, train_predictions)\n",
    "    test_r2 = r2_score(y_test, test_predictions)\n",
    "    test_mape = mean_absolute_percentage_error(y_test, test_predictions)\n",
    "    \n",
    "    \n",
    "    print(f\"Training R²: {train_r2}\")\n",
    "    print(f\"Testing R²: {test_r2}\")\n",
    "    print(f\"Training Mape: {train_mape}\")\n",
    "    print(f\"Testing Mape: {test_mape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "242a57a0-408a-4931-907a-3f28e5b9663d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_size: 1000000000\n",
      "Training R²: 0.7766619350559375\n",
      "Testing R²: 0.7526961320797898\n",
      "Training Mape: 0.19822847082735487\n",
      "Testing Mape: 0.2075689213583482\n",
      "chunk_size: 2000000000\n",
      "Training R²: 0.8131491318799707\n",
      "Testing R²: 0.778099729569258\n",
      "Training Mape: 0.17806313116063063\n",
      "Testing Mape: 0.19088055862222772\n",
      "chunk_size: 3000000000\n",
      "Training R²: 0.8336577599652779\n",
      "Testing R²: 0.8092368424336918\n",
      "Training Mape: 0.16453501709496313\n",
      "Testing Mape: 0.17420368086376847\n",
      "chunk_size: 4000000000\n",
      "Training R²: 0.8494653102013139\n",
      "Testing R²: 0.805911470986955\n",
      "Training Mape: 0.15232186306925688\n",
      "Testing Mape: 0.16648279414446826\n",
      "chunk_size: 5000000000\n",
      "Training R²: 0.8735227188210615\n",
      "Testing R²: 0.7911154943003806\n",
      "Training Mape: 0.13782874173664436\n",
      "Testing Mape: 0.16758454297996625\n",
      "chunk_size: 10000000000\n",
      "Training R²: 0.9016421676781816\n",
      "Testing R²: 0.8191406935669961\n",
      "Training Mape: 0.1196150305920635\n",
      "Testing Mape: 0.16146744116892342\n",
      "chunk_size: 20000000000\n",
      "Training R²: 0.9263319231216207\n",
      "Testing R²: 0.805577814764915\n",
      "Training Mape: 0.0971187690686834\n",
      "Testing Mape: 0.1605380642385878\n",
      "chunk_size: 25000000000\n",
      "Training R²: 0.9394558146639762\n",
      "Testing R²: 0.8045345269922675\n",
      "Training Mape: 0.09095971983278575\n",
      "Testing Mape: 0.16034735030804734\n"
     ]
    }
   ],
   "source": [
    "# server to InO\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "def process_csv_feautures(file_paths, chunk_size):\n",
    "    all_features = []\n",
    "    for file_path in file_paths:\n",
    "        df = pd.read_csv(file_path + '_' + str(chunk_size))\n",
    "        df = df.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "        features = []\n",
    "        for index, row in df.iterrows():\n",
    "            inst = row['instructions']\n",
    "            cyc = row['cpu-cycles']\n",
    "            ipc = inst/cyc\n",
    "            features.append([\n",
    "                row['instructions'],\n",
    "                row['cpu-cycles'], \n",
    "                row['br_pred'], \n",
    "                row['br_mis_pred'], \n",
    "                row['l1d_cache'], \n",
    "                row['l1d_tlb'], \n",
    "                row['l1d_tlb_rd'], \n",
    "                row['l1d_tlb_wr'], \n",
    "                row['l2d_cache_rd'], \n",
    "                row['l2d_cache_wr'], \n",
    "                row['l2d_cache'], \n",
    "                row['l1i_cache'], \n",
    "                row['l1i_cache_refill'], \n",
    "                row['remote_access'], \n",
    "                row['dtlb_walk'], \n",
    "                row['itlb_walk'], \n",
    "                row['l1i_tlb'], \n",
    "                row['l2d_tlb'], \n",
    "                row['l2d_tlb_rd'], \n",
    "                row['l2d_tlb_wr'], \n",
    "                row['vfp_spec'], \n",
    "                row['inst_spec'], \n",
    "                row['ase_spec'], \n",
    "                row['stall_backend'], \n",
    "                row['stall_frontend'], \n",
    "                row['ll_cache_miss_rd'], \n",
    "                row['mem_access'], \n",
    "                row['mem_access_rd'], \n",
    "                row['mem_access_wr'],\n",
    "                ipc])\n",
    "        all_features.extend(features)\n",
    "    return all_features\n",
    "\n",
    "\n",
    "def process_csv_ipc(file_paths, chunk_size):\n",
    "    all_ipc_values = []\n",
    "    for file_path in file_paths:\n",
    "        df = pd.read_csv(file_path + '_' + str(chunk_size))\n",
    "        df = df.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "        for _, row in df.iterrows():\n",
    "            all_ipc_values.append(row['instructions:u'] / row['cpu-cycles:u'])  \n",
    "    return all_ipc_values\n",
    "\n",
    "source = [\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_500',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_502',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_505',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_520',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_523',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_525',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_531',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_541',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_548',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_557',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_503',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_507',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_508',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_510',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_511',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_519',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_521',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_527',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_538',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_544',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_549',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_554',\n",
    "]\n",
    "    \n",
    "target = [\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_500',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_502',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_505',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_520',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_523',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_525',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_531',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_541',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_548',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_557',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_503',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_507',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_508',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_510',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_511',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_519',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_521',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_527',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_538',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_544',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_549',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_554',\n",
    "]\n",
    "\n",
    "chunks = [1000000000,2000000000,3000000000,4000000000,5000000000,10000000000,20000000000,25000000000]\n",
    "for chunk_size in chunks:\n",
    "    in_order_hardware_counters_features = process_csv_feautures(source, chunk_size)\n",
    "    out_of_order_ipc = process_csv_ipc(target, chunk_size)\n",
    "    print('chunk_size: ' + str(chunk_size))\n",
    "    \n",
    "    data = {\n",
    "        'In Order': in_order_hardware_counters_features,\n",
    "        'Out of Order': out_of_order_ipc\n",
    "    }\n",
    "    \n",
    "    length = min(len(in_order_hardware_counters_features), len(out_of_order_ipc))\n",
    "    \n",
    "    # Shorten the longen array\n",
    "    in_order_hardware_counters_features = in_order_hardware_counters_features[:length]\n",
    "    out_of_order_ipc = out_of_order_ipc[:length]\n",
    "    \n",
    "    # separate features and target\n",
    "    X = pd.DataFrame(\n",
    "        in_order_hardware_counters_features, \n",
    "        columns=[\n",
    "            'instructions',\n",
    "            'cpu-cycles', \n",
    "            'br_pred', \n",
    "            'br_mis_pred', \n",
    "            'l1d_cache', \n",
    "            'l1d_tlb', \n",
    "            'l1d_tlb_rd', \n",
    "            'l1d_tlb_wr', \n",
    "            'l2d_cache_rd', \n",
    "            'l2d_cache_wr', \n",
    "            'l2d_cache', \n",
    "            'l1i_cache', \n",
    "            'l1i_cache_refill', \n",
    "            'remote_access', \n",
    "            'dtlb_walk', \n",
    "            'itlb_walk', \n",
    "            'l1i_tlb', \n",
    "            'l2d_tlb', \n",
    "            'l2d_tlb_rd', \n",
    "            'l2d_tlb_wr', \n",
    "            'vfp_spec', \n",
    "            'inst_spec', \n",
    "            'ase_spec', \n",
    "            'stall_backend', \n",
    "            'stall_frontend', \n",
    "            'll_cache_miss_rd', \n",
    "            'mem_access', \n",
    "            'mem_access_rd', \n",
    "            'mem_access_wr',\n",
    "            'ipc'])  # Features\n",
    "    y = pd.Series(out_of_order_ipc) \n",
    "    \n",
    "    # split \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    rf_model = RandomForestRegressor(n_estimators=100)\n",
    "    gb_model = GradientBoostingRegressor(n_estimators=100)\n",
    "    ensemble_model = gb_model\n",
    "    #ensemble_model = VotingRegressor([('rf', rf_model), ('gb', gb_model)])\n",
    "    ensemble_model.fit(X_train, y_train)\n",
    "    \n",
    "    ensemble_predictions = ensemble_model.predict(X_test)\n",
    "    # Calculate R²\n",
    "    ensemble_r2 = r2_score(y_test, ensemble_predictions)\n",
    "    \n",
    "    #bruh wut\n",
    "    train_predictions = ensemble_model.predict(X_train)\n",
    "    test_predictions = ensemble_model.predict(X_test)\n",
    "    \n",
    "    # Calculate R² for training and testing sets\n",
    "    train_r2 = r2_score(y_train, train_predictions)\n",
    "    train_mape = mean_absolute_percentage_error(y_train, train_predictions)\n",
    "    test_r2 = r2_score(y_test, test_predictions)\n",
    "    test_mape = mean_absolute_percentage_error(y_test, test_predictions)\n",
    "    \n",
    "    \n",
    "    print(f\"Training R²: {train_r2}\")\n",
    "    print(f\"Testing R²: {test_r2}\")\n",
    "    print(f\"Training Mape: {train_mape}\")\n",
    "    print(f\"Testing Mape: {test_mape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec5f8efc-7df1-419f-9947-e3cc17903d60",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_size: 1000000000\n",
      "Training R²: 0.7731397865591598\n",
      "Testing R²: 0.7441179338941282\n",
      "Training Mape: 0.14769044776695972\n",
      "Testing Mape: 0.154459504032066\n",
      "chunk_size: 2000000000\n",
      "Training R²: 0.782394193828998\n",
      "Testing R²: 0.7380180388482798\n",
      "Training Mape: 0.13884475481265512\n",
      "Testing Mape: 0.15323965566114636\n",
      "chunk_size: 3000000000\n",
      "Training R²: 0.8118629466640039\n",
      "Testing R²: 0.7777999119993249\n",
      "Training Mape: 0.13291254786001885\n",
      "Testing Mape: 0.13704152900480202\n",
      "chunk_size: 4000000000\n",
      "Training R²: 0.83204482624144\n",
      "Testing R²: 0.7840143189304456\n",
      "Training Mape: 0.11968535059986456\n",
      "Testing Mape: 0.13024344966784876\n",
      "chunk_size: 5000000000\n",
      "Training R²: 0.8474109841053103\n",
      "Testing R²: 0.7984282592714552\n",
      "Training Mape: 0.11438449848047196\n",
      "Testing Mape: 0.12244566378123288\n",
      "chunk_size: 10000000000\n",
      "Training R²: 0.8834747306791049\n",
      "Testing R²: 0.7976192823720671\n",
      "Training Mape: 0.09982522491534683\n",
      "Testing Mape: 0.12949740085613518\n",
      "chunk_size: 20000000000\n",
      "Training R²: 0.9237187691478093\n",
      "Testing R²: 0.7359381669655356\n",
      "Training Mape: 0.0784906311581008\n",
      "Testing Mape: 0.14490185481547854\n",
      "chunk_size: 25000000000\n",
      "Training R²: 0.9297875077722607\n",
      "Testing R²: 0.7135849658395041\n",
      "Training Mape: 0.07844638748188508\n",
      "Testing Mape: 0.13665338387886644\n"
     ]
    }
   ],
   "source": [
    "# server to OOO\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "def process_csv_feautures(file_paths, chunk_size):\n",
    "    all_features = []\n",
    "    for file_path in file_paths:\n",
    "        df = pd.read_csv(file_path + '_' + str(chunk_size))\n",
    "        df = df.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "        features = []\n",
    "        for index, row in df.iterrows():\n",
    "            inst = row['instructions']\n",
    "            cyc = row['cpu-cycles']\n",
    "            ipc = inst/cyc\n",
    "            features.append([\n",
    "                row['instructions'],\n",
    "                row['cpu-cycles'], \n",
    "                row['br_pred'], \n",
    "                row['br_mis_pred'], \n",
    "                row['l1d_cache'], \n",
    "                row['l1d_tlb'], \n",
    "                row['l1d_tlb_rd'], \n",
    "                row['l1d_tlb_wr'], \n",
    "                row['l2d_cache_rd'], \n",
    "                row['l2d_cache_wr'], \n",
    "                row['l2d_cache'], \n",
    "                row['l1i_cache'], \n",
    "                row['l1i_cache_refill'], \n",
    "                row['remote_access'], \n",
    "                row['dtlb_walk'], \n",
    "                row['itlb_walk'], \n",
    "                row['l1i_tlb'], \n",
    "                row['l2d_tlb'], \n",
    "                row['l2d_tlb_rd'], \n",
    "                row['l2d_tlb_wr'], \n",
    "                row['vfp_spec'], \n",
    "                row['inst_spec'], \n",
    "                row['ase_spec'], \n",
    "                row['stall_backend'], \n",
    "                row['stall_frontend'], \n",
    "                row['ll_cache_miss_rd'], \n",
    "                row['mem_access'], \n",
    "                row['mem_access_rd'], \n",
    "                row['mem_access_wr'],\n",
    "                ipc])\n",
    "        all_features.extend(features)\n",
    "    return all_features\n",
    "\n",
    "\n",
    "def process_csv_ipc(file_paths, chunk_size):\n",
    "    all_ipc_values = []\n",
    "    for file_path in file_paths:\n",
    "        df = pd.read_csv(file_path + '_' + str(chunk_size))\n",
    "        df = df.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "        for _, row in df.iterrows():\n",
    "            all_ipc_values.append(row['instructions:u'] / row['cpu-cycles:u'])  \n",
    "    return all_ipc_values\n",
    "\n",
    "source = [\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_500',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_502',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_505',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_520',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_523',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_525',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_531',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_541',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_548',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_557',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_503',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_507',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_508',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_510',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_511',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_519',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_521',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_527',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_538',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_544',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_549',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_554',\n",
    "]\n",
    "    \n",
    "target = [\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_500',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_502',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_505',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_520',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_523',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_525',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_531',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_541',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_548',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_557',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_503',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_507',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_508',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_510',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_511',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_519',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_521',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_527',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_538',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_544',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_549',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_554',\n",
    "]\n",
    "\n",
    "chunks = [1000000000,2000000000,3000000000,4000000000,5000000000,10000000000,20000000000,25000000000]\n",
    "for chunk_size in chunks:\n",
    "    in_order_hardware_counters_features = process_csv_feautures(source, chunk_size)\n",
    "    out_of_order_ipc = process_csv_ipc(target, chunk_size)\n",
    "    print('chunk_size: ' + str(chunk_size))\n",
    "    \n",
    "    data = {\n",
    "        'In Order': in_order_hardware_counters_features,\n",
    "        'Out of Order': out_of_order_ipc\n",
    "    }\n",
    "    \n",
    "    length = min(len(in_order_hardware_counters_features), len(out_of_order_ipc))\n",
    "    \n",
    "    # Shorten the longen array\n",
    "    in_order_hardware_counters_features = in_order_hardware_counters_features[:length]\n",
    "    out_of_order_ipc = out_of_order_ipc[:length]\n",
    "    \n",
    "    # separate features and target\n",
    "    X = pd.DataFrame(\n",
    "        in_order_hardware_counters_features, \n",
    "        columns=[\n",
    "            'instructions',\n",
    "            'cpu-cycles', \n",
    "            'br_pred', \n",
    "            'br_mis_pred', \n",
    "            'l1d_cache', \n",
    "            'l1d_tlb', \n",
    "            'l1d_tlb_rd', \n",
    "            'l1d_tlb_wr', \n",
    "            'l2d_cache_rd', \n",
    "            'l2d_cache_wr', \n",
    "            'l2d_cache', \n",
    "            'l1i_cache', \n",
    "            'l1i_cache_refill', \n",
    "            'remote_access', \n",
    "            'dtlb_walk', \n",
    "            'itlb_walk', \n",
    "            'l1i_tlb', \n",
    "            'l2d_tlb', \n",
    "            'l2d_tlb_rd', \n",
    "            'l2d_tlb_wr', \n",
    "            'vfp_spec', \n",
    "            'inst_spec', \n",
    "            'ase_spec', \n",
    "            'stall_backend', \n",
    "            'stall_frontend', \n",
    "            'll_cache_miss_rd', \n",
    "            'mem_access', \n",
    "            'mem_access_rd', \n",
    "            'mem_access_wr',\n",
    "            'ipc'])  # Features\n",
    "    y = pd.Series(out_of_order_ipc) \n",
    "    \n",
    "    # split \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    rf_model = RandomForestRegressor(n_estimators=100)\n",
    "    gb_model = GradientBoostingRegressor(n_estimators=100)\n",
    "    ensemble_model = gb_model\n",
    "    #ensemble_model = VotingRegressor([('rf', rf_model), ('gb', gb_model)])\n",
    "    ensemble_model.fit(X_train, y_train)\n",
    "    \n",
    "    ensemble_predictions = ensemble_model.predict(X_test)\n",
    "    # Calculate R²\n",
    "    ensemble_r2 = r2_score(y_test, ensemble_predictions)\n",
    "    \n",
    "    #bruh wut\n",
    "    train_predictions = ensemble_model.predict(X_train)\n",
    "    test_predictions = ensemble_model.predict(X_test)\n",
    "    \n",
    "    # Calculate R² for training and testing sets\n",
    "    train_r2 = r2_score(y_train, train_predictions)\n",
    "    train_mape = mean_absolute_percentage_error(y_train, train_predictions)\n",
    "    test_r2 = r2_score(y_test, test_predictions)\n",
    "    test_mape = mean_absolute_percentage_error(y_test, test_predictions)\n",
    "    \n",
    "    \n",
    "    print(f\"Training R²: {train_r2}\")\n",
    "    print(f\"Testing R²: {test_r2}\")\n",
    "    print(f\"Training Mape: {train_mape}\")\n",
    "    print(f\"Testing Mape: {test_mape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3da4ac5-3733-449b-a193-d1a4a8c500cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_size: 1000000000\n",
      "Training R²: 0.705489363697261\n",
      "Testing R²: 0.672208745532567\n",
      "Training Mape: 0.15984310370787497\n",
      "Testing Mape: 0.16386329557480814\n",
      "chunk_size: 2000000000\n",
      "Training R²: 0.748091002508475\n",
      "Testing R²: 0.6756074207767131\n",
      "Training Mape: 0.13765617252380638\n",
      "Testing Mape: 0.15984092625906388\n",
      "chunk_size: 3000000000\n",
      "Training R²: 0.7678786955699797\n",
      "Testing R²: 0.7288229619799038\n",
      "Training Mape: 0.13225171045409453\n",
      "Testing Mape: 0.14034473492557836\n",
      "chunk_size: 4000000000\n",
      "Training R²: 0.786664110502733\n",
      "Testing R²: 0.7129227853965467\n",
      "Training Mape: 0.1279202775882085\n",
      "Testing Mape: 0.14177361329075058\n",
      "chunk_size: 5000000000\n",
      "Training R²: 0.7981954393814171\n",
      "Testing R²: 0.7109333033344922\n",
      "Training Mape: 0.12293347757751975\n",
      "Testing Mape: 0.1331818222234514\n",
      "chunk_size: 10000000000\n",
      "Training R²: 0.8573813907347163\n",
      "Testing R²: 0.7258259850828868\n",
      "Training Mape: 0.10497281137915611\n",
      "Testing Mape: 0.13580802706806483\n",
      "chunk_size: 20000000000\n",
      "Training R²: 0.9000861252692676\n",
      "Testing R²: 0.7481364489065152\n",
      "Training Mape: 0.09210906321654151\n",
      "Testing Mape: 0.1525909126101741\n",
      "chunk_size: 25000000000\n",
      "Training R²: 0.9330785254138441\n",
      "Testing R²: 0.7019728455018128\n",
      "Training Mape: 0.07549170469385375\n",
      "Testing Mape: 0.1423526309452241\n"
     ]
    }
   ],
   "source": [
    "# server to desktop\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "def process_csv_feautures(file_paths, chunk_size):\n",
    "    all_features = []\n",
    "    for file_path in file_paths:\n",
    "        df = pd.read_csv(file_path + '_' + str(chunk_size))\n",
    "        df = df.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "        features = []\n",
    "        for index, row in df.iterrows():\n",
    "            inst = row['instructions']\n",
    "            cyc = row['cpu-cycles']\n",
    "            ipc = inst/cyc\n",
    "            features.append([\n",
    "                row['instructions'],\n",
    "                row['cpu-cycles'], \n",
    "                row['br_pred'], \n",
    "                row['br_mis_pred'], \n",
    "                row['l1d_cache'], \n",
    "                row['l1d_tlb'], \n",
    "                row['l1d_tlb_rd'], \n",
    "                row['l1d_tlb_wr'], \n",
    "                row['l2d_cache_rd'], \n",
    "                row['l2d_cache_wr'], \n",
    "                row['l2d_cache'], \n",
    "                row['l1i_cache'], \n",
    "                row['l1i_cache_refill'], \n",
    "                row['remote_access'], \n",
    "                row['dtlb_walk'], \n",
    "                row['itlb_walk'], \n",
    "                row['l1i_tlb'], \n",
    "                row['l2d_tlb'], \n",
    "                row['l2d_tlb_rd'], \n",
    "                row['l2d_tlb_wr'], \n",
    "                row['vfp_spec'], \n",
    "                row['inst_spec'], \n",
    "                row['ase_spec'], \n",
    "                row['stall_backend'], \n",
    "                row['stall_frontend'], \n",
    "                row['ll_cache_miss_rd'], \n",
    "                row['mem_access'], \n",
    "                row['mem_access_rd'], \n",
    "                row['mem_access_wr'],\n",
    "                ipc])\n",
    "        all_features.extend(features)\n",
    "    return all_features\n",
    "\n",
    "\n",
    "def process_csv_ipc(file_paths, chunk_size):\n",
    "    all_ipc_values = []\n",
    "    for file_path in file_paths:\n",
    "        df = pd.read_csv(file_path + '_' + str(chunk_size))\n",
    "        df = df.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "        for _, row in df.iterrows():\n",
    "            all_ipc_values.append(row['instructions'] / row['cpu-cycles'])  \n",
    "    return all_ipc_values\n",
    "\n",
    "source = [\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_500',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_502',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_505',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_520',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_523',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_525',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_531',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_541',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_548',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_557',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_503',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_507',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_508',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_510',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_511',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_519',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_521',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_527',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_538',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_544',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_549',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_554',\n",
    "]\n",
    "    \n",
    "target = [\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_500',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_502',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_505',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_520',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_523',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_525',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_531',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_541',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_548',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_557',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_503',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_507',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_508',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_510',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_511',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_519',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_521',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_527',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_538',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_544',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_549',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_3.0GHz_554',\n",
    "]\n",
    "\n",
    "chunks = [1000000000,2000000000,3000000000,4000000000,5000000000,10000000000,20000000000,25000000000]\n",
    "for chunk_size in chunks:\n",
    "    in_order_hardware_counters_features = process_csv_feautures(source, chunk_size)\n",
    "    out_of_order_ipc = process_csv_ipc(target, chunk_size)\n",
    "    print('chunk_size: ' + str(chunk_size))\n",
    "    \n",
    "    data = {\n",
    "        'In Order': in_order_hardware_counters_features,\n",
    "        'Out of Order': out_of_order_ipc\n",
    "    }\n",
    "    \n",
    "    length = min(len(in_order_hardware_counters_features), len(out_of_order_ipc))\n",
    "    \n",
    "    # Shorten the longen array\n",
    "    in_order_hardware_counters_features = in_order_hardware_counters_features[:length]\n",
    "    out_of_order_ipc = out_of_order_ipc[:length]\n",
    "    \n",
    "    # separate features and target\n",
    "    X = pd.DataFrame(\n",
    "        in_order_hardware_counters_features, \n",
    "        columns=[\n",
    "            'instructions',\n",
    "            'cpu-cycles', \n",
    "            'br_pred', \n",
    "            'br_mis_pred', \n",
    "            'l1d_cache', \n",
    "            'l1d_tlb', \n",
    "            'l1d_tlb_rd', \n",
    "            'l1d_tlb_wr', \n",
    "            'l2d_cache_rd', \n",
    "            'l2d_cache_wr', \n",
    "            'l2d_cache', \n",
    "            'l1i_cache', \n",
    "            'l1i_cache_refill', \n",
    "            'remote_access', \n",
    "            'dtlb_walk', \n",
    "            'itlb_walk', \n",
    "            'l1i_tlb', \n",
    "            'l2d_tlb', \n",
    "            'l2d_tlb_rd', \n",
    "            'l2d_tlb_wr', \n",
    "            'vfp_spec', \n",
    "            'inst_spec', \n",
    "            'ase_spec', \n",
    "            'stall_backend', \n",
    "            'stall_frontend', \n",
    "            'll_cache_miss_rd', \n",
    "            'mem_access', \n",
    "            'mem_access_rd', \n",
    "            'mem_access_wr',\n",
    "            'ipc'])  # Features\n",
    "    y = pd.Series(out_of_order_ipc) \n",
    "    \n",
    "    # split \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    rf_model = RandomForestRegressor(n_estimators=100)\n",
    "    gb_model = GradientBoostingRegressor(n_estimators=100)\n",
    "    ensemble_model = gb_model\n",
    "    #ensemble_model = VotingRegressor([('rf', rf_model), ('gb', gb_model)])\n",
    "    ensemble_model.fit(X_train, y_train)\n",
    "    \n",
    "    ensemble_predictions = ensemble_model.predict(X_test)\n",
    "    # Calculate R²\n",
    "    ensemble_r2 = r2_score(y_test, ensemble_predictions)\n",
    "    \n",
    "    #bruh wut\n",
    "    train_predictions = ensemble_model.predict(X_train)\n",
    "    test_predictions = ensemble_model.predict(X_test)\n",
    "    \n",
    "    # Calculate R² for training and testing sets\n",
    "    train_r2 = r2_score(y_train, train_predictions)\n",
    "    train_mape = mean_absolute_percentage_error(y_train, train_predictions)\n",
    "    test_r2 = r2_score(y_test, test_predictions)\n",
    "    test_mape = mean_absolute_percentage_error(y_test, test_predictions)\n",
    "    \n",
    "    \n",
    "    print(f\"Training R²: {train_r2}\")\n",
    "    print(f\"Testing R²: {test_r2}\")\n",
    "    print(f\"Training Mape: {train_mape}\")\n",
    "    print(f\"Testing Mape: {test_mape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4742e29-4899-4697-ac5a-3686ece51cdb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

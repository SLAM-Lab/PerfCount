{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93e518bb-30f5-41ce-afb6-2ef8e8d90ad2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_size: 200\n",
      "Training R²: 0.9913012473746192\n",
      "Testing R²: 0.9868485686886534\n",
      "Training Mape: 0.02292627608924239\n",
      "Testing Mape: 0.027968745470932373\n",
      "chunk_size: 400\n",
      "Training R²: 0.944281744575588\n",
      "Testing R²: 0.9066774166282804\n",
      "Training Mape: 0.051784728585945794\n",
      "Testing Mape: 0.06414485839881504\n",
      "chunk_size: 600\n",
      "Training R²: 0.9108954632426258\n",
      "Testing R²: 0.8885947201584979\n",
      "Training Mape: 0.06771206036159785\n",
      "Testing Mape: 0.07589914596517532\n",
      "chunk_size: 800\n",
      "Training R²: 0.8869809718846426\n",
      "Testing R²: 0.8551448579306697\n",
      "Training Mape: 0.07784398939193223\n",
      "Testing Mape: 0.08233339226401971\n",
      "chunk_size: 1000\n",
      "Training R²: 0.8593125170244575\n",
      "Testing R²: 0.842426579848459\n",
      "Training Mape: 0.08758830008685584\n",
      "Testing Mape: 0.09221700890884169\n"
     ]
    }
   ],
   "source": [
    "# InO to OOO\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "def process_csv_feautures(file_paths, chunk_size):\n",
    "    all_features = []\n",
    "    for file_path in file_paths:\n",
    "        df = pd.read_csv(file_path + '_' + str(chunk_size))\n",
    "        df = df.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "        features = []\n",
    "        for index, row in df.iterrows():\n",
    "            inst = row['instructions:u']\n",
    "            cyc = row['cpu-cycles:u']\n",
    "            ipc = inst/cyc\n",
    "            features.append([\n",
    "                row['instructions:u'],\n",
    "                row['cpu-cycles:u'],\n",
    "                row['armv8_pmuv3/stall_backend/u'],\n",
    "                row['armv8_pmuv3/stall_frontend/u'],\n",
    "                row['armv8_pmuv3/br_mis_pred/u'],\n",
    "                row['armv8_pmuv3/br_pred/u'],\n",
    "                row['armv8_pmuv3/br_retired/u'],\n",
    "                row['armv8_pmuv3/inst_spec/u'],\n",
    "                row['armv8_pmuv3/inst_retired/u'],\n",
    "                row['armv8_pmuv3/l1d_cache/u'],\n",
    "                row['armv8_pmuv3/l1d_cache_refill/u'],\n",
    "                row['armv8_pmuv3/l1d_cache_wb/u'],\n",
    "                row['armv8_pmuv3/l1d_tlb/u'],\n",
    "                row['armv8_pmuv3/l1d_tlb_refill/u'],\n",
    "                row['armv8_pmuv3/l1i_cache/u'],\n",
    "                row['armv8_pmuv3/l1i_cache_refill/u'],\n",
    "                row['armv8_pmuv3/l1i_tlb/u'],\n",
    "                row['armv8_pmuv3/l1i_tlb_refill/u'],\n",
    "                row['armv8_pmuv3/l2d_cache/u'],\n",
    "                row['armv8_pmuv3/l2d_cache_allocate/u'],\n",
    "                row['armv8_pmuv3/l2d_cache_refill/u'],\n",
    "                row['armv8_pmuv3/l2d_cache_wb/u'],\n",
    "                row['armv8_pmuv3/l2d_tlb/u'],\n",
    "                row['armv8_pmuv3/l2d_tlb_refill/u'],\n",
    "                row['armv8_pmuv3/l3d_cache/u'],\n",
    "                row['armv8_pmuv3/l3d_cache_allocate/u'],\n",
    "                row['armv8_pmuv3/l3d_cache_refill/u'],\n",
    "                row['armv8_pmuv3/l3d_cache_wb/u'],\n",
    "                row['armv8_pmuv3/mem_access/u'],\n",
    "                ipc])\n",
    "        all_features.extend(features)\n",
    "    return all_features\n",
    "\n",
    "\n",
    "def process_csv_ipc(file_paths, chunk_size):\n",
    "    all_ipc_values = []\n",
    "    for file_path in file_paths:\n",
    "        df = pd.read_csv(file_path + '_' + str(chunk_size))\n",
    "        df = df.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "        for _, row in df.iterrows():\n",
    "            all_ipc_values.append(row['instructions:u'] / row['cpu-cycles:u'])  \n",
    "    return all_ipc_values\n",
    "\n",
    "source = [\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_500',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_502',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_505',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_520',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_523',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_525',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_531',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_541',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_548',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_557',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_503',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_507',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_508',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_510',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_511',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_519',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_521',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_527',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_538',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_544',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_549',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_554',\n",
    "]\n",
    "    \n",
    "target = [\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_500',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_502',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_505',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_520',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_523',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_525',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_531',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_541',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_548',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_557',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_503',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_507',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_508',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_510',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_511',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_519',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_521',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_527',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_538',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_544',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_549',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_554',\n",
    "]\n",
    "\n",
    "chunks = [200, 400, 600, 800, 1000]\n",
    "for chunk_size in chunks:\n",
    "    in_order_hardware_counters_features = process_csv_feautures(source, chunk_size)\n",
    "    out_of_order_ipc = process_csv_ipc(target, chunk_size)\n",
    "    print('chunk_size: ' + str(chunk_size))\n",
    "    \n",
    "    data = {\n",
    "        'In Order': in_order_hardware_counters_features,\n",
    "        'Out of Order': out_of_order_ipc\n",
    "    }\n",
    "    \n",
    "    length = min(len(in_order_hardware_counters_features), len(out_of_order_ipc))\n",
    "    \n",
    "    # Shorten the longen array\n",
    "    in_order_hardware_counters_features = in_order_hardware_counters_features[:length]\n",
    "    out_of_order_ipc = out_of_order_ipc[:length]\n",
    "    \n",
    "    # separate features and target\n",
    "    X = pd.DataFrame(\n",
    "        in_order_hardware_counters_features, \n",
    "        columns=[\n",
    "            'instructions:u',\n",
    "            'cpu-cycles:u',\n",
    "            'armv8_pmuv3/stall_backend/u',\n",
    "            'armv8_pmuv3/stall_frontend/u',\n",
    "            'armv8_pmuv3/br_mis_pred/u',\n",
    "            'armv8_pmuv3/br_pred/u',\n",
    "            'armv8_pmuv3/br_retired/u',\n",
    "            'armv8_pmuv3/inst_spec/u',\n",
    "            'armv8_pmuv3/inst_retired/u',\n",
    "            'armv8_pmuv3/l1d_cache/u',\n",
    "            'armv8_pmuv3/l1d_cache_refill/u',\n",
    "            'armv8_pmuv3/l1d_cache_wb/u',\n",
    "            'armv8_pmuv3/l1d_tlb/u',\n",
    "            'armv8_pmuv3/l1d_tlb_refill/u',\n",
    "            'armv8_pmuv3/l1i_cache/u',\n",
    "            'armv8_pmuv3/l1i_cache_refill/u',\n",
    "            'armv8_pmuv3/l1i_tlb/u',\n",
    "            'armv8_pmuv3/l1i_tlb_refill/u',\n",
    "            'armv8_pmuv3/l2d_cache/u',\n",
    "            'armv8_pmuv3/l2d_cache_allocate/u',\n",
    "            'armv8_pmuv3/l2d_cache_refill/u',\n",
    "            'armv8_pmuv3/l2d_cache_wb/u',\n",
    "            'armv8_pmuv3/l2d_tlb/u',\n",
    "            'armv8_pmuv3/l2d_tlb_refill/u',\n",
    "            'armv8_pmuv3/l3d_cache/u',\n",
    "            'armv8_pmuv3/l3d_cache_allocate/u',\n",
    "            'armv8_pmuv3/l3d_cache_refill/u',\n",
    "            'armv8_pmuv3/l3d_cache_wb/u',\n",
    "            'armv8_pmuv3/mem_access/u',\n",
    "            'ipc'])  # Features\n",
    "    y = pd.Series(out_of_order_ipc) \n",
    "    \n",
    "    # split \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    rf_model = RandomForestRegressor(n_estimators=100)\n",
    "    gb_model = GradientBoostingRegressor(n_estimators=100)\n",
    "    ensemble_model = gb_model\n",
    "    #ensemble_model = VotingRegressor([('rf', rf_model), ('gb', gb_model)])\n",
    "    ensemble_model.fit(X_train, y_train)\n",
    "    \n",
    "    ensemble_predictions = ensemble_model.predict(X_test)\n",
    "    # Calculate R²\n",
    "    ensemble_r2 = r2_score(y_test, ensemble_predictions)\n",
    "    \n",
    "    #bruh wut\n",
    "    train_predictions = ensemble_model.predict(X_train)\n",
    "    test_predictions = ensemble_model.predict(X_test)\n",
    "    \n",
    "    # Calculate R² for training and testing sets\n",
    "    train_r2 = r2_score(y_train, train_predictions)\n",
    "    train_mape = mean_absolute_percentage_error(y_train, train_predictions)\n",
    "    test_r2 = r2_score(y_test, test_predictions)\n",
    "    test_mape = mean_absolute_percentage_error(y_test, test_predictions)\n",
    "    \n",
    "    \n",
    "    print(f\"Training R²: {train_r2}\")\n",
    "    print(f\"Testing R²: {test_r2}\")\n",
    "    print(f\"Training Mape: {train_mape}\")\n",
    "    print(f\"Testing Mape: {test_mape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00f469f5-6b2c-4abc-9e4e-b11967a1b731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_size: 200\n",
      "Training R²: 0.986534096874625\n",
      "Testing R²: 0.9751675441128445\n",
      "Training Mape: 0.02871735575715219\n",
      "Testing Mape: 0.03226719599562379\n",
      "chunk_size: 400\n",
      "Training R²: 0.9533168803461105\n",
      "Testing R²: 0.9383388190554369\n",
      "Training Mape: 0.048416003393265426\n",
      "Testing Mape: 0.05639164320418491\n",
      "chunk_size: 600\n",
      "Training R²: 0.9188472299639803\n",
      "Testing R²: 0.904414968398469\n",
      "Training Mape: 0.06871137703505756\n",
      "Testing Mape: 0.07315157701695552\n",
      "chunk_size: 800\n",
      "Training R²: 0.9046297722771992\n",
      "Testing R²: 0.8844572150812117\n",
      "Training Mape: 0.07842162915179596\n",
      "Testing Mape: 0.0839158917160562\n",
      "chunk_size: 1000\n",
      "Training R²: 0.8900405539508415\n",
      "Testing R²: 0.8746398000634155\n",
      "Training Mape: 0.08536399910834018\n",
      "Testing Mape: 0.08947419764153604\n"
     ]
    }
   ],
   "source": [
    "# InO to desktop\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "def process_csv_feautures(file_paths, chunk_size):\n",
    "    all_features = []\n",
    "    for file_path in file_paths:\n",
    "        df = pd.read_csv(file_path + '_' + str(chunk_size))\n",
    "        df = df.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "        features = []\n",
    "        for index, row in df.iterrows():\n",
    "            inst = row['instructions:u']\n",
    "            cyc = row['cpu-cycles:u']\n",
    "            ipc = inst/cyc\n",
    "            features.append([\n",
    "                row['instructions:u'],\n",
    "                row['cpu-cycles:u'],\n",
    "                row['armv8_pmuv3/stall_backend/u'],\n",
    "                row['armv8_pmuv3/stall_frontend/u'],\n",
    "                row['armv8_pmuv3/br_mis_pred/u'],\n",
    "                row['armv8_pmuv3/br_pred/u'],\n",
    "                row['armv8_pmuv3/br_retired/u'],\n",
    "                row['armv8_pmuv3/inst_spec/u'],\n",
    "                row['armv8_pmuv3/inst_retired/u'],\n",
    "                row['armv8_pmuv3/l1d_cache/u'],\n",
    "                row['armv8_pmuv3/l1d_cache_refill/u'],\n",
    "                row['armv8_pmuv3/l1d_cache_wb/u'],\n",
    "                row['armv8_pmuv3/l1d_tlb/u'],\n",
    "                row['armv8_pmuv3/l1d_tlb_refill/u'],\n",
    "                row['armv8_pmuv3/l1i_cache/u'],\n",
    "                row['armv8_pmuv3/l1i_cache_refill/u'],\n",
    "                row['armv8_pmuv3/l1i_tlb/u'],\n",
    "                row['armv8_pmuv3/l1i_tlb_refill/u'],\n",
    "                row['armv8_pmuv3/l2d_cache/u'],\n",
    "                row['armv8_pmuv3/l2d_cache_allocate/u'],\n",
    "                row['armv8_pmuv3/l2d_cache_refill/u'],\n",
    "                row['armv8_pmuv3/l2d_cache_wb/u'],\n",
    "                row['armv8_pmuv3/l2d_tlb/u'],\n",
    "                row['armv8_pmuv3/l2d_tlb_refill/u'],\n",
    "                row['armv8_pmuv3/l3d_cache/u'],\n",
    "                row['armv8_pmuv3/l3d_cache_allocate/u'],\n",
    "                row['armv8_pmuv3/l3d_cache_refill/u'],\n",
    "                row['armv8_pmuv3/l3d_cache_wb/u'],\n",
    "                row['armv8_pmuv3/mem_access/u'],\n",
    "                ipc])\n",
    "        all_features.extend(features)\n",
    "    return all_features\n",
    "\n",
    "\n",
    "def process_csv_ipc(file_paths, chunk_size):\n",
    "    all_ipc_values = []\n",
    "    for file_path in file_paths:\n",
    "        df = pd.read_csv(file_path + '_' + str(chunk_size))\n",
    "        df = df.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "        for _, row in df.iterrows():\n",
    "            all_ipc_values.append(row['instructions'] / row['cpu-cycles'])  \n",
    "    return all_ipc_values\n",
    "\n",
    "source = [\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_500',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_502',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_505',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_520',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_523',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_525',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_531',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_541',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_548',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_557',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_503',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_507',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_508',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_510',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_511',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_519',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_521',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_527',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_538',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_544',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_549',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_554',\n",
    "]\n",
    "    \n",
    "target = [\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_500',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_502',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_505',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_520',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_523',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_525',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_531',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_541',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_548',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_557',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_503',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_507',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_508',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_510',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_511',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_519',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_521',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_527',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_538',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_544',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_549',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_554',\n",
    "]\n",
    "\n",
    "chunks = [200, 400, 600, 800, 1000]\n",
    "for chunk_size in chunks:\n",
    "    in_order_hardware_counters_features = process_csv_feautures(source, chunk_size)\n",
    "    out_of_order_ipc = process_csv_ipc(target, chunk_size)\n",
    "    print('chunk_size: ' + str(chunk_size))\n",
    "    \n",
    "    data = {\n",
    "        'In Order': in_order_hardware_counters_features,\n",
    "        'Out of Order': out_of_order_ipc\n",
    "    }\n",
    "    \n",
    "    length = min(len(in_order_hardware_counters_features), len(out_of_order_ipc))\n",
    "    \n",
    "    # Shorten the longen array\n",
    "    in_order_hardware_counters_features = in_order_hardware_counters_features[:length]\n",
    "    out_of_order_ipc = out_of_order_ipc[:length]\n",
    "    \n",
    "    # separate features and target\n",
    "    X = pd.DataFrame(\n",
    "        in_order_hardware_counters_features, \n",
    "        columns=[\n",
    "            'instructions:u',\n",
    "            'cpu-cycles:u',\n",
    "            'armv8_pmuv3/stall_backend/u',\n",
    "            'armv8_pmuv3/stall_frontend/u',\n",
    "            'armv8_pmuv3/br_mis_pred/u',\n",
    "            'armv8_pmuv3/br_pred/u',\n",
    "            'armv8_pmuv3/br_retired/u',\n",
    "            'armv8_pmuv3/inst_spec/u',\n",
    "            'armv8_pmuv3/inst_retired/u',\n",
    "            'armv8_pmuv3/l1d_cache/u',\n",
    "            'armv8_pmuv3/l1d_cache_refill/u',\n",
    "            'armv8_pmuv3/l1d_cache_wb/u',\n",
    "            'armv8_pmuv3/l1d_tlb/u',\n",
    "            'armv8_pmuv3/l1d_tlb_refill/u',\n",
    "            'armv8_pmuv3/l1i_cache/u',\n",
    "            'armv8_pmuv3/l1i_cache_refill/u',\n",
    "            'armv8_pmuv3/l1i_tlb/u',\n",
    "            'armv8_pmuv3/l1i_tlb_refill/u',\n",
    "            'armv8_pmuv3/l2d_cache/u',\n",
    "            'armv8_pmuv3/l2d_cache_allocate/u',\n",
    "            'armv8_pmuv3/l2d_cache_refill/u',\n",
    "            'armv8_pmuv3/l2d_cache_wb/u',\n",
    "            'armv8_pmuv3/l2d_tlb/u',\n",
    "            'armv8_pmuv3/l2d_tlb_refill/u',\n",
    "            'armv8_pmuv3/l3d_cache/u',\n",
    "            'armv8_pmuv3/l3d_cache_allocate/u',\n",
    "            'armv8_pmuv3/l3d_cache_refill/u',\n",
    "            'armv8_pmuv3/l3d_cache_wb/u',\n",
    "            'armv8_pmuv3/mem_access/u',\n",
    "            'ipc'])  # Features\n",
    "    y = pd.Series(out_of_order_ipc) \n",
    "    \n",
    "    # split \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    rf_model = RandomForestRegressor(n_estimators=100)\n",
    "    gb_model = GradientBoostingRegressor(n_estimators=100)\n",
    "    ensemble_model = gb_model\n",
    "    #ensemble_model = VotingRegressor([('rf', rf_model), ('gb', gb_model)])\n",
    "    ensemble_model.fit(X_train, y_train)\n",
    "    \n",
    "    ensemble_predictions = ensemble_model.predict(X_test)\n",
    "    # Calculate R²\n",
    "    ensemble_r2 = r2_score(y_test, ensemble_predictions)\n",
    "    \n",
    "    #bruh wut\n",
    "    train_predictions = ensemble_model.predict(X_train)\n",
    "    test_predictions = ensemble_model.predict(X_test)\n",
    "    \n",
    "    # Calculate R² for training and testing sets\n",
    "    train_r2 = r2_score(y_train, train_predictions)\n",
    "    train_mape = mean_absolute_percentage_error(y_train, train_predictions)\n",
    "    test_r2 = r2_score(y_test, test_predictions)\n",
    "    test_mape = mean_absolute_percentage_error(y_test, test_predictions)\n",
    "    \n",
    "    \n",
    "    print(f\"Training R²: {train_r2}\")\n",
    "    print(f\"Testing R²: {test_r2}\")\n",
    "    print(f\"Training Mape: {train_mape}\")\n",
    "    print(f\"Testing Mape: {test_mape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0cf61275-6147-4e9f-819b-2b49da8d4f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_size: 200\n",
      "Training R²: 0.9803388430909404\n",
      "Testing R²: 0.971101204684222\n",
      "Training Mape: 0.025276912655404393\n",
      "Testing Mape: 0.030528780231993975\n",
      "chunk_size: 400\n",
      "Training R²: 0.9197406488297424\n",
      "Testing R²: 0.8762491801593958\n",
      "Training Mape: 0.04844743139477376\n",
      "Testing Mape: 0.05674320404097222\n",
      "chunk_size: 600\n",
      "Training R²: 0.8772356639526557\n",
      "Testing R²: 0.8650551396893699\n",
      "Training Mape: 0.06214332224666701\n",
      "Testing Mape: 0.06639898467982713\n",
      "chunk_size: 800\n",
      "Training R²: 0.8512823619291654\n",
      "Testing R²: 0.8281825264385427\n",
      "Training Mape: 0.07047904567398386\n",
      "Testing Mape: 0.07505374051805816\n",
      "chunk_size: 1000\n",
      "Training R²: 0.821395408704361\n",
      "Testing R²: 0.8102495955616588\n",
      "Training Mape: 0.08108366340515416\n",
      "Testing Mape: 0.08348434479697467\n"
     ]
    }
   ],
   "source": [
    "# InO to server\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "def process_csv_feautures(file_paths, chunk_size):\n",
    "    all_features = []\n",
    "    for file_path in file_paths:\n",
    "        df = pd.read_csv(file_path + '_' + str(chunk_size))\n",
    "        df = df.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "        features = []\n",
    "        for index, row in df.iterrows():\n",
    "            inst = row['instructions:u']\n",
    "            cyc = row['cpu-cycles:u']\n",
    "            ipc = inst/cyc\n",
    "            features.append([\n",
    "                row['instructions:u'],\n",
    "                row['cpu-cycles:u'],\n",
    "                row['armv8_pmuv3/stall_backend/u'],\n",
    "                row['armv8_pmuv3/stall_frontend/u'],\n",
    "                row['armv8_pmuv3/br_mis_pred/u'],\n",
    "                row['armv8_pmuv3/br_pred/u'],\n",
    "                row['armv8_pmuv3/br_retired/u'],\n",
    "                row['armv8_pmuv3/inst_spec/u'],\n",
    "                row['armv8_pmuv3/inst_retired/u'],\n",
    "                row['armv8_pmuv3/l1d_cache/u'],\n",
    "                row['armv8_pmuv3/l1d_cache_refill/u'],\n",
    "                row['armv8_pmuv3/l1d_cache_wb/u'],\n",
    "                row['armv8_pmuv3/l1d_tlb/u'],\n",
    "                row['armv8_pmuv3/l1d_tlb_refill/u'],\n",
    "                row['armv8_pmuv3/l1i_cache/u'],\n",
    "                row['armv8_pmuv3/l1i_cache_refill/u'],\n",
    "                row['armv8_pmuv3/l1i_tlb/u'],\n",
    "                row['armv8_pmuv3/l1i_tlb_refill/u'],\n",
    "                row['armv8_pmuv3/l2d_cache/u'],\n",
    "                row['armv8_pmuv3/l2d_cache_allocate/u'],\n",
    "                row['armv8_pmuv3/l2d_cache_refill/u'],\n",
    "                row['armv8_pmuv3/l2d_cache_wb/u'],\n",
    "                row['armv8_pmuv3/l2d_tlb/u'],\n",
    "                row['armv8_pmuv3/l2d_tlb_refill/u'],\n",
    "                row['armv8_pmuv3/l3d_cache/u'],\n",
    "                row['armv8_pmuv3/l3d_cache_allocate/u'],\n",
    "                row['armv8_pmuv3/l3d_cache_refill/u'],\n",
    "                row['armv8_pmuv3/l3d_cache_wb/u'],\n",
    "                row['armv8_pmuv3/mem_access/u'],\n",
    "                ipc])\n",
    "        all_features.extend(features)\n",
    "    return all_features\n",
    "\n",
    "\n",
    "def process_csv_ipc(file_paths, chunk_size):\n",
    "    all_ipc_values = []\n",
    "    for file_path in file_paths:\n",
    "        df = pd.read_csv(file_path + '_' + str(chunk_size))\n",
    "        df = df.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "        for _, row in df.iterrows():\n",
    "            all_ipc_values.append(row['instructions'] / row['cpu-cycles'])  \n",
    "    return all_ipc_values\n",
    "\n",
    "source = [\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_500',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_502',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_505',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_520',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_523',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_525',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_531',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_541',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_548',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_557',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_503',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_507',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_508',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_510',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_511',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_519',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_521',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_527',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_538',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_544',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_549',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_554',\n",
    "]\n",
    "    \n",
    "target = [\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_500',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_502',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_505',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_520',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_523',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_525',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_531',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_541',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_548',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_557',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_503',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_507',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_508',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_510',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_511',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_519',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_521',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_527',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_538',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_544',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_549',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_554',\n",
    "]\n",
    "\n",
    "chunks = [200, 400, 600, 800, 1000]\n",
    "for chunk_size in chunks:\n",
    "    in_order_hardware_counters_features = process_csv_feautures(source, chunk_size)\n",
    "    out_of_order_ipc = process_csv_ipc(target, chunk_size)\n",
    "    print('chunk_size: ' + str(chunk_size))\n",
    "    \n",
    "    data = {\n",
    "        'In Order': in_order_hardware_counters_features,\n",
    "        'Out of Order': out_of_order_ipc\n",
    "    }\n",
    "    \n",
    "    length = min(len(in_order_hardware_counters_features), len(out_of_order_ipc))\n",
    "    \n",
    "    # Shorten the longen array\n",
    "    in_order_hardware_counters_features = in_order_hardware_counters_features[:length]\n",
    "    out_of_order_ipc = out_of_order_ipc[:length]\n",
    "    \n",
    "    # separate features and target\n",
    "    X = pd.DataFrame(\n",
    "        in_order_hardware_counters_features, \n",
    "        columns=[\n",
    "            'instructions:u',\n",
    "            'cpu-cycles:u',\n",
    "            'armv8_pmuv3/stall_backend/u',\n",
    "            'armv8_pmuv3/stall_frontend/u',\n",
    "            'armv8_pmuv3/br_mis_pred/u',\n",
    "            'armv8_pmuv3/br_pred/u',\n",
    "            'armv8_pmuv3/br_retired/u',\n",
    "            'armv8_pmuv3/inst_spec/u',\n",
    "            'armv8_pmuv3/inst_retired/u',\n",
    "            'armv8_pmuv3/l1d_cache/u',\n",
    "            'armv8_pmuv3/l1d_cache_refill/u',\n",
    "            'armv8_pmuv3/l1d_cache_wb/u',\n",
    "            'armv8_pmuv3/l1d_tlb/u',\n",
    "            'armv8_pmuv3/l1d_tlb_refill/u',\n",
    "            'armv8_pmuv3/l1i_cache/u',\n",
    "            'armv8_pmuv3/l1i_cache_refill/u',\n",
    "            'armv8_pmuv3/l1i_tlb/u',\n",
    "            'armv8_pmuv3/l1i_tlb_refill/u',\n",
    "            'armv8_pmuv3/l2d_cache/u',\n",
    "            'armv8_pmuv3/l2d_cache_allocate/u',\n",
    "            'armv8_pmuv3/l2d_cache_refill/u',\n",
    "            'armv8_pmuv3/l2d_cache_wb/u',\n",
    "            'armv8_pmuv3/l2d_tlb/u',\n",
    "            'armv8_pmuv3/l2d_tlb_refill/u',\n",
    "            'armv8_pmuv3/l3d_cache/u',\n",
    "            'armv8_pmuv3/l3d_cache_allocate/u',\n",
    "            'armv8_pmuv3/l3d_cache_refill/u',\n",
    "            'armv8_pmuv3/l3d_cache_wb/u',\n",
    "            'armv8_pmuv3/mem_access/u',\n",
    "            'ipc'])  # Features\n",
    "    y = pd.Series(out_of_order_ipc) \n",
    "    \n",
    "    # split \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    rf_model = RandomForestRegressor(n_estimators=100)\n",
    "    gb_model = GradientBoostingRegressor(n_estimators=100)\n",
    "    ensemble_model = gb_model\n",
    "    #ensemble_model = VotingRegressor([('rf', rf_model), ('gb', gb_model)])\n",
    "    ensemble_model.fit(X_train, y_train)\n",
    "    \n",
    "    ensemble_predictions = ensemble_model.predict(X_test)\n",
    "    # Calculate R²\n",
    "    ensemble_r2 = r2_score(y_test, ensemble_predictions)\n",
    "    \n",
    "    #bruh wut\n",
    "    train_predictions = ensemble_model.predict(X_train)\n",
    "    test_predictions = ensemble_model.predict(X_test)\n",
    "    \n",
    "    # Calculate R² for training and testing sets\n",
    "    train_r2 = r2_score(y_train, train_predictions)\n",
    "    train_mape = mean_absolute_percentage_error(y_train, train_predictions)\n",
    "    test_r2 = r2_score(y_test, test_predictions)\n",
    "    test_mape = mean_absolute_percentage_error(y_test, test_predictions)\n",
    "    \n",
    "    \n",
    "    print(f\"Training R²: {train_r2}\")\n",
    "    print(f\"Testing R²: {test_r2}\")\n",
    "    print(f\"Training Mape: {train_mape}\")\n",
    "    print(f\"Testing Mape: {test_mape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7568794-df2e-4edf-9fe3-074ff0c5bf84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_size: 200\n",
      "Training R²: 0.9931870904190125\n",
      "Testing R²: 0.9883070439318633\n",
      "Training Mape: 0.03286739042561342\n",
      "Testing Mape: 0.03993141906018072\n",
      "chunk_size: 400\n",
      "Training R²: 0.9390921716974631\n",
      "Testing R²: 0.8830574276275086\n",
      "Training Mape: 0.08338740085616547\n",
      "Testing Mape: 0.0998795349860269\n",
      "chunk_size: 600\n",
      "Training R²: 0.9030523113745973\n",
      "Testing R²: 0.8673126346291912\n",
      "Training Mape: 0.11833261557693703\n",
      "Testing Mape: 0.12840399613983952\n",
      "chunk_size: 800\n",
      "Training R²: 0.8832259117020909\n",
      "Testing R²: 0.8545163120247757\n",
      "Training Mape: 0.13240370207968383\n",
      "Testing Mape: 0.14442972044936533\n",
      "chunk_size: 1000\n",
      "Training R²: 0.8597513258273279\n",
      "Testing R²: 0.8517555707432907\n",
      "Training Mape: 0.1484880096633163\n",
      "Testing Mape: 0.15133087972518366\n"
     ]
    }
   ],
   "source": [
    "# OOO to InO\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "def process_csv_feautures(file_paths, chunk_size):\n",
    "    all_features = []\n",
    "    for file_path in file_paths:\n",
    "        df = pd.read_csv(file_path + '_' + str(chunk_size))\n",
    "        df = df.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "        features = []\n",
    "        for index, row in df.iterrows():\n",
    "            inst = row['instructions:u']\n",
    "            cyc = row['cpu-cycles:u']\n",
    "            ipc = inst/cyc\n",
    "            features.append([\n",
    "                row['instructions:u'],\n",
    "                row['cpu-cycles:u'],\n",
    "                row['armv8_pmuv3/stall_backend/u'],\n",
    "                row['armv8_pmuv3/stall_frontend/u'],\n",
    "                row['armv8_pmuv3/br_mis_pred/u'],\n",
    "                row['armv8_pmuv3/br_pred/u'],\n",
    "                row['armv8_pmuv3/br_retired/u'],\n",
    "                row['armv8_pmuv3/inst_spec/u'],\n",
    "                row['armv8_pmuv3/inst_retired/u'],\n",
    "                row['armv8_pmuv3/l1d_cache/u'],\n",
    "                row['armv8_pmuv3/l1d_cache_refill/u'],\n",
    "                row['armv8_pmuv3/l1d_cache_wb/u'],\n",
    "                row['armv8_pmuv3/l1d_tlb/u'],\n",
    "                row['armv8_pmuv3/l1d_tlb_refill/u'],\n",
    "                row['armv8_pmuv3/l1i_cache/u'],\n",
    "                row['armv8_pmuv3/l1i_cache_refill/u'],\n",
    "                row['armv8_pmuv3/l1i_tlb/u'],\n",
    "                row['armv8_pmuv3/l1i_tlb_refill/u'],\n",
    "                row['armv8_pmuv3/l2d_cache/u'],\n",
    "                row['armv8_pmuv3/l2d_cache_allocate/u'],\n",
    "                row['armv8_pmuv3/l2d_cache_refill/u'],\n",
    "                row['armv8_pmuv3/l2d_cache_wb/u'],\n",
    "                row['armv8_pmuv3/l2d_tlb/u'],\n",
    "                row['armv8_pmuv3/l2d_tlb_refill/u'],\n",
    "                row['armv8_pmuv3/l3d_cache/u'],\n",
    "                row['armv8_pmuv3/l3d_cache_allocate/u'],\n",
    "                row['armv8_pmuv3/l3d_cache_refill/u'],\n",
    "                row['armv8_pmuv3/l3d_cache_wb/u'],\n",
    "                row['armv8_pmuv3/mem_access/u'],\n",
    "                ipc])\n",
    "        all_features.extend(features)\n",
    "    return all_features\n",
    "\n",
    "\n",
    "def process_csv_ipc(file_paths, chunk_size):\n",
    "    all_ipc_values = []\n",
    "    for file_path in file_paths:\n",
    "        df = pd.read_csv(file_path + '_' + str(chunk_size))\n",
    "        df = df.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "        for _, row in df.iterrows():\n",
    "            all_ipc_values.append(row['instructions:u'] / row['cpu-cycles:u'])  \n",
    "    return all_ipc_values\n",
    "\n",
    "source = [\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_500',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_502',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_505',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_520',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_523',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_525',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_531',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_541',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_548',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_557',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_503',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_507',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_508',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_510',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_511',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_519',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_521',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_527',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_538',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_544',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_549',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_554',\n",
    "]\n",
    "    \n",
    "target = [\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_500',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_502',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_505',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_520',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_523',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_525',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_531',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_541',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_548',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_557',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_503',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_507',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_508',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_510',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_511',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_519',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_521',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_527',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_538',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_544',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_549',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_554',\n",
    "]\n",
    "\n",
    "chunks = [200, 400, 600, 800, 1000]\n",
    "for chunk_size in chunks:\n",
    "    in_order_hardware_counters_features = process_csv_feautures(source, chunk_size)\n",
    "    out_of_order_ipc = process_csv_ipc(target, chunk_size)\n",
    "    print('chunk_size: ' + str(chunk_size))\n",
    "    \n",
    "    data = {\n",
    "        'In Order': in_order_hardware_counters_features,\n",
    "        'Out of Order': out_of_order_ipc\n",
    "    }\n",
    "    \n",
    "    length = min(len(in_order_hardware_counters_features), len(out_of_order_ipc))\n",
    "    \n",
    "    # Shorten the longen array\n",
    "    in_order_hardware_counters_features = in_order_hardware_counters_features[:length]\n",
    "    out_of_order_ipc = out_of_order_ipc[:length]\n",
    "    \n",
    "    # separate features and target\n",
    "    X = pd.DataFrame(\n",
    "        in_order_hardware_counters_features, \n",
    "        columns=[\n",
    "            'instructions:u',\n",
    "            'cpu-cycles:u',\n",
    "            'armv8_pmuv3/stall_backend/u',\n",
    "            'armv8_pmuv3/stall_frontend/u',\n",
    "            'armv8_pmuv3/br_mis_pred/u',\n",
    "            'armv8_pmuv3/br_pred/u',\n",
    "            'armv8_pmuv3/br_retired/u',\n",
    "            'armv8_pmuv3/inst_spec/u',\n",
    "            'armv8_pmuv3/inst_retired/u',\n",
    "            'armv8_pmuv3/l1d_cache/u',\n",
    "            'armv8_pmuv3/l1d_cache_refill/u',\n",
    "            'armv8_pmuv3/l1d_cache_wb/u',\n",
    "            'armv8_pmuv3/l1d_tlb/u',\n",
    "            'armv8_pmuv3/l1d_tlb_refill/u',\n",
    "            'armv8_pmuv3/l1i_cache/u',\n",
    "            'armv8_pmuv3/l1i_cache_refill/u',\n",
    "            'armv8_pmuv3/l1i_tlb/u',\n",
    "            'armv8_pmuv3/l1i_tlb_refill/u',\n",
    "            'armv8_pmuv3/l2d_cache/u',\n",
    "            'armv8_pmuv3/l2d_cache_allocate/u',\n",
    "            'armv8_pmuv3/l2d_cache_refill/u',\n",
    "            'armv8_pmuv3/l2d_cache_wb/u',\n",
    "            'armv8_pmuv3/l2d_tlb/u',\n",
    "            'armv8_pmuv3/l2d_tlb_refill/u',\n",
    "            'armv8_pmuv3/l3d_cache/u',\n",
    "            'armv8_pmuv3/l3d_cache_allocate/u',\n",
    "            'armv8_pmuv3/l3d_cache_refill/u',\n",
    "            'armv8_pmuv3/l3d_cache_wb/u',\n",
    "            'armv8_pmuv3/mem_access/u',\n",
    "            'ipc'])  # Features\n",
    "    y = pd.Series(out_of_order_ipc) \n",
    "    \n",
    "    # split \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    rf_model = RandomForestRegressor(n_estimators=100)\n",
    "    gb_model = GradientBoostingRegressor(n_estimators=100)\n",
    "    ensemble_model = gb_model\n",
    "    #ensemble_model = VotingRegressor([('rf', rf_model), ('gb', gb_model)])\n",
    "    ensemble_model.fit(X_train, y_train)\n",
    "    \n",
    "    ensemble_predictions = ensemble_model.predict(X_test)\n",
    "    # Calculate R²\n",
    "    ensemble_r2 = r2_score(y_test, ensemble_predictions)\n",
    "    \n",
    "    #bruh wut\n",
    "    train_predictions = ensemble_model.predict(X_train)\n",
    "    test_predictions = ensemble_model.predict(X_test)\n",
    "    \n",
    "    # Calculate R² for training and testing sets\n",
    "    train_r2 = r2_score(y_train, train_predictions)\n",
    "    train_mape = mean_absolute_percentage_error(y_train, train_predictions)\n",
    "    test_r2 = r2_score(y_test, test_predictions)\n",
    "    test_mape = mean_absolute_percentage_error(y_test, test_predictions)\n",
    "    \n",
    "    \n",
    "    print(f\"Training R²: {train_r2}\")\n",
    "    print(f\"Testing R²: {test_r2}\")\n",
    "    print(f\"Training Mape: {train_mape}\")\n",
    "    print(f\"Testing Mape: {test_mape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f01428c-6bb4-456a-bfef-8d37b8b7b34c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_size: 200\n",
      "Training R²: 0.9820593782209812\n",
      "Testing R²: 0.9697673830561647\n",
      "Training Mape: 0.03176374484046091\n",
      "Testing Mape: 0.03704360520029021\n",
      "chunk_size: 400\n",
      "Training R²: 0.9441580001325022\n",
      "Testing R²: 0.9069341055401812\n",
      "Training Mape: 0.05629138720818884\n",
      "Testing Mape: 0.0655394411058626\n",
      "chunk_size: 600\n",
      "Training R²: 0.9127005181263393\n",
      "Testing R²: 0.9064500886118066\n",
      "Training Mape: 0.07086157638407796\n",
      "Testing Mape: 0.07195454128044998\n",
      "chunk_size: 800\n",
      "Training R²: 0.8961783891793188\n",
      "Testing R²: 0.8864692500308409\n",
      "Training Mape: 0.08062976503935256\n",
      "Testing Mape: 0.08550211552713717\n",
      "chunk_size: 1000\n",
      "Training R²: 0.8774881117267449\n",
      "Testing R²: 0.8658562769257003\n",
      "Training Mape: 0.0900138619331687\n",
      "Testing Mape: 0.0940298462610939\n"
     ]
    }
   ],
   "source": [
    "# OOO to desktop\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "def process_csv_feautures(file_paths, chunk_size):\n",
    "    all_features = []\n",
    "    for file_path in file_paths:\n",
    "        df = pd.read_csv(file_path + '_' + str(chunk_size))\n",
    "        df = df.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "        features = []\n",
    "        for index, row in df.iterrows():\n",
    "            inst = row['instructions:u']\n",
    "            cyc = row['cpu-cycles:u']\n",
    "            ipc = inst/cyc\n",
    "            features.append([\n",
    "                row['instructions:u'],\n",
    "                row['cpu-cycles:u'],\n",
    "                row['armv8_pmuv3/stall_backend/u'],\n",
    "                row['armv8_pmuv3/stall_frontend/u'],\n",
    "                row['armv8_pmuv3/br_mis_pred/u'],\n",
    "                row['armv8_pmuv3/br_pred/u'],\n",
    "                row['armv8_pmuv3/br_retired/u'],\n",
    "                row['armv8_pmuv3/inst_spec/u'],\n",
    "                row['armv8_pmuv3/inst_retired/u'],\n",
    "                row['armv8_pmuv3/l1d_cache/u'],\n",
    "                row['armv8_pmuv3/l1d_cache_refill/u'],\n",
    "                row['armv8_pmuv3/l1d_cache_wb/u'],\n",
    "                row['armv8_pmuv3/l1d_tlb/u'],\n",
    "                row['armv8_pmuv3/l1d_tlb_refill/u'],\n",
    "                row['armv8_pmuv3/l1i_cache/u'],\n",
    "                row['armv8_pmuv3/l1i_cache_refill/u'],\n",
    "                row['armv8_pmuv3/l1i_tlb/u'],\n",
    "                row['armv8_pmuv3/l1i_tlb_refill/u'],\n",
    "                row['armv8_pmuv3/l2d_cache/u'],\n",
    "                row['armv8_pmuv3/l2d_cache_allocate/u'],\n",
    "                row['armv8_pmuv3/l2d_cache_refill/u'],\n",
    "                row['armv8_pmuv3/l2d_cache_wb/u'],\n",
    "                row['armv8_pmuv3/l2d_tlb/u'],\n",
    "                row['armv8_pmuv3/l2d_tlb_refill/u'],\n",
    "                row['armv8_pmuv3/l3d_cache/u'],\n",
    "                row['armv8_pmuv3/l3d_cache_allocate/u'],\n",
    "                row['armv8_pmuv3/l3d_cache_refill/u'],\n",
    "                row['armv8_pmuv3/l3d_cache_wb/u'],\n",
    "                row['armv8_pmuv3/mem_access/u'],\n",
    "                ipc])\n",
    "        all_features.extend(features)\n",
    "    return all_features\n",
    "\n",
    "\n",
    "def process_csv_ipc(file_paths, chunk_size):\n",
    "    all_ipc_values = []\n",
    "    for file_path in file_paths:\n",
    "        df = pd.read_csv(file_path + '_' + str(chunk_size))\n",
    "        df = df.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "        for _, row in df.iterrows():\n",
    "            all_ipc_values.append(row['instructions'] / row['cpu-cycles'])  \n",
    "    return all_ipc_values\n",
    "\n",
    "source = [\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_500',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_502',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_505',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_520',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_523',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_525',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_531',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_541',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_548',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_557',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_503',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_507',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_508',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_510',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_511',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_519',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_521',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_527',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_538',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_544',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_549',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_554',\n",
    "]\n",
    "    \n",
    "target = [\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_500',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_502',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_505',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_520',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_523',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_525',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_531',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_541',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_548',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_557',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_503',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_507',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_508',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_510',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_511',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_519',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_521',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_527',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_538',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_544',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_549',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_554',\n",
    "]\n",
    "\n",
    "chunks = [200, 400, 600, 800, 1000]\n",
    "for chunk_size in chunks:\n",
    "    in_order_hardware_counters_features = process_csv_feautures(source, chunk_size)\n",
    "    out_of_order_ipc = process_csv_ipc(target, chunk_size)\n",
    "    print('chunk_size: ' + str(chunk_size))\n",
    "    \n",
    "    data = {\n",
    "        'In Order': in_order_hardware_counters_features,\n",
    "        'Out of Order': out_of_order_ipc\n",
    "    }\n",
    "    \n",
    "    length = min(len(in_order_hardware_counters_features), len(out_of_order_ipc))\n",
    "    \n",
    "    # Shorten the longen array\n",
    "    in_order_hardware_counters_features = in_order_hardware_counters_features[:length]\n",
    "    out_of_order_ipc = out_of_order_ipc[:length]\n",
    "    \n",
    "    # separate features and target\n",
    "    X = pd.DataFrame(\n",
    "        in_order_hardware_counters_features, \n",
    "        columns=[\n",
    "            'instructions:u',\n",
    "            'cpu-cycles:u',\n",
    "            'armv8_pmuv3/stall_backend/u',\n",
    "            'armv8_pmuv3/stall_frontend/u',\n",
    "            'armv8_pmuv3/br_mis_pred/u',\n",
    "            'armv8_pmuv3/br_pred/u',\n",
    "            'armv8_pmuv3/br_retired/u',\n",
    "            'armv8_pmuv3/inst_spec/u',\n",
    "            'armv8_pmuv3/inst_retired/u',\n",
    "            'armv8_pmuv3/l1d_cache/u',\n",
    "            'armv8_pmuv3/l1d_cache_refill/u',\n",
    "            'armv8_pmuv3/l1d_cache_wb/u',\n",
    "            'armv8_pmuv3/l1d_tlb/u',\n",
    "            'armv8_pmuv3/l1d_tlb_refill/u',\n",
    "            'armv8_pmuv3/l1i_cache/u',\n",
    "            'armv8_pmuv3/l1i_cache_refill/u',\n",
    "            'armv8_pmuv3/l1i_tlb/u',\n",
    "            'armv8_pmuv3/l1i_tlb_refill/u',\n",
    "            'armv8_pmuv3/l2d_cache/u',\n",
    "            'armv8_pmuv3/l2d_cache_allocate/u',\n",
    "            'armv8_pmuv3/l2d_cache_refill/u',\n",
    "            'armv8_pmuv3/l2d_cache_wb/u',\n",
    "            'armv8_pmuv3/l2d_tlb/u',\n",
    "            'armv8_pmuv3/l2d_tlb_refill/u',\n",
    "            'armv8_pmuv3/l3d_cache/u',\n",
    "            'armv8_pmuv3/l3d_cache_allocate/u',\n",
    "            'armv8_pmuv3/l3d_cache_refill/u',\n",
    "            'armv8_pmuv3/l3d_cache_wb/u',\n",
    "            'armv8_pmuv3/mem_access/u',\n",
    "            'ipc'])  # Features\n",
    "    y = pd.Series(out_of_order_ipc) \n",
    "    \n",
    "    # split \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    rf_model = RandomForestRegressor(n_estimators=100)\n",
    "    gb_model = GradientBoostingRegressor(n_estimators=100)\n",
    "    ensemble_model = gb_model\n",
    "    #ensemble_model = VotingRegressor([('rf', rf_model), ('gb', gb_model)])\n",
    "    ensemble_model.fit(X_train, y_train)\n",
    "    \n",
    "    ensemble_predictions = ensemble_model.predict(X_test)\n",
    "    # Calculate R²\n",
    "    ensemble_r2 = r2_score(y_test, ensemble_predictions)\n",
    "    \n",
    "    #bruh wut\n",
    "    train_predictions = ensemble_model.predict(X_train)\n",
    "    test_predictions = ensemble_model.predict(X_test)\n",
    "    \n",
    "    # Calculate R² for training and testing sets\n",
    "    train_r2 = r2_score(y_train, train_predictions)\n",
    "    train_mape = mean_absolute_percentage_error(y_train, train_predictions)\n",
    "    test_r2 = r2_score(y_test, test_predictions)\n",
    "    test_mape = mean_absolute_percentage_error(y_test, test_predictions)\n",
    "    \n",
    "    \n",
    "    print(f\"Training R²: {train_r2}\")\n",
    "    print(f\"Testing R²: {test_r2}\")\n",
    "    print(f\"Training Mape: {train_mape}\")\n",
    "    print(f\"Testing Mape: {test_mape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06a70f2c-cf59-42c3-b5a5-4314d11da410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_size: 200\n",
      "Training R²: 0.9853871464321969\n",
      "Testing R²: 0.9773144898523457\n",
      "Training Mape: 0.020944644048750863\n",
      "Testing Mape: 0.024590049071805172\n",
      "chunk_size: 400\n",
      "Training R²: 0.9479453134872452\n",
      "Testing R²: 0.9279484104650808\n",
      "Training Mape: 0.03840870721561051\n",
      "Testing Mape: 0.04271688875206402\n",
      "chunk_size: 600\n",
      "Training R²: 0.925361906678976\n",
      "Testing R²: 0.910593667925253\n",
      "Training Mape: 0.04751269636096779\n",
      "Testing Mape: 0.05111975115793863\n",
      "chunk_size: 800\n",
      "Training R²: 0.907749458254958\n",
      "Testing R²: 0.8830923859081198\n",
      "Training Mape: 0.05419376603742329\n",
      "Testing Mape: 0.059783182364563515\n",
      "chunk_size: 1000\n",
      "Training R²: 0.8842327753521003\n",
      "Testing R²: 0.872973592736151\n",
      "Training Mape: 0.061303171378770324\n",
      "Testing Mape: 0.0651861443540396\n"
     ]
    }
   ],
   "source": [
    "# OOO to server\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "def process_csv_feautures(file_paths, chunk_size):\n",
    "    all_features = []\n",
    "    for file_path in file_paths:\n",
    "        df = pd.read_csv(file_path + '_' + str(chunk_size))\n",
    "        df = df.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "        features = []\n",
    "        for index, row in df.iterrows():\n",
    "            inst = row['instructions:u']\n",
    "            cyc = row['cpu-cycles:u']\n",
    "            ipc = inst/cyc\n",
    "            features.append([\n",
    "                row['instructions:u'],\n",
    "                row['cpu-cycles:u'],\n",
    "                row['armv8_pmuv3/stall_backend/u'],\n",
    "                row['armv8_pmuv3/stall_frontend/u'],\n",
    "                row['armv8_pmuv3/br_mis_pred/u'],\n",
    "                row['armv8_pmuv3/br_pred/u'],\n",
    "                row['armv8_pmuv3/br_retired/u'],\n",
    "                row['armv8_pmuv3/inst_spec/u'],\n",
    "                row['armv8_pmuv3/inst_retired/u'],\n",
    "                row['armv8_pmuv3/l1d_cache/u'],\n",
    "                row['armv8_pmuv3/l1d_cache_refill/u'],\n",
    "                row['armv8_pmuv3/l1d_cache_wb/u'],\n",
    "                row['armv8_pmuv3/l1d_tlb/u'],\n",
    "                row['armv8_pmuv3/l1d_tlb_refill/u'],\n",
    "                row['armv8_pmuv3/l1i_cache/u'],\n",
    "                row['armv8_pmuv3/l1i_cache_refill/u'],\n",
    "                row['armv8_pmuv3/l1i_tlb/u'],\n",
    "                row['armv8_pmuv3/l1i_tlb_refill/u'],\n",
    "                row['armv8_pmuv3/l2d_cache/u'],\n",
    "                row['armv8_pmuv3/l2d_cache_allocate/u'],\n",
    "                row['armv8_pmuv3/l2d_cache_refill/u'],\n",
    "                row['armv8_pmuv3/l2d_cache_wb/u'],\n",
    "                row['armv8_pmuv3/l2d_tlb/u'],\n",
    "                row['armv8_pmuv3/l2d_tlb_refill/u'],\n",
    "                row['armv8_pmuv3/l3d_cache/u'],\n",
    "                row['armv8_pmuv3/l3d_cache_allocate/u'],\n",
    "                row['armv8_pmuv3/l3d_cache_refill/u'],\n",
    "                row['armv8_pmuv3/l3d_cache_wb/u'],\n",
    "                row['armv8_pmuv3/mem_access/u'],\n",
    "                ipc])\n",
    "        all_features.extend(features)\n",
    "    return all_features\n",
    "\n",
    "\n",
    "def process_csv_ipc(file_paths, chunk_size):\n",
    "    all_ipc_values = []\n",
    "    for file_path in file_paths:\n",
    "        df = pd.read_csv(file_path + '_' + str(chunk_size))\n",
    "        df = df.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "        for _, row in df.iterrows():\n",
    "            all_ipc_values.append(row['instructions'] / row['cpu-cycles'])  \n",
    "    return all_ipc_values\n",
    "\n",
    "source = [\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_500',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_502',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_505',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_520',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_523',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_525',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_531',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_541',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_548',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_557',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_503',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_507',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_508',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_510',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_511',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_519',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_521',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_527',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_538',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_544',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_549',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_554',\n",
    "]\n",
    "    \n",
    "target = [\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_500',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_502',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_505',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_520',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_523',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_525',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_531',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_541',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_548',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_557',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_503',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_507',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_508',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_510',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_511',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_519',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_521',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_527',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_538',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_544',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_549',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_554',\n",
    "]\n",
    "\n",
    "chunks = [200, 400, 600, 800, 1000]\n",
    "for chunk_size in chunks:\n",
    "    in_order_hardware_counters_features = process_csv_feautures(source, chunk_size)\n",
    "    out_of_order_ipc = process_csv_ipc(target, chunk_size)\n",
    "    print('chunk_size: ' + str(chunk_size))\n",
    "    \n",
    "    data = {\n",
    "        'In Order': in_order_hardware_counters_features,\n",
    "        'Out of Order': out_of_order_ipc\n",
    "    }\n",
    "    \n",
    "    length = min(len(in_order_hardware_counters_features), len(out_of_order_ipc))\n",
    "    \n",
    "    # Shorten the longen array\n",
    "    in_order_hardware_counters_features = in_order_hardware_counters_features[:length]\n",
    "    out_of_order_ipc = out_of_order_ipc[:length]\n",
    "    \n",
    "    # separate features and target\n",
    "    X = pd.DataFrame(\n",
    "        in_order_hardware_counters_features, \n",
    "        columns=[\n",
    "            'instructions:u',\n",
    "            'cpu-cycles:u',\n",
    "            'armv8_pmuv3/stall_backend/u',\n",
    "            'armv8_pmuv3/stall_frontend/u',\n",
    "            'armv8_pmuv3/br_mis_pred/u',\n",
    "            'armv8_pmuv3/br_pred/u',\n",
    "            'armv8_pmuv3/br_retired/u',\n",
    "            'armv8_pmuv3/inst_spec/u',\n",
    "            'armv8_pmuv3/inst_retired/u',\n",
    "            'armv8_pmuv3/l1d_cache/u',\n",
    "            'armv8_pmuv3/l1d_cache_refill/u',\n",
    "            'armv8_pmuv3/l1d_cache_wb/u',\n",
    "            'armv8_pmuv3/l1d_tlb/u',\n",
    "            'armv8_pmuv3/l1d_tlb_refill/u',\n",
    "            'armv8_pmuv3/l1i_cache/u',\n",
    "            'armv8_pmuv3/l1i_cache_refill/u',\n",
    "            'armv8_pmuv3/l1i_tlb/u',\n",
    "            'armv8_pmuv3/l1i_tlb_refill/u',\n",
    "            'armv8_pmuv3/l2d_cache/u',\n",
    "            'armv8_pmuv3/l2d_cache_allocate/u',\n",
    "            'armv8_pmuv3/l2d_cache_refill/u',\n",
    "            'armv8_pmuv3/l2d_cache_wb/u',\n",
    "            'armv8_pmuv3/l2d_tlb/u',\n",
    "            'armv8_pmuv3/l2d_tlb_refill/u',\n",
    "            'armv8_pmuv3/l3d_cache/u',\n",
    "            'armv8_pmuv3/l3d_cache_allocate/u',\n",
    "            'armv8_pmuv3/l3d_cache_refill/u',\n",
    "            'armv8_pmuv3/l3d_cache_wb/u',\n",
    "            'armv8_pmuv3/mem_access/u',\n",
    "            'ipc'])  # Features\n",
    "    y = pd.Series(out_of_order_ipc) \n",
    "    \n",
    "    # split \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    rf_model = RandomForestRegressor(n_estimators=100)\n",
    "    gb_model = GradientBoostingRegressor(n_estimators=100)\n",
    "    ensemble_model = gb_model\n",
    "    #ensemble_model = VotingRegressor([('rf', rf_model), ('gb', gb_model)])\n",
    "    ensemble_model.fit(X_train, y_train)\n",
    "    \n",
    "    ensemble_predictions = ensemble_model.predict(X_test)\n",
    "    # Calculate R²\n",
    "    ensemble_r2 = r2_score(y_test, ensemble_predictions)\n",
    "    \n",
    "    #bruh wut\n",
    "    train_predictions = ensemble_model.predict(X_train)\n",
    "    test_predictions = ensemble_model.predict(X_test)\n",
    "    \n",
    "    # Calculate R² for training and testing sets\n",
    "    train_r2 = r2_score(y_train, train_predictions)\n",
    "    train_mape = mean_absolute_percentage_error(y_train, train_predictions)\n",
    "    test_r2 = r2_score(y_test, test_predictions)\n",
    "    test_mape = mean_absolute_percentage_error(y_test, test_predictions)\n",
    "    \n",
    "    \n",
    "    print(f\"Training R²: {train_r2}\")\n",
    "    print(f\"Testing R²: {test_r2}\")\n",
    "    print(f\"Training Mape: {train_mape}\")\n",
    "    print(f\"Testing Mape: {test_mape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6dc43da5-edad-4f8b-b5f3-ccd64577191b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_size: 200\n",
      "Training R²: 0.9814564391554962\n",
      "Testing R²: 0.9718592509432137\n",
      "Training Mape: 0.02397386758850154\n",
      "Testing Mape: 0.028307260312597744\n",
      "chunk_size: 400\n",
      "Training R²: 0.9265231026017101\n",
      "Testing R²: 0.897537350607259\n",
      "Training Mape: 0.04520625800305858\n",
      "Testing Mape: 0.05289404656888263\n",
      "chunk_size: 600\n",
      "Training R²: 0.896726284579211\n",
      "Testing R²: 0.8767572406340154\n",
      "Training Mape: 0.05649024862472983\n",
      "Testing Mape: 0.061627324428061674\n",
      "chunk_size: 800\n",
      "Training R²: 0.8723709123944405\n",
      "Testing R²: 0.8575812929543823\n",
      "Training Mape: 0.06531019641505557\n",
      "Testing Mape: 0.06970287529769312\n",
      "chunk_size: 1000\n",
      "Training R²: 0.8392050784905029\n",
      "Testing R²: 0.8294692186698962\n",
      "Training Mape: 0.07469225107611\n",
      "Testing Mape: 0.07762525185836076\n"
     ]
    }
   ],
   "source": [
    "# desktop to server\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "def process_csv_feautures(file_paths, chunk_size):\n",
    "    all_features = []\n",
    "    for file_path in file_paths:\n",
    "        df = pd.read_csv(file_path + '_' + str(chunk_size))\n",
    "        df = df.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "        features = []\n",
    "        for index, row in df.iterrows():\n",
    "            inst = row['instructions']\n",
    "            cyc = row['cpu-cycles']\n",
    "            ipc = inst/cyc\n",
    "            features.append([\n",
    "                row['instructions'],\n",
    "                row['cpu-cycles'],\n",
    "                row['br_pred'],\n",
    "                row['br_mis_pred'],\n",
    "                row['l1d_cache_rd'],\n",
    "                row['l1d_cache_wr'],\n",
    "                row['l1d_cache'],\n",
    "                row['l1i_cache'],\n",
    "                row['l1i_cache_refill'],\n",
    "                row['context-switches'],\n",
    "                row['l2d_cache_rd'],\n",
    "                row['l2d_cache_wr'],\n",
    "                row['l2d_cache'],\n",
    "                row['l1d_tlb'],\n",
    "                row['l1d_tlb_refill_rd'],\n",
    "                row['l1d_tlb_refill_wr'],\n",
    "                row['dtlb_walk'],\n",
    "                row['itlb_walk'],\n",
    "                row['page-faults'],\n",
    "                row['l2d_tlb_access'],\n",
    "                row['l2i_tlb_access'],\n",
    "                row['l1i_tlb_refill'],\n",
    "                row['iTLB-loads'],\n",
    "                row['iTLB-load-misses'],\n",
    "                row['branch-loads'],\n",
    "                row['dTLB-loads'],\n",
    "                row['dTLB-load-misses'],\n",
    "                row['branch-load-misses'],\n",
    "                row['vfp_spec'],\n",
    "                row['inst_spec'],\n",
    "                row['ase_spec'],\n",
    "                row['bx_stall'],\n",
    "                row['decode_stall'],\n",
    "                row['dispatch_stall'],\n",
    "                row['fx_stall'],\n",
    "                row['ixa_stall'],\n",
    "                row['ixb_stall'],\n",
    "                row['lx_stall'],\n",
    "                row['sx_stall'],\n",
    "                row['bus_access'],\n",
    "                row['mem_access'],\n",
    "                row['mem_access_rd'],\n",
    "                row['mem_access_wr'],\n",
    "                ipc])\n",
    "        all_features.extend(features)\n",
    "    return all_features\n",
    "\n",
    "\n",
    "def process_csv_ipc(file_paths, chunk_size):\n",
    "    all_ipc_values = []\n",
    "    for file_path in file_paths:\n",
    "        df = pd.read_csv(file_path + '_' + str(chunk_size))\n",
    "        df = df.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "        for _, row in df.iterrows():\n",
    "            all_ipc_values.append(row['instructions'] / row['cpu-cycles'])  \n",
    "    return all_ipc_values\n",
    "\n",
    "source = [\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_500',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_502',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_505',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_520',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_523',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_525',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_531',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_541',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_548',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_557',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_503',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_507',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_508',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_510',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_511',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_519',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_521',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_527',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_538',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_544',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_549',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_554',\n",
    "]\n",
    "    \n",
    "target = [\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_500',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_502',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_505',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_520',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_523',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_525',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_531',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_541',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_548',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_557',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_503',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_507',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_508',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_510',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_511',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_519',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_521',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_527',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_538',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_544',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_549',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_554',\n",
    "]\n",
    "\n",
    "chunks = [200, 400, 600, 800, 1000]\n",
    "for chunk_size in chunks:\n",
    "    in_order_hardware_counters_features = process_csv_feautures(source, chunk_size)\n",
    "    out_of_order_ipc = process_csv_ipc(target, chunk_size)\n",
    "    print('chunk_size: ' + str(chunk_size))\n",
    "    \n",
    "    data = {\n",
    "        'In Order': in_order_hardware_counters_features,\n",
    "        'Out of Order': out_of_order_ipc\n",
    "    }\n",
    "    \n",
    "    length = min(len(in_order_hardware_counters_features), len(out_of_order_ipc))\n",
    "    \n",
    "    # Shorten the longen array\n",
    "    in_order_hardware_counters_features = in_order_hardware_counters_features[:length]\n",
    "    out_of_order_ipc = out_of_order_ipc[:length]\n",
    "    \n",
    "    # separate features and target\n",
    "    X = pd.DataFrame(\n",
    "        in_order_hardware_counters_features, \n",
    "        columns=[\n",
    "            'instructions',\n",
    "            'cpu-cycles',\n",
    "            'br_pred',\n",
    "            'br_mis_pred',\n",
    "            'l1d_cache_rd',\n",
    "            'l1d_cache_wr',\n",
    "            'l1d_cache',\n",
    "            'l1i_cache',\n",
    "            'l1i_cache_refill',\n",
    "            'context-switches',\n",
    "            'l2d_cache_rd',\n",
    "            'l2d_cache_wr',\n",
    "            'l2d_cache',\n",
    "            'l1d_tlb',\n",
    "            'l1d_tlb_refill_rd',\n",
    "            'l1d_tlb_refill_wr',\n",
    "            'dtlb_walk',\n",
    "            'itlb_walk',\n",
    "            'page-faults',\n",
    "            'l2d_tlb_access',\n",
    "            'l2i_tlb_access',\n",
    "            'l1i_tlb_refill',\n",
    "            'iTLB-loads',\n",
    "            'iTLB-load-misses',\n",
    "            'branch-loads',\n",
    "            'dTLB-loads',\n",
    "            'dTLB-load-misses',\n",
    "            'branch-load-misses',\n",
    "            'vfp_spec',\n",
    "            'inst_spec',\n",
    "            'ase_spec',\n",
    "            'bx_stall',\n",
    "            'decode_stall',\n",
    "            'dispatch_stall',\n",
    "            'fx_stall',\n",
    "            'ixa_stall',\n",
    "            'ixb_stall',\n",
    "            'lx_stall',\n",
    "            'sx_stall',\n",
    "            'bus_access',\n",
    "            'mem_access',\n",
    "            'mem_access_rd',\n",
    "            'mem_access_wr',\n",
    "            'ipc'])  # Features\n",
    "    y = pd.Series(out_of_order_ipc) \n",
    "    \n",
    "    # split \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    rf_model = RandomForestRegressor(n_estimators=100)\n",
    "    gb_model = GradientBoostingRegressor(n_estimators=100)\n",
    "    ensemble_model = gb_model\n",
    "    #ensemble_model = VotingRegressor([('rf', rf_model), ('gb', gb_model)])\n",
    "    ensemble_model.fit(X_train, y_train)\n",
    "    \n",
    "    ensemble_predictions = ensemble_model.predict(X_test)\n",
    "    # Calculate R²\n",
    "    ensemble_r2 = r2_score(y_test, ensemble_predictions)\n",
    "    \n",
    "    #bruh wut\n",
    "    train_predictions = ensemble_model.predict(X_train)\n",
    "    test_predictions = ensemble_model.predict(X_test)\n",
    "    \n",
    "    # Calculate R² for training and testing sets\n",
    "    train_r2 = r2_score(y_train, train_predictions)\n",
    "    train_mape = mean_absolute_percentage_error(y_train, train_predictions)\n",
    "    test_r2 = r2_score(y_test, test_predictions)\n",
    "    test_mape = mean_absolute_percentage_error(y_test, test_predictions)\n",
    "    \n",
    "    \n",
    "    print(f\"Training R²: {train_r2}\")\n",
    "    print(f\"Testing R²: {test_r2}\")\n",
    "    print(f\"Training Mape: {train_mape}\")\n",
    "    print(f\"Testing Mape: {test_mape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2983595c-dd6d-4752-835a-596ca83e0da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_size: 200\n",
      "Training R²: 0.9875323292874805\n",
      "Testing R²: 0.9777710037937948\n",
      "Training Mape: 0.045925862282553465\n",
      "Testing Mape: 0.05183813174797467\n",
      "chunk_size: 400\n",
      "Training R²: 0.9646221188906265\n",
      "Testing R²: 0.9350604138257119\n",
      "Training Mape: 0.07421405343334593\n",
      "Testing Mape: 0.08681573421103189\n",
      "chunk_size: 600\n",
      "Training R²: 0.9404057859175298\n",
      "Testing R²: 0.8997133288932074\n",
      "Training Mape: 0.09067458163260457\n",
      "Testing Mape: 0.1043852857496227\n",
      "chunk_size: 800\n",
      "Training R²: 0.9187674118305613\n",
      "Testing R²: 0.9017633606851183\n",
      "Training Mape: 0.10736000616186331\n",
      "Testing Mape: 0.11790905017522024\n",
      "chunk_size: 1000\n",
      "Training R²: 0.9058768830937702\n",
      "Testing R²: 0.8947936657547545\n",
      "Training Mape: 0.11494967590814045\n",
      "Testing Mape: 0.12002133393702014\n"
     ]
    }
   ],
   "source": [
    "# desktop to InO\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "def process_csv_feautures(file_paths, chunk_size):\n",
    "    all_features = []\n",
    "    for file_path in file_paths:\n",
    "        df = pd.read_csv(file_path + '_' + str(chunk_size))\n",
    "        df = df.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "        features = []\n",
    "        for index, row in df.iterrows():\n",
    "            inst = row['instructions']\n",
    "            cyc = row['cpu-cycles']\n",
    "            ipc = inst/cyc\n",
    "            features.append([\n",
    "                row['instructions'],\n",
    "                row['cpu-cycles'],\n",
    "                row['br_pred'],\n",
    "                row['br_mis_pred'],\n",
    "                row['l1d_cache_rd'],\n",
    "                row['l1d_cache_wr'],\n",
    "                row['l1d_cache'],\n",
    "                row['l1i_cache'],\n",
    "                row['l1i_cache_refill'],\n",
    "                row['context-switches'],\n",
    "                row['l2d_cache_rd'],\n",
    "                row['l2d_cache_wr'],\n",
    "                row['l2d_cache'],\n",
    "                row['l1d_tlb'],\n",
    "                row['l1d_tlb_refill_rd'],\n",
    "                row['l1d_tlb_refill_wr'],\n",
    "                row['dtlb_walk'],\n",
    "                row['itlb_walk'],\n",
    "                row['page-faults'],\n",
    "                row['l2d_tlb_access'],\n",
    "                row['l2i_tlb_access'],\n",
    "                row['l1i_tlb_refill'],\n",
    "                row['iTLB-loads'],\n",
    "                row['iTLB-load-misses'],\n",
    "                row['branch-loads'],\n",
    "                row['dTLB-loads'],\n",
    "                row['dTLB-load-misses'],\n",
    "                row['branch-load-misses'],\n",
    "                row['vfp_spec'],\n",
    "                row['inst_spec'],\n",
    "                row['ase_spec'],\n",
    "                row['bx_stall'],\n",
    "                row['decode_stall'],\n",
    "                row['dispatch_stall'],\n",
    "                row['fx_stall'],\n",
    "                row['ixa_stall'],\n",
    "                row['ixb_stall'],\n",
    "                row['lx_stall'],\n",
    "                row['sx_stall'],\n",
    "                row['bus_access'],\n",
    "                row['mem_access'],\n",
    "                row['mem_access_rd'],\n",
    "                row['mem_access_wr'],\n",
    "                ipc])\n",
    "        all_features.extend(features)\n",
    "    return all_features\n",
    "\n",
    "\n",
    "def process_csv_ipc(file_paths, chunk_size):\n",
    "    all_ipc_values = []\n",
    "    for file_path in file_paths:\n",
    "        df = pd.read_csv(file_path + '_' + str(chunk_size))\n",
    "        df = df.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "        for _, row in df.iterrows():\n",
    "            all_ipc_values.append(row['instructions:u'] / row['cpu-cycles:u'])  \n",
    "    return all_ipc_values\n",
    "\n",
    "source = [\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_500',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_502',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_505',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_520',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_523',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_525',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_531',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_541',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_548',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_557',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_503',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_507',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_508',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_510',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_511',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_519',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_521',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_527',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_538',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_544',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_549',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_554',\n",
    "]\n",
    "    \n",
    "target = [\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_500',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_502',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_505',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_520',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_523',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_525',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_531',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_541',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_548',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_557',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_503',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_507',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_508',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_510',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_511',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_519',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_521',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_527',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_538',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_544',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_549',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_554',\n",
    "]\n",
    "\n",
    "chunks = [200, 400, 600, 800, 1000]\n",
    "for chunk_size in chunks:\n",
    "    in_order_hardware_counters_features = process_csv_feautures(source, chunk_size)\n",
    "    out_of_order_ipc = process_csv_ipc(target, chunk_size)\n",
    "    print('chunk_size: ' + str(chunk_size))\n",
    "    \n",
    "    data = {\n",
    "        'In Order': in_order_hardware_counters_features,\n",
    "        'Out of Order': out_of_order_ipc\n",
    "    }\n",
    "    \n",
    "    length = min(len(in_order_hardware_counters_features), len(out_of_order_ipc))\n",
    "    \n",
    "    # Shorten the longen array\n",
    "    in_order_hardware_counters_features = in_order_hardware_counters_features[:length]\n",
    "    out_of_order_ipc = out_of_order_ipc[:length]\n",
    "    \n",
    "    # separate features and target\n",
    "    X = pd.DataFrame(\n",
    "        in_order_hardware_counters_features, \n",
    "        columns=[\n",
    "            'instructions',\n",
    "            'cpu-cycles',\n",
    "            'br_pred',\n",
    "            'br_mis_pred',\n",
    "            'l1d_cache_rd',\n",
    "            'l1d_cache_wr',\n",
    "            'l1d_cache',\n",
    "            'l1i_cache',\n",
    "            'l1i_cache_refill',\n",
    "            'context-switches',\n",
    "            'l2d_cache_rd',\n",
    "            'l2d_cache_wr',\n",
    "            'l2d_cache',\n",
    "            'l1d_tlb',\n",
    "            'l1d_tlb_refill_rd',\n",
    "            'l1d_tlb_refill_wr',\n",
    "            'dtlb_walk',\n",
    "            'itlb_walk',\n",
    "            'page-faults',\n",
    "            'l2d_tlb_access',\n",
    "            'l2i_tlb_access',\n",
    "            'l1i_tlb_refill',\n",
    "            'iTLB-loads',\n",
    "            'iTLB-load-misses',\n",
    "            'branch-loads',\n",
    "            'dTLB-loads',\n",
    "            'dTLB-load-misses',\n",
    "            'branch-load-misses',\n",
    "            'vfp_spec',\n",
    "            'inst_spec',\n",
    "            'ase_spec',\n",
    "            'bx_stall',\n",
    "            'decode_stall',\n",
    "            'dispatch_stall',\n",
    "            'fx_stall',\n",
    "            'ixa_stall',\n",
    "            'ixb_stall',\n",
    "            'lx_stall',\n",
    "            'sx_stall',\n",
    "            'bus_access',\n",
    "            'mem_access',\n",
    "            'mem_access_rd',\n",
    "            'mem_access_wr',\n",
    "            'ipc'])  # Features\n",
    "    y = pd.Series(out_of_order_ipc) \n",
    "    \n",
    "    # split \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    rf_model = RandomForestRegressor(n_estimators=100)\n",
    "    gb_model = GradientBoostingRegressor(n_estimators=100)\n",
    "    ensemble_model = gb_model\n",
    "    #ensemble_model = VotingRegressor([('rf', rf_model), ('gb', gb_model)])\n",
    "    ensemble_model.fit(X_train, y_train)\n",
    "    \n",
    "    ensemble_predictions = ensemble_model.predict(X_test)\n",
    "    # Calculate R²\n",
    "    ensemble_r2 = r2_score(y_test, ensemble_predictions)\n",
    "    \n",
    "    #bruh wut\n",
    "    train_predictions = ensemble_model.predict(X_train)\n",
    "    test_predictions = ensemble_model.predict(X_test)\n",
    "    \n",
    "    # Calculate R² for training and testing sets\n",
    "    train_r2 = r2_score(y_train, train_predictions)\n",
    "    train_mape = mean_absolute_percentage_error(y_train, train_predictions)\n",
    "    test_r2 = r2_score(y_test, test_predictions)\n",
    "    test_mape = mean_absolute_percentage_error(y_test, test_predictions)\n",
    "    \n",
    "    \n",
    "    print(f\"Training R²: {train_r2}\")\n",
    "    print(f\"Testing R²: {test_r2}\")\n",
    "    print(f\"Training Mape: {train_mape}\")\n",
    "    print(f\"Testing Mape: {test_mape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2bb2831e-56ab-49c6-b9d7-6a047be1c524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_size: 200\n",
      "Training R²: 0.9850721812785823\n",
      "Testing R²: 0.9771999035782183\n",
      "Training Mape: 0.030700735903363457\n",
      "Testing Mape: 0.036861213203379505\n",
      "chunk_size: 400\n",
      "Training R²: 0.9437255548476787\n",
      "Testing R²: 0.9180059743587625\n",
      "Training Mape: 0.05313153767254502\n",
      "Testing Mape: 0.06265859855581324\n",
      "chunk_size: 600\n",
      "Training R²: 0.9220933712485649\n",
      "Testing R²: 0.9044847505295616\n",
      "Training Mape: 0.06259548139454515\n",
      "Testing Mape: 0.06901463605179756\n",
      "chunk_size: 800\n",
      "Training R²: 0.9046103046005063\n",
      "Testing R²: 0.8870709503268143\n",
      "Training Mape: 0.07167440242560176\n",
      "Testing Mape: 0.07544380794698902\n",
      "chunk_size: 1000\n",
      "Training R²: 0.8836445036351613\n",
      "Testing R²: 0.8705868972646741\n",
      "Training Mape: 0.07941153606784968\n",
      "Testing Mape: 0.081622046038825\n"
     ]
    }
   ],
   "source": [
    "# desktop to InO\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "def process_csv_feautures(file_paths, chunk_size):\n",
    "    all_features = []\n",
    "    for file_path in file_paths:\n",
    "        df = pd.read_csv(file_path + '_' + str(chunk_size))\n",
    "        df = df.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "        features = []\n",
    "        for index, row in df.iterrows():\n",
    "            inst = row['instructions']\n",
    "            cyc = row['cpu-cycles']\n",
    "            ipc = inst/cyc\n",
    "            features.append([\n",
    "                row['instructions'],\n",
    "                row['cpu-cycles'],\n",
    "                row['br_pred'],\n",
    "                row['br_mis_pred'],\n",
    "                row['l1d_cache_rd'],\n",
    "                row['l1d_cache_wr'],\n",
    "                row['l1d_cache'],\n",
    "                row['l1i_cache'],\n",
    "                row['l1i_cache_refill'],\n",
    "                row['context-switches'],\n",
    "                row['l2d_cache_rd'],\n",
    "                row['l2d_cache_wr'],\n",
    "                row['l2d_cache'],\n",
    "                row['l1d_tlb'],\n",
    "                row['l1d_tlb_refill_rd'],\n",
    "                row['l1d_tlb_refill_wr'],\n",
    "                row['dtlb_walk'],\n",
    "                row['itlb_walk'],\n",
    "                row['page-faults'],\n",
    "                row['l2d_tlb_access'],\n",
    "                row['l2i_tlb_access'],\n",
    "                row['l1i_tlb_refill'],\n",
    "                row['iTLB-loads'],\n",
    "                row['iTLB-load-misses'],\n",
    "                row['branch-loads'],\n",
    "                row['dTLB-loads'],\n",
    "                row['dTLB-load-misses'],\n",
    "                row['branch-load-misses'],\n",
    "                row['vfp_spec'],\n",
    "                row['inst_spec'],\n",
    "                row['ase_spec'],\n",
    "                row['bx_stall'],\n",
    "                row['decode_stall'],\n",
    "                row['dispatch_stall'],\n",
    "                row['fx_stall'],\n",
    "                row['ixa_stall'],\n",
    "                row['ixb_stall'],\n",
    "                row['lx_stall'],\n",
    "                row['sx_stall'],\n",
    "                row['bus_access'],\n",
    "                row['mem_access'],\n",
    "                row['mem_access_rd'],\n",
    "                row['mem_access_wr'],\n",
    "                ipc])\n",
    "        all_features.extend(features)\n",
    "    return all_features\n",
    "\n",
    "\n",
    "def process_csv_ipc(file_paths, chunk_size):\n",
    "    all_ipc_values = []\n",
    "    for file_path in file_paths:\n",
    "        df = pd.read_csv(file_path + '_' + str(chunk_size))\n",
    "        df = df.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "        for _, row in df.iterrows():\n",
    "            all_ipc_values.append(row['instructions:u'] / row['cpu-cycles:u'])  \n",
    "    return all_ipc_values\n",
    "\n",
    "source = [\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_500',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_502',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_505',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_520',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_523',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_525',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_531',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_541',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_548',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_557',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_503',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_507',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_508',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_510',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_511',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_519',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_521',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_527',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_538',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_544',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_549',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_554',\n",
    "]\n",
    "    \n",
    "target = [\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_500',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_502',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_505',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_520',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_523',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_525',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_531',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_541',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_548',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_557',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_503',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_507',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_508',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_510',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_511',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_519',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_521',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_527',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_538',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_544',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_549',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_554',\n",
    "]\n",
    "\n",
    "chunks = [200, 400, 600, 800, 1000]\n",
    "for chunk_size in chunks:\n",
    "    in_order_hardware_counters_features = process_csv_feautures(source, chunk_size)\n",
    "    out_of_order_ipc = process_csv_ipc(target, chunk_size)\n",
    "    print('chunk_size: ' + str(chunk_size))\n",
    "    \n",
    "    data = {\n",
    "        'In Order': in_order_hardware_counters_features,\n",
    "        'Out of Order': out_of_order_ipc\n",
    "    }\n",
    "    \n",
    "    length = min(len(in_order_hardware_counters_features), len(out_of_order_ipc))\n",
    "    \n",
    "    # Shorten the longen array\n",
    "    in_order_hardware_counters_features = in_order_hardware_counters_features[:length]\n",
    "    out_of_order_ipc = out_of_order_ipc[:length]\n",
    "    \n",
    "    # separate features and target\n",
    "    X = pd.DataFrame(\n",
    "        in_order_hardware_counters_features, \n",
    "        columns=[\n",
    "            'instructions',\n",
    "            'cpu-cycles',\n",
    "            'br_pred',\n",
    "            'br_mis_pred',\n",
    "            'l1d_cache_rd',\n",
    "            'l1d_cache_wr',\n",
    "            'l1d_cache',\n",
    "            'l1i_cache',\n",
    "            'l1i_cache_refill',\n",
    "            'context-switches',\n",
    "            'l2d_cache_rd',\n",
    "            'l2d_cache_wr',\n",
    "            'l2d_cache',\n",
    "            'l1d_tlb',\n",
    "            'l1d_tlb_refill_rd',\n",
    "            'l1d_tlb_refill_wr',\n",
    "            'dtlb_walk',\n",
    "            'itlb_walk',\n",
    "            'page-faults',\n",
    "            'l2d_tlb_access',\n",
    "            'l2i_tlb_access',\n",
    "            'l1i_tlb_refill',\n",
    "            'iTLB-loads',\n",
    "            'iTLB-load-misses',\n",
    "            'branch-loads',\n",
    "            'dTLB-loads',\n",
    "            'dTLB-load-misses',\n",
    "            'branch-load-misses',\n",
    "            'vfp_spec',\n",
    "            'inst_spec',\n",
    "            'ase_spec',\n",
    "            'bx_stall',\n",
    "            'decode_stall',\n",
    "            'dispatch_stall',\n",
    "            'fx_stall',\n",
    "            'ixa_stall',\n",
    "            'ixb_stall',\n",
    "            'lx_stall',\n",
    "            'sx_stall',\n",
    "            'bus_access',\n",
    "            'mem_access',\n",
    "            'mem_access_rd',\n",
    "            'mem_access_wr',\n",
    "            'ipc'])  # Features\n",
    "    y = pd.Series(out_of_order_ipc) \n",
    "    \n",
    "    # split \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    rf_model = RandomForestRegressor(n_estimators=100)\n",
    "    gb_model = GradientBoostingRegressor(n_estimators=100)\n",
    "    ensemble_model = gb_model\n",
    "    #ensemble_model = VotingRegressor([('rf', rf_model), ('gb', gb_model)])\n",
    "    ensemble_model.fit(X_train, y_train)\n",
    "    \n",
    "    ensemble_predictions = ensemble_model.predict(X_test)\n",
    "    # Calculate R²\n",
    "    ensemble_r2 = r2_score(y_test, ensemble_predictions)\n",
    "    \n",
    "    #bruh wut\n",
    "    train_predictions = ensemble_model.predict(X_train)\n",
    "    test_predictions = ensemble_model.predict(X_test)\n",
    "    \n",
    "    # Calculate R² for training and testing sets\n",
    "    train_r2 = r2_score(y_train, train_predictions)\n",
    "    train_mape = mean_absolute_percentage_error(y_train, train_predictions)\n",
    "    test_r2 = r2_score(y_test, test_predictions)\n",
    "    test_mape = mean_absolute_percentage_error(y_test, test_predictions)\n",
    "    \n",
    "    \n",
    "    print(f\"Training R²: {train_r2}\")\n",
    "    print(f\"Testing R²: {test_r2}\")\n",
    "    print(f\"Training Mape: {train_mape}\")\n",
    "    print(f\"Testing Mape: {test_mape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "242a57a0-408a-4931-907a-3f28e5b9663d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_size: 200\n",
      "Training R²: 0.9846324229945186\n",
      "Testing R²: 0.9716242331505356\n",
      "Training Mape: 0.05363245311145086\n",
      "Testing Mape: 0.06519395631436878\n",
      "chunk_size: 400\n",
      "Training R²: 0.9140350927756694\n",
      "Testing R²: 0.872693069138293\n",
      "Training Mape: 0.11304800782822501\n",
      "Testing Mape: 0.1294043992430083\n",
      "chunk_size: 600\n",
      "Training R²: 0.8732449072771586\n",
      "Testing R²: 0.8222821821067623\n",
      "Training Mape: 0.1458150216947856\n",
      "Testing Mape: 0.15415646696458446\n",
      "chunk_size: 800\n",
      "Training R²: 0.8409068520899763\n",
      "Testing R²: 0.8104536388836806\n",
      "Training Mape: 0.16668525699432774\n",
      "Testing Mape: 0.17728077571647877\n",
      "chunk_size: 1000\n",
      "Training R²: 0.8133683968195505\n",
      "Testing R²: 0.8076533407234713\n",
      "Training Mape: 0.18710599826874325\n",
      "Testing Mape: 0.1884525322820542\n"
     ]
    }
   ],
   "source": [
    "# server to InO\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "def process_csv_feautures(file_paths, chunk_size):\n",
    "    all_features = []\n",
    "    for file_path in file_paths:\n",
    "        df = pd.read_csv(file_path + '_' + str(chunk_size))\n",
    "        df = df.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "        features = []\n",
    "        for index, row in df.iterrows():\n",
    "            inst = row['instructions']\n",
    "            cyc = row['cpu-cycles']\n",
    "            ipc = inst/cyc\n",
    "            features.append([\n",
    "                row['instructions'],\n",
    "                row['cpu-cycles'], \n",
    "                row['br_pred'], \n",
    "                row['br_mis_pred'], \n",
    "                row['l1d_cache'], \n",
    "                row['l1d_tlb'], \n",
    "                row['l1d_tlb_rd'], \n",
    "                row['l1d_tlb_wr'], \n",
    "                row['l2d_cache_rd'], \n",
    "                row['l2d_cache_wr'], \n",
    "                row['l2d_cache'], \n",
    "                row['l1i_cache'], \n",
    "                row['l1i_cache_refill'], \n",
    "                row['remote_access'], \n",
    "                row['dtlb_walk'], \n",
    "                row['itlb_walk'], \n",
    "                row['l1i_tlb'], \n",
    "                row['l2d_tlb'], \n",
    "                row['l2d_tlb_rd'], \n",
    "                row['l2d_tlb_wr'], \n",
    "                row['vfp_spec'], \n",
    "                row['inst_spec'], \n",
    "                row['ase_spec'], \n",
    "                row['stall_backend'], \n",
    "                row['stall_frontend'], \n",
    "                row['ll_cache_miss_rd'], \n",
    "                row['mem_access'], \n",
    "                row['mem_access_rd'], \n",
    "                row['mem_access_wr'],\n",
    "                ipc])\n",
    "        all_features.extend(features)\n",
    "    return all_features\n",
    "\n",
    "\n",
    "def process_csv_ipc(file_paths, chunk_size):\n",
    "    all_ipc_values = []\n",
    "    for file_path in file_paths:\n",
    "        df = pd.read_csv(file_path + '_' + str(chunk_size))\n",
    "        df = df.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "        for _, row in df.iterrows():\n",
    "            all_ipc_values.append(row['instructions:u'] / row['cpu-cycles:u'])  \n",
    "    return all_ipc_values\n",
    "\n",
    "source = [\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_500',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_502',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_505',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_520',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_523',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_525',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_531',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_541',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_548',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_557',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_503',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_507',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_508',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_510',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_511',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_519',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_521',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_527',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_538',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_544',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_549',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_554',\n",
    "]\n",
    "    \n",
    "target = [\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_500',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_502',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_505',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_520',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_523',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_525',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_531',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_541',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_548',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_557',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_503',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_507',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_508',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_510',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_511',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_519',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_521',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_527',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_538',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_544',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_549',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_554',\n",
    "]\n",
    "\n",
    "chunks = [200, 400, 600, 800, 1000]\n",
    "for chunk_size in chunks:\n",
    "    in_order_hardware_counters_features = process_csv_feautures(source, chunk_size)\n",
    "    out_of_order_ipc = process_csv_ipc(target, chunk_size)\n",
    "    print('chunk_size: ' + str(chunk_size))\n",
    "    \n",
    "    data = {\n",
    "        'In Order': in_order_hardware_counters_features,\n",
    "        'Out of Order': out_of_order_ipc\n",
    "    }\n",
    "    \n",
    "    length = min(len(in_order_hardware_counters_features), len(out_of_order_ipc))\n",
    "    \n",
    "    # Shorten the longen array\n",
    "    in_order_hardware_counters_features = in_order_hardware_counters_features[:length]\n",
    "    out_of_order_ipc = out_of_order_ipc[:length]\n",
    "    \n",
    "    # separate features and target\n",
    "    X = pd.DataFrame(\n",
    "        in_order_hardware_counters_features, \n",
    "        columns=[\n",
    "            'instructions',\n",
    "            'cpu-cycles', \n",
    "            'br_pred', \n",
    "            'br_mis_pred', \n",
    "            'l1d_cache', \n",
    "            'l1d_tlb', \n",
    "            'l1d_tlb_rd', \n",
    "            'l1d_tlb_wr', \n",
    "            'l2d_cache_rd', \n",
    "            'l2d_cache_wr', \n",
    "            'l2d_cache', \n",
    "            'l1i_cache', \n",
    "            'l1i_cache_refill', \n",
    "            'remote_access', \n",
    "            'dtlb_walk', \n",
    "            'itlb_walk', \n",
    "            'l1i_tlb', \n",
    "            'l2d_tlb', \n",
    "            'l2d_tlb_rd', \n",
    "            'l2d_tlb_wr', \n",
    "            'vfp_spec', \n",
    "            'inst_spec', \n",
    "            'ase_spec', \n",
    "            'stall_backend', \n",
    "            'stall_frontend', \n",
    "            'll_cache_miss_rd', \n",
    "            'mem_access', \n",
    "            'mem_access_rd', \n",
    "            'mem_access_wr',\n",
    "            'ipc'])  # Features\n",
    "    y = pd.Series(out_of_order_ipc) \n",
    "    \n",
    "    # split \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    rf_model = RandomForestRegressor(n_estimators=100)\n",
    "    gb_model = GradientBoostingRegressor(n_estimators=100)\n",
    "    ensemble_model = gb_model\n",
    "    #ensemble_model = VotingRegressor([('rf', rf_model), ('gb', gb_model)])\n",
    "    ensemble_model.fit(X_train, y_train)\n",
    "    \n",
    "    ensemble_predictions = ensemble_model.predict(X_test)\n",
    "    # Calculate R²\n",
    "    ensemble_r2 = r2_score(y_test, ensemble_predictions)\n",
    "    \n",
    "    #bruh wut\n",
    "    train_predictions = ensemble_model.predict(X_train)\n",
    "    test_predictions = ensemble_model.predict(X_test)\n",
    "    \n",
    "    # Calculate R² for training and testing sets\n",
    "    train_r2 = r2_score(y_train, train_predictions)\n",
    "    train_mape = mean_absolute_percentage_error(y_train, train_predictions)\n",
    "    test_r2 = r2_score(y_test, test_predictions)\n",
    "    test_mape = mean_absolute_percentage_error(y_test, test_predictions)\n",
    "    \n",
    "    \n",
    "    print(f\"Training R²: {train_r2}\")\n",
    "    print(f\"Testing R²: {test_r2}\")\n",
    "    print(f\"Training Mape: {train_mape}\")\n",
    "    print(f\"Testing Mape: {test_mape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec5f8efc-7df1-419f-9947-e3cc17903d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_size: 200\n",
      "Training R²: 0.9821097239864184\n",
      "Testing R²: 0.9724075431596473\n",
      "Training Mape: 0.03160554088913627\n",
      "Testing Mape: 0.03761502905241759\n",
      "chunk_size: 400\n",
      "Training R²: 0.9449098606633984\n",
      "Testing R²: 0.9186971665200215\n",
      "Training Mape: 0.052989846653637145\n",
      "Testing Mape: 0.058905811191312066\n",
      "chunk_size: 600\n",
      "Training R²: 0.9186438096305525\n",
      "Testing R²: 0.8947870489541689\n",
      "Training Mape: 0.06596054450944439\n",
      "Testing Mape: 0.07267968969664448\n",
      "chunk_size: 800\n",
      "Training R²: 0.8934469567369828\n",
      "Testing R²: 0.8653106994779889\n",
      "Training Mape: 0.07613016012493053\n",
      "Testing Mape: 0.08010674919864448\n",
      "chunk_size: 1000\n",
      "Training R²: 0.8590163596564797\n",
      "Testing R²: 0.8514045976068804\n",
      "Training Mape: 0.08823413366296554\n",
      "Testing Mape: 0.09164921837860686\n"
     ]
    }
   ],
   "source": [
    "# server to OOO\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "def process_csv_feautures(file_paths, chunk_size):\n",
    "    all_features = []\n",
    "    for file_path in file_paths:\n",
    "        df = pd.read_csv(file_path + '_' + str(chunk_size))\n",
    "        df = df.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "        features = []\n",
    "        for index, row in df.iterrows():\n",
    "            inst = row['instructions']\n",
    "            cyc = row['cpu-cycles']\n",
    "            ipc = inst/cyc\n",
    "            features.append([\n",
    "                row['instructions'],\n",
    "                row['cpu-cycles'], \n",
    "                row['br_pred'], \n",
    "                row['br_mis_pred'], \n",
    "                row['l1d_cache'], \n",
    "                row['l1d_tlb'], \n",
    "                row['l1d_tlb_rd'], \n",
    "                row['l1d_tlb_wr'], \n",
    "                row['l2d_cache_rd'], \n",
    "                row['l2d_cache_wr'], \n",
    "                row['l2d_cache'], \n",
    "                row['l1i_cache'], \n",
    "                row['l1i_cache_refill'], \n",
    "                row['remote_access'], \n",
    "                row['dtlb_walk'], \n",
    "                row['itlb_walk'], \n",
    "                row['l1i_tlb'], \n",
    "                row['l2d_tlb'], \n",
    "                row['l2d_tlb_rd'], \n",
    "                row['l2d_tlb_wr'], \n",
    "                row['vfp_spec'], \n",
    "                row['inst_spec'], \n",
    "                row['ase_spec'], \n",
    "                row['stall_backend'], \n",
    "                row['stall_frontend'], \n",
    "                row['ll_cache_miss_rd'], \n",
    "                row['mem_access'], \n",
    "                row['mem_access_rd'], \n",
    "                row['mem_access_wr'],\n",
    "                ipc])\n",
    "        all_features.extend(features)\n",
    "    return all_features\n",
    "\n",
    "\n",
    "def process_csv_ipc(file_paths, chunk_size):\n",
    "    all_ipc_values = []\n",
    "    for file_path in file_paths:\n",
    "        df = pd.read_csv(file_path + '_' + str(chunk_size))\n",
    "        df = df.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "        for _, row in df.iterrows():\n",
    "            all_ipc_values.append(row['instructions:u'] / row['cpu-cycles:u'])  \n",
    "    return all_ipc_values\n",
    "\n",
    "source = [\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_500',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_502',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_505',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_520',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_523',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_525',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_531',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_541',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_548',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_557',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_503',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_507',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_508',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_510',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_511',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_519',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_521',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_527',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_538',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_544',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_549',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_554',\n",
    "]\n",
    "    \n",
    "target = [\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_500',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_502',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_505',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_520',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_523',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_525',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_531',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_541',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_548',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_557',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_503',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_507',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_508',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_510',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_511',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_519',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_521',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_527',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_538',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_544',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_549',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_554',\n",
    "]\n",
    "\n",
    "chunks = [200, 400, 600, 800, 1000]\n",
    "for chunk_size in chunks:\n",
    "    in_order_hardware_counters_features = process_csv_feautures(source, chunk_size)\n",
    "    out_of_order_ipc = process_csv_ipc(target, chunk_size)\n",
    "    print('chunk_size: ' + str(chunk_size))\n",
    "    \n",
    "    data = {\n",
    "        'In Order': in_order_hardware_counters_features,\n",
    "        'Out of Order': out_of_order_ipc\n",
    "    }\n",
    "    \n",
    "    length = min(len(in_order_hardware_counters_features), len(out_of_order_ipc))\n",
    "    \n",
    "    # Shorten the longen array\n",
    "    in_order_hardware_counters_features = in_order_hardware_counters_features[:length]\n",
    "    out_of_order_ipc = out_of_order_ipc[:length]\n",
    "    \n",
    "    # separate features and target\n",
    "    X = pd.DataFrame(\n",
    "        in_order_hardware_counters_features, \n",
    "        columns=[\n",
    "            'instructions',\n",
    "            'cpu-cycles', \n",
    "            'br_pred', \n",
    "            'br_mis_pred', \n",
    "            'l1d_cache', \n",
    "            'l1d_tlb', \n",
    "            'l1d_tlb_rd', \n",
    "            'l1d_tlb_wr', \n",
    "            'l2d_cache_rd', \n",
    "            'l2d_cache_wr', \n",
    "            'l2d_cache', \n",
    "            'l1i_cache', \n",
    "            'l1i_cache_refill', \n",
    "            'remote_access', \n",
    "            'dtlb_walk', \n",
    "            'itlb_walk', \n",
    "            'l1i_tlb', \n",
    "            'l2d_tlb', \n",
    "            'l2d_tlb_rd', \n",
    "            'l2d_tlb_wr', \n",
    "            'vfp_spec', \n",
    "            'inst_spec', \n",
    "            'ase_spec', \n",
    "            'stall_backend', \n",
    "            'stall_frontend', \n",
    "            'll_cache_miss_rd', \n",
    "            'mem_access', \n",
    "            'mem_access_rd', \n",
    "            'mem_access_wr',\n",
    "            'ipc'])  # Features\n",
    "    y = pd.Series(out_of_order_ipc) \n",
    "    \n",
    "    # split \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    rf_model = RandomForestRegressor(n_estimators=100)\n",
    "    gb_model = GradientBoostingRegressor(n_estimators=100)\n",
    "    ensemble_model = gb_model\n",
    "    #ensemble_model = VotingRegressor([('rf', rf_model), ('gb', gb_model)])\n",
    "    ensemble_model.fit(X_train, y_train)\n",
    "    \n",
    "    ensemble_predictions = ensemble_model.predict(X_test)\n",
    "    # Calculate R²\n",
    "    ensemble_r2 = r2_score(y_test, ensemble_predictions)\n",
    "    \n",
    "    #bruh wut\n",
    "    train_predictions = ensemble_model.predict(X_train)\n",
    "    test_predictions = ensemble_model.predict(X_test)\n",
    "    \n",
    "    # Calculate R² for training and testing sets\n",
    "    train_r2 = r2_score(y_train, train_predictions)\n",
    "    train_mape = mean_absolute_percentage_error(y_train, train_predictions)\n",
    "    test_r2 = r2_score(y_test, test_predictions)\n",
    "    test_mape = mean_absolute_percentage_error(y_test, test_predictions)\n",
    "    \n",
    "    \n",
    "    print(f\"Training R²: {train_r2}\")\n",
    "    print(f\"Testing R²: {test_r2}\")\n",
    "    print(f\"Training Mape: {train_mape}\")\n",
    "    print(f\"Testing Mape: {test_mape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3da4ac5-3733-449b-a193-d1a4a8c500cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_size: 200\n",
      "Training R²: 0.984458957654995\n",
      "Testing R²: 0.9764525830628886\n",
      "Training Mape: 0.032738134031471904\n",
      "Testing Mape: 0.036490851642208944\n",
      "chunk_size: 400\n",
      "Training R²: 0.9170333283322165\n",
      "Testing R²: 0.8848575430626274\n",
      "Training Mape: 0.06618370140890283\n",
      "Testing Mape: 0.07420910235474465\n",
      "chunk_size: 600\n",
      "Training R²: 0.8864321110777169\n",
      "Testing R²: 0.8742467764112702\n",
      "Training Mape: 0.08234085254966815\n",
      "Testing Mape: 0.08481815228295908\n",
      "chunk_size: 800\n",
      "Training R²: 0.858900908735785\n",
      "Testing R²: 0.8510251969236655\n",
      "Training Mape: 0.09835601598953995\n",
      "Testing Mape: 0.09996188466092307\n",
      "chunk_size: 1000\n",
      "Training R²: 0.8283209690205037\n",
      "Testing R²: 0.8145242536514459\n",
      "Training Mape: 0.11209781602070532\n",
      "Testing Mape: 0.11749065940723707\n"
     ]
    }
   ],
   "source": [
    "# server to desktop\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "def process_csv_feautures(file_paths, chunk_size):\n",
    "    all_features = []\n",
    "    for file_path in file_paths:\n",
    "        df = pd.read_csv(file_path + '_' + str(chunk_size))\n",
    "        df = df.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "        features = []\n",
    "        for index, row in df.iterrows():\n",
    "            inst = row['instructions']\n",
    "            cyc = row['cpu-cycles']\n",
    "            ipc = inst/cyc\n",
    "            features.append([\n",
    "                row['instructions'],\n",
    "                row['cpu-cycles'], \n",
    "                row['br_pred'], \n",
    "                row['br_mis_pred'], \n",
    "                row['l1d_cache'], \n",
    "                row['l1d_tlb'], \n",
    "                row['l1d_tlb_rd'], \n",
    "                row['l1d_tlb_wr'], \n",
    "                row['l2d_cache_rd'], \n",
    "                row['l2d_cache_wr'], \n",
    "                row['l2d_cache'], \n",
    "                row['l1i_cache'], \n",
    "                row['l1i_cache_refill'], \n",
    "                row['remote_access'], \n",
    "                row['dtlb_walk'], \n",
    "                row['itlb_walk'], \n",
    "                row['l1i_tlb'], \n",
    "                row['l2d_tlb'], \n",
    "                row['l2d_tlb_rd'], \n",
    "                row['l2d_tlb_wr'], \n",
    "                row['vfp_spec'], \n",
    "                row['inst_spec'], \n",
    "                row['ase_spec'], \n",
    "                row['stall_backend'], \n",
    "                row['stall_frontend'], \n",
    "                row['ll_cache_miss_rd'], \n",
    "                row['mem_access'], \n",
    "                row['mem_access_rd'], \n",
    "                row['mem_access_wr'],\n",
    "                ipc])\n",
    "        all_features.extend(features)\n",
    "    return all_features\n",
    "\n",
    "\n",
    "def process_csv_ipc(file_paths, chunk_size):\n",
    "    all_ipc_values = []\n",
    "    for file_path in file_paths:\n",
    "        df = pd.read_csv(file_path + '_' + str(chunk_size))\n",
    "        df = df.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "        for _, row in df.iterrows():\n",
    "            all_ipc_values.append(row['instructions'] / row['cpu-cycles'])  \n",
    "    return all_ipc_values\n",
    "\n",
    "source = [\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_500',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_502',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_505',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_520',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_523',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_525',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_531',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_541',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_548',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_557',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_503',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_507',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_508',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_510',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_511',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_519',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_521',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_527',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_538',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_544',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_549',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_554',\n",
    "]\n",
    "    \n",
    "target = [\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_500',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_502',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_505',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_520',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_523',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_525',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_531',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_541',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_548',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_557',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_503',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_507',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_508',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_510',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_511',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_519',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_521',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_527',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_538',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_544',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_549',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_554',\n",
    "]\n",
    "\n",
    "chunks = [200, 400, 600, 800, 1000]\n",
    "for chunk_size in chunks:\n",
    "    in_order_hardware_counters_features = process_csv_feautures(source, chunk_size)\n",
    "    out_of_order_ipc = process_csv_ipc(target, chunk_size)\n",
    "    print('chunk_size: ' + str(chunk_size))\n",
    "    \n",
    "    data = {\n",
    "        'In Order': in_order_hardware_counters_features,\n",
    "        'Out of Order': out_of_order_ipc\n",
    "    }\n",
    "    \n",
    "    length = min(len(in_order_hardware_counters_features), len(out_of_order_ipc))\n",
    "    \n",
    "    # Shorten the longen array\n",
    "    in_order_hardware_counters_features = in_order_hardware_counters_features[:length]\n",
    "    out_of_order_ipc = out_of_order_ipc[:length]\n",
    "    \n",
    "    # separate features and target\n",
    "    X = pd.DataFrame(\n",
    "        in_order_hardware_counters_features, \n",
    "        columns=[\n",
    "            'instructions',\n",
    "            'cpu-cycles', \n",
    "            'br_pred', \n",
    "            'br_mis_pred', \n",
    "            'l1d_cache', \n",
    "            'l1d_tlb', \n",
    "            'l1d_tlb_rd', \n",
    "            'l1d_tlb_wr', \n",
    "            'l2d_cache_rd', \n",
    "            'l2d_cache_wr', \n",
    "            'l2d_cache', \n",
    "            'l1i_cache', \n",
    "            'l1i_cache_refill', \n",
    "            'remote_access', \n",
    "            'dtlb_walk', \n",
    "            'itlb_walk', \n",
    "            'l1i_tlb', \n",
    "            'l2d_tlb', \n",
    "            'l2d_tlb_rd', \n",
    "            'l2d_tlb_wr', \n",
    "            'vfp_spec', \n",
    "            'inst_spec', \n",
    "            'ase_spec', \n",
    "            'stall_backend', \n",
    "            'stall_frontend', \n",
    "            'll_cache_miss_rd', \n",
    "            'mem_access', \n",
    "            'mem_access_rd', \n",
    "            'mem_access_wr',\n",
    "            'ipc'])  # Features\n",
    "    y = pd.Series(out_of_order_ipc) \n",
    "    \n",
    "    # split \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    rf_model = RandomForestRegressor(n_estimators=100)\n",
    "    gb_model = GradientBoostingRegressor(n_estimators=100)\n",
    "    ensemble_model = gb_model\n",
    "    #ensemble_model = VotingRegressor([('rf', rf_model), ('gb', gb_model)])\n",
    "    ensemble_model.fit(X_train, y_train)\n",
    "    \n",
    "    ensemble_predictions = ensemble_model.predict(X_test)\n",
    "    # Calculate R²\n",
    "    ensemble_r2 = r2_score(y_test, ensemble_predictions)\n",
    "    \n",
    "    #bruh wut\n",
    "    train_predictions = ensemble_model.predict(X_train)\n",
    "    test_predictions = ensemble_model.predict(X_test)\n",
    "    \n",
    "    # Calculate R² for training and testing sets\n",
    "    train_r2 = r2_score(y_train, train_predictions)\n",
    "    train_mape = mean_absolute_percentage_error(y_train, train_predictions)\n",
    "    test_r2 = r2_score(y_test, test_predictions)\n",
    "    test_mape = mean_absolute_percentage_error(y_test, test_predictions)\n",
    "    \n",
    "    \n",
    "    print(f\"Training R²: {train_r2}\")\n",
    "    print(f\"Testing R²: {test_r2}\")\n",
    "    print(f\"Training Mape: {train_mape}\")\n",
    "    print(f\"Testing Mape: {test_mape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4742e29-4899-4697-ac5a-3686ece51cdb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93e518bb-30f5-41ce-afb6-2ef8e8d90ad2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_size: 1000000000\n",
      "Training R²: 0.7580865725483777\n",
      "Testing R²: 0.7420112067975866\n",
      "Training Mape: 0.12667612702235917\n",
      "Testing Mape: 0.1310119969248322\n",
      "chunk_size: 2000000000\n",
      "Training R²: 0.8333401400768289\n",
      "Testing R²: 0.8028116872339028\n",
      "Training Mape: 0.09671214765928998\n",
      "Testing Mape: 0.10972802086888042\n",
      "chunk_size: 5000000000\n",
      "Training R²: 0.9246723600788995\n",
      "Testing R²: 0.8808290288625468\n",
      "Training Mape: 0.05967270307334755\n",
      "Testing Mape: 0.06730510037943976\n",
      "chunk_size: 10000000000\n",
      "Training R²: 0.9513597832617902\n",
      "Testing R²: 0.9328879251555539\n",
      "Training Mape: 0.04337188358048306\n",
      "Testing Mape: 0.05184048214078268\n",
      "chunk_size: 20000000000\n",
      "Training R²: 0.9750095669839633\n",
      "Testing R²: 0.944272499646238\n",
      "Training Mape: 0.03301455168748521\n",
      "Testing Mape: 0.0462956621808335\n"
     ]
    }
   ],
   "source": [
    "# InO to OOO\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "def process_csv_feautures(file_paths, chunk_size):\n",
    "    all_features = []\n",
    "    for file_path in file_paths:\n",
    "        df = pd.read_csv(file_path + '_' + str(chunk_size) + '.csv')\n",
    "        df = df.dropna() \n",
    "        features = []\n",
    "        for index, row in df.iterrows():\n",
    "            inst = row['instructions:u']\n",
    "            cyc = row['cpu-cycles:u']\n",
    "            ipc = inst/cyc\n",
    "\n",
    "            features.append([\n",
    "                row['instructions:u'],\n",
    "                row['cpu-cycles:u'],\n",
    "                row['armv8_pmuv3/stall_backend/u'],\n",
    "                row['armv8_pmuv3/stall_frontend/u'],\n",
    "                row['armv8_pmuv3/br_mis_pred/u'],\n",
    "                row['armv8_pmuv3/br_pred/u'],\n",
    "                row['armv8_pmuv3/br_retired/u'],\n",
    "                row['armv8_pmuv3/inst_spec/u'],\n",
    "                row['armv8_pmuv3/inst_retired/u'],\n",
    "                row['armv8_pmuv3/l1d_cache/u'],\n",
    "                row['armv8_pmuv3/l1d_cache_refill/u'],\n",
    "                row['armv8_pmuv3/l1d_cache_wb/u'],\n",
    "                row['armv8_pmuv3/l1d_tlb/u'],\n",
    "                row['armv8_pmuv3/l1d_tlb_refill/u'],\n",
    "                row['armv8_pmuv3/l1i_cache/u'],\n",
    "                row['armv8_pmuv3/l1i_cache_refill/u'],\n",
    "                row['armv8_pmuv3/l1i_tlb/u'],\n",
    "                row['armv8_pmuv3/l1i_tlb_refill/u'],\n",
    "                row['armv8_pmuv3/l2d_cache/u'],\n",
    "                row['armv8_pmuv3/l2d_cache_allocate/u'],\n",
    "                row['armv8_pmuv3/l2d_cache_refill/u'],\n",
    "                row['armv8_pmuv3/l2d_cache_wb/u'],\n",
    "                row['armv8_pmuv3/l2d_tlb/u'],\n",
    "                row['armv8_pmuv3/l2d_tlb_refill/u'],\n",
    "                row['armv8_pmuv3/l3d_cache/u'],\n",
    "                row['armv8_pmuv3/l3d_cache_allocate/u'],\n",
    "                row['armv8_pmuv3/l3d_cache_refill/u'],\n",
    "                row['armv8_pmuv3/l3d_cache_wb/u'],\n",
    "                row['armv8_pmuv3/mem_access/u'],\n",
    "                ipc])\n",
    "        all_features.extend(features)\n",
    "    return all_features\n",
    "\n",
    "\n",
    "def process_csv_ipc(file_paths, chunk_size):\n",
    "    all_ipc_values = []\n",
    "    for file_path in file_paths:\n",
    "        df = pd.read_csv(file_path + '_' + str(chunk_size) + '.csv')\n",
    "        df = df.dropna() \n",
    "        for _, row in df.iterrows():\n",
    "            all_ipc_values.append(row['instructions:u'] / row['cpu-cycles:u'])  \n",
    "    return all_ipc_values\n",
    "\n",
    "source = [\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_500',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_502',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_505',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_520',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_523',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_525',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_531',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_541',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_548',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_557',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_503',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_507',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_508',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_510',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_511',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_519',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_521',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_527',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_538',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_544',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_549',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_554',\n",
    "]\n",
    "    \n",
    "target = [\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_500',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_502',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_505',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_520',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_523',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_525',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_531',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_541',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_548',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_557',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_503',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_507',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_508',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_510',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_511',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_519',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_521',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_527',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_538',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_544',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_549',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_554',\n",
    "]\n",
    "\n",
    "chunks = [1000000000, 2000000000, 5000000000, 10000000000, 20000000000]\n",
    "for chunk_size in chunks:\n",
    "    in_order_hardware_counters_features = process_csv_feautures(source, chunk_size)\n",
    "    out_of_order_ipc = process_csv_ipc(target, chunk_size)\n",
    "    print('chunk_size: ' + str(chunk_size))\n",
    "    \n",
    "    data = {\n",
    "        'In Order': in_order_hardware_counters_features,\n",
    "        'Out of Order': out_of_order_ipc\n",
    "    }\n",
    "    \n",
    "    length = min(len(in_order_hardware_counters_features), len(out_of_order_ipc))\n",
    "    \n",
    "    # Shorten the longen array\n",
    "    in_order_hardware_counters_features = in_order_hardware_counters_features[:length]\n",
    "    out_of_order_ipc = out_of_order_ipc[:length]\n",
    "    \n",
    "    # separate features and target\n",
    "    X = pd.DataFrame(\n",
    "        in_order_hardware_counters_features, \n",
    "        columns=[\n",
    "            'instructions:u',\n",
    "            'cpu-cycles:u',\n",
    "            'armv8_pmuv3/stall_backend/u',\n",
    "            'armv8_pmuv3/stall_frontend/u',\n",
    "            'armv8_pmuv3/br_mis_pred/u',\n",
    "            'armv8_pmuv3/br_pred/u',\n",
    "            'armv8_pmuv3/br_retired/u',\n",
    "            'armv8_pmuv3/inst_spec/u',\n",
    "            'armv8_pmuv3/inst_retired/u',\n",
    "            'armv8_pmuv3/l1d_cache/u',\n",
    "            'armv8_pmuv3/l1d_cache_refill/u',\n",
    "            'armv8_pmuv3/l1d_cache_wb/u',\n",
    "            'armv8_pmuv3/l1d_tlb/u',\n",
    "            'armv8_pmuv3/l1d_tlb_refill/u',\n",
    "            'armv8_pmuv3/l1i_cache/u',\n",
    "            'armv8_pmuv3/l1i_cache_refill/u',\n",
    "            'armv8_pmuv3/l1i_tlb/u',\n",
    "            'armv8_pmuv3/l1i_tlb_refill/u',\n",
    "            'armv8_pmuv3/l2d_cache/u',\n",
    "            'armv8_pmuv3/l2d_cache_allocate/u',\n",
    "            'armv8_pmuv3/l2d_cache_refill/u',\n",
    "            'armv8_pmuv3/l2d_cache_wb/u',\n",
    "            'armv8_pmuv3/l2d_tlb/u',\n",
    "            'armv8_pmuv3/l2d_tlb_refill/u',\n",
    "            'armv8_pmuv3/l3d_cache/u',\n",
    "            'armv8_pmuv3/l3d_cache_allocate/u',\n",
    "            'armv8_pmuv3/l3d_cache_refill/u',\n",
    "            'armv8_pmuv3/l3d_cache_wb/u',\n",
    "            'armv8_pmuv3/mem_access/u',\n",
    "            'ipc'])  # Features\n",
    "    y = pd.Series(out_of_order_ipc) \n",
    "    \n",
    "    # split \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    rf_model = RandomForestRegressor(n_estimators=100)\n",
    "    gb_model = GradientBoostingRegressor(n_estimators=100)\n",
    "    ensemble_model = gb_model\n",
    "    #ensemble_model = VotingRegressor([('rf', rf_model), ('gb', gb_model)])\n",
    "    ensemble_model.fit(X_train, y_train)\n",
    "    \n",
    "    ensemble_predictions = ensemble_model.predict(X_test)\n",
    "    # Calculate R²\n",
    "    ensemble_r2 = r2_score(y_test, ensemble_predictions)\n",
    "    \n",
    "    #bruh wut\n",
    "    train_predictions = ensemble_model.predict(X_train)\n",
    "    test_predictions = ensemble_model.predict(X_test)\n",
    "    \n",
    "    # Calculate R² for training and testing sets\n",
    "    train_r2 = r2_score(y_train, train_predictions)\n",
    "    train_mape = mean_absolute_percentage_error(y_train, train_predictions)\n",
    "    test_r2 = r2_score(y_test, test_predictions)\n",
    "    test_mape = mean_absolute_percentage_error(y_test, test_predictions)\n",
    "    \n",
    "    \n",
    "    print(f\"Training R²: {train_r2}\")\n",
    "    print(f\"Testing R²: {test_r2}\")\n",
    "    print(f\"Training Mape: {train_mape}\")\n",
    "    print(f\"Testing Mape: {test_mape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00f469f5-6b2c-4abc-9e4e-b11967a1b731",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_size: 1000000000\n",
      "Training R²: 0.7283906708352629\n",
      "Testing R²: 0.7102241420301633\n",
      "Training Mape: 0.14771661612046666\n",
      "Testing Mape: 0.15154157270234833\n",
      "chunk_size: 2000000000\n",
      "Training R²: 0.7705026534898123\n",
      "Testing R²: 0.7333497448633932\n",
      "Training Mape: 0.13330080669555705\n",
      "Testing Mape: 0.14093286502501204\n",
      "chunk_size: 5000000000\n",
      "Training R²: 0.8298649769425959\n",
      "Testing R²: 0.775042520137363\n",
      "Training Mape: 0.1101512092993374\n",
      "Testing Mape: 0.12542822963320835\n",
      "chunk_size: 10000000000\n",
      "Training R²: 0.8656988652420055\n",
      "Testing R²: 0.80687730575107\n",
      "Training Mape: 0.10037577703435388\n",
      "Testing Mape: 0.11353397320226834\n",
      "chunk_size: 20000000000\n",
      "Training R²: 0.9089053009890232\n",
      "Testing R²: 0.7591192211013293\n",
      "Training Mape: 0.08624976899465271\n",
      "Testing Mape: 0.12637948221627548\n"
     ]
    }
   ],
   "source": [
    "# InO to desktop\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "def process_csv_feautures(file_paths, chunk_size):\n",
    "    all_features = []\n",
    "    for file_path in file_paths:\n",
    "        df = pd.read_csv(file_path + '_' + str(chunk_size) + '.csv')\n",
    "        df = df.dropna() \n",
    "        features = []\n",
    "        for index, row in df.iterrows():\n",
    "            inst = row['instructions:u']\n",
    "            cyc = row['cpu-cycles:u']\n",
    "            ipc = inst/cyc\n",
    "            features.append([\n",
    "                row['instructions:u'],\n",
    "                row['cpu-cycles:u'],\n",
    "                row['armv8_pmuv3/stall_backend/u'],\n",
    "                row['armv8_pmuv3/stall_frontend/u'],\n",
    "                row['armv8_pmuv3/br_mis_pred/u'],\n",
    "                row['armv8_pmuv3/br_pred/u'],\n",
    "                row['armv8_pmuv3/br_retired/u'],\n",
    "                row['armv8_pmuv3/inst_spec/u'],\n",
    "                row['armv8_pmuv3/inst_retired/u'],\n",
    "                row['armv8_pmuv3/l1d_cache/u'],\n",
    "                row['armv8_pmuv3/l1d_cache_refill/u'],\n",
    "                row['armv8_pmuv3/l1d_cache_wb/u'],\n",
    "                row['armv8_pmuv3/l1d_tlb/u'],\n",
    "                row['armv8_pmuv3/l1d_tlb_refill/u'],\n",
    "                row['armv8_pmuv3/l1i_cache/u'],\n",
    "                row['armv8_pmuv3/l1i_cache_refill/u'],\n",
    "                row['armv8_pmuv3/l1i_tlb/u'],\n",
    "                row['armv8_pmuv3/l1i_tlb_refill/u'],\n",
    "                row['armv8_pmuv3/l2d_cache/u'],\n",
    "                row['armv8_pmuv3/l2d_cache_allocate/u'],\n",
    "                row['armv8_pmuv3/l2d_cache_refill/u'],\n",
    "                row['armv8_pmuv3/l2d_cache_wb/u'],\n",
    "                row['armv8_pmuv3/l2d_tlb/u'],\n",
    "                row['armv8_pmuv3/l2d_tlb_refill/u'],\n",
    "                row['armv8_pmuv3/l3d_cache/u'],\n",
    "                row['armv8_pmuv3/l3d_cache_allocate/u'],\n",
    "                row['armv8_pmuv3/l3d_cache_refill/u'],\n",
    "                row['armv8_pmuv3/l3d_cache_wb/u'],\n",
    "                row['armv8_pmuv3/mem_access/u'],\n",
    "                ipc])\n",
    "        all_features.extend(features)\n",
    "    return all_features\n",
    "\n",
    "\n",
    "def process_csv_ipc(file_paths, chunk_size):\n",
    "    all_ipc_values = []\n",
    "    for file_path in file_paths:\n",
    "        df = pd.read_csv(file_path + '_' + str(chunk_size) + '.csv')\n",
    "        df = df.dropna() \n",
    "        for _, row in df.iterrows():\n",
    "            all_ipc_values.append(row['instructions'] / row['cpu-cycles'])  \n",
    "    return all_ipc_values\n",
    "\n",
    "source = [\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_500',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_502',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_505',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_520',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_523',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_525',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_531',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_541',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_548',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_557',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_503',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_507',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_508',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_510',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_511',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_519',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_521',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_527',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_538',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_544',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_549',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_554',\n",
    "]\n",
    "    \n",
    "target = [\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_500',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_502',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_505',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_520',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_523',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_525',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_531',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_541',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_548',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_557',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_503',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_507',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_508',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_510',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_511',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_519',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_521',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_527',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_538',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_544',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_549',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_554',\n",
    "]\n",
    "\n",
    "chunks = [1000000000, 2000000000, 5000000000, 10000000000, 20000000000]\n",
    "for chunk_size in chunks:\n",
    "    in_order_hardware_counters_features = process_csv_feautures(source, chunk_size)\n",
    "    out_of_order_ipc = process_csv_ipc(target, chunk_size)\n",
    "    print('chunk_size: ' + str(chunk_size))\n",
    "    \n",
    "    data = {\n",
    "        'In Order': in_order_hardware_counters_features,\n",
    "        'Out of Order': out_of_order_ipc\n",
    "    }\n",
    "    \n",
    "    length = min(len(in_order_hardware_counters_features), len(out_of_order_ipc))\n",
    "    \n",
    "    # Shorten the longen array\n",
    "    in_order_hardware_counters_features = in_order_hardware_counters_features[:length]\n",
    "    out_of_order_ipc = out_of_order_ipc[:length]\n",
    "    \n",
    "    # separate features and target\n",
    "    X = pd.DataFrame(\n",
    "        in_order_hardware_counters_features, \n",
    "        columns=[\n",
    "            'instructions:u',\n",
    "            'cpu-cycles:u',\n",
    "            'armv8_pmuv3/stall_backend/u',\n",
    "            'armv8_pmuv3/stall_frontend/u',\n",
    "            'armv8_pmuv3/br_mis_pred/u',\n",
    "            'armv8_pmuv3/br_pred/u',\n",
    "            'armv8_pmuv3/br_retired/u',\n",
    "            'armv8_pmuv3/inst_spec/u',\n",
    "            'armv8_pmuv3/inst_retired/u',\n",
    "            'armv8_pmuv3/l1d_cache/u',\n",
    "            'armv8_pmuv3/l1d_cache_refill/u',\n",
    "            'armv8_pmuv3/l1d_cache_wb/u',\n",
    "            'armv8_pmuv3/l1d_tlb/u',\n",
    "            'armv8_pmuv3/l1d_tlb_refill/u',\n",
    "            'armv8_pmuv3/l1i_cache/u',\n",
    "            'armv8_pmuv3/l1i_cache_refill/u',\n",
    "            'armv8_pmuv3/l1i_tlb/u',\n",
    "            'armv8_pmuv3/l1i_tlb_refill/u',\n",
    "            'armv8_pmuv3/l2d_cache/u',\n",
    "            'armv8_pmuv3/l2d_cache_allocate/u',\n",
    "            'armv8_pmuv3/l2d_cache_refill/u',\n",
    "            'armv8_pmuv3/l2d_cache_wb/u',\n",
    "            'armv8_pmuv3/l2d_tlb/u',\n",
    "            'armv8_pmuv3/l2d_tlb_refill/u',\n",
    "            'armv8_pmuv3/l3d_cache/u',\n",
    "            'armv8_pmuv3/l3d_cache_allocate/u',\n",
    "            'armv8_pmuv3/l3d_cache_refill/u',\n",
    "            'armv8_pmuv3/l3d_cache_wb/u',\n",
    "            'armv8_pmuv3/mem_access/u',\n",
    "            'ipc'])  # Features\n",
    "    y = pd.Series(out_of_order_ipc) \n",
    "    \n",
    "    # split \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    rf_model = RandomForestRegressor(n_estimators=100)\n",
    "    gb_model = GradientBoostingRegressor(n_estimators=100)\n",
    "    ensemble_model = gb_model\n",
    "    #ensemble_model = VotingRegressor([('rf', rf_model), ('gb', gb_model)])\n",
    "    ensemble_model.fit(X_train, y_train)\n",
    "    \n",
    "    ensemble_predictions = ensemble_model.predict(X_test)\n",
    "    # Calculate R²\n",
    "    ensemble_r2 = r2_score(y_test, ensemble_predictions)\n",
    "    \n",
    "    #bruh wut\n",
    "    train_predictions = ensemble_model.predict(X_train)\n",
    "    test_predictions = ensemble_model.predict(X_test)\n",
    "    \n",
    "    # Calculate R² for training and testing sets\n",
    "    train_r2 = r2_score(y_train, train_predictions)\n",
    "    train_mape = mean_absolute_percentage_error(y_train, train_predictions)\n",
    "    test_r2 = r2_score(y_test, test_predictions)\n",
    "    test_mape = mean_absolute_percentage_error(y_test, test_predictions)\n",
    "    \n",
    "    \n",
    "    print(f\"Training R²: {train_r2}\")\n",
    "    print(f\"Testing R²: {test_r2}\")\n",
    "    print(f\"Training Mape: {train_mape}\")\n",
    "    print(f\"Testing Mape: {test_mape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0cf61275-6147-4e9f-819b-2b49da8d4f30",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_size: 1000000000\n",
      "Training R²: 0.7856204829459994\n",
      "Testing R²: 0.775215059203578\n",
      "Training Mape: 0.08721545390573811\n",
      "Testing Mape: 0.0886475255153743\n",
      "chunk_size: 2000000000\n",
      "Training R²: 0.8233946211523939\n",
      "Testing R²: 0.7873597598796571\n",
      "Training Mape: 0.07680933134042899\n",
      "Testing Mape: 0.08340478077432345\n",
      "chunk_size: 5000000000\n",
      "Training R²: 0.849935334444201\n",
      "Testing R²: 0.8004532274770517\n",
      "Training Mape: 0.070459906204091\n",
      "Testing Mape: 0.07851630894452319\n",
      "chunk_size: 10000000000\n",
      "Training R²: 0.8681522219759086\n",
      "Testing R²: 0.8276180515006146\n",
      "Training Mape: 0.06500933562857504\n",
      "Testing Mape: 0.0739942661071752\n",
      "chunk_size: 20000000000\n",
      "Training R²: 0.9084588408200678\n",
      "Testing R²: 0.8300619722619625\n",
      "Training Mape: 0.05471790957255694\n",
      "Testing Mape: 0.07104132665752658\n"
     ]
    }
   ],
   "source": [
    "# InO to server\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "def process_csv_feautures(file_paths, chunk_size):\n",
    "    all_features = []\n",
    "    for file_path in file_paths:\n",
    "        df = pd.read_csv(file_path + '_' + str(chunk_size) + '.csv')\n",
    "        df = df.dropna() \n",
    "        features = []\n",
    "        for index, row in df.iterrows():\n",
    "            inst = row['instructions:u']\n",
    "            cyc = row['cpu-cycles:u']\n",
    "            ipc = inst/cyc\n",
    "            features.append([\n",
    "                row['instructions:u'],\n",
    "                row['cpu-cycles:u'],\n",
    "                row['armv8_pmuv3/stall_backend/u'],\n",
    "                row['armv8_pmuv3/stall_frontend/u'],\n",
    "                row['armv8_pmuv3/br_mis_pred/u'],\n",
    "                row['armv8_pmuv3/br_pred/u'],\n",
    "                row['armv8_pmuv3/br_retired/u'],\n",
    "                row['armv8_pmuv3/inst_spec/u'],\n",
    "                row['armv8_pmuv3/inst_retired/u'],\n",
    "                row['armv8_pmuv3/l1d_cache/u'],\n",
    "                row['armv8_pmuv3/l1d_cache_refill/u'],\n",
    "                row['armv8_pmuv3/l1d_cache_wb/u'],\n",
    "                row['armv8_pmuv3/l1d_tlb/u'],\n",
    "                row['armv8_pmuv3/l1d_tlb_refill/u'],\n",
    "                row['armv8_pmuv3/l1i_cache/u'],\n",
    "                row['armv8_pmuv3/l1i_cache_refill/u'],\n",
    "                row['armv8_pmuv3/l1i_tlb/u'],\n",
    "                row['armv8_pmuv3/l1i_tlb_refill/u'],\n",
    "                row['armv8_pmuv3/l2d_cache/u'],\n",
    "                row['armv8_pmuv3/l2d_cache_allocate/u'],\n",
    "                row['armv8_pmuv3/l2d_cache_refill/u'],\n",
    "                row['armv8_pmuv3/l2d_cache_wb/u'],\n",
    "                row['armv8_pmuv3/l2d_tlb/u'],\n",
    "                row['armv8_pmuv3/l2d_tlb_refill/u'],\n",
    "                row['armv8_pmuv3/l3d_cache/u'],\n",
    "                row['armv8_pmuv3/l3d_cache_allocate/u'],\n",
    "                row['armv8_pmuv3/l3d_cache_refill/u'],\n",
    "                row['armv8_pmuv3/l3d_cache_wb/u'],\n",
    "                row['armv8_pmuv3/mem_access/u'],\n",
    "                ipc])\n",
    "        all_features.extend(features)\n",
    "    return all_features\n",
    "\n",
    "\n",
    "def process_csv_ipc(file_paths, chunk_size):\n",
    "    all_ipc_values = []\n",
    "    for file_path in file_paths:\n",
    "        df = pd.read_csv(file_path + '_' + str(chunk_size) + '.csv')\n",
    "        df = df.dropna() \n",
    "        for _, row in df.iterrows():\n",
    "            all_ipc_values.append(row['instructions'] / row['cpu-cycles'])  \n",
    "    return all_ipc_values\n",
    "\n",
    "source = [\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_500',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_502',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_505',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_520',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_523',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_525',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_531',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_541',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_548',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_557',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_503',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_507',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_508',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_510',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_511',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_519',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_521',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_527',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_538',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_544',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_549',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_554',\n",
    "]\n",
    "    \n",
    "target = [\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_500',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_502',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_505',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_520',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_523',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_525',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_531',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_541',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_548',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_557',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_503',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_507',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_508',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_510',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_511',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_519',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_521',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_527',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_538',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_544',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_549',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_554',\n",
    "]\n",
    "\n",
    "chunks = [1000000000, 2000000000, 5000000000, 10000000000, 20000000000]\n",
    "for chunk_size in chunks:\n",
    "    in_order_hardware_counters_features = process_csv_feautures(source, chunk_size)\n",
    "    out_of_order_ipc = process_csv_ipc(target, chunk_size)\n",
    "    print('chunk_size: ' + str(chunk_size))\n",
    "    \n",
    "    data = {\n",
    "        'In Order': in_order_hardware_counters_features,\n",
    "        'Out of Order': out_of_order_ipc\n",
    "    }\n",
    "    \n",
    "    length = min(len(in_order_hardware_counters_features), len(out_of_order_ipc))\n",
    "    \n",
    "    # Shorten the longen array\n",
    "    in_order_hardware_counters_features = in_order_hardware_counters_features[:length]\n",
    "    out_of_order_ipc = out_of_order_ipc[:length]\n",
    "    \n",
    "    # separate features and target\n",
    "    X = pd.DataFrame(\n",
    "        in_order_hardware_counters_features, \n",
    "        columns=[\n",
    "            'instructions:u',\n",
    "            'cpu-cycles:u',\n",
    "            'armv8_pmuv3/stall_backend/u',\n",
    "            'armv8_pmuv3/stall_frontend/u',\n",
    "            'armv8_pmuv3/br_mis_pred/u',\n",
    "            'armv8_pmuv3/br_pred/u',\n",
    "            'armv8_pmuv3/br_retired/u',\n",
    "            'armv8_pmuv3/inst_spec/u',\n",
    "            'armv8_pmuv3/inst_retired/u',\n",
    "            'armv8_pmuv3/l1d_cache/u',\n",
    "            'armv8_pmuv3/l1d_cache_refill/u',\n",
    "            'armv8_pmuv3/l1d_cache_wb/u',\n",
    "            'armv8_pmuv3/l1d_tlb/u',\n",
    "            'armv8_pmuv3/l1d_tlb_refill/u',\n",
    "            'armv8_pmuv3/l1i_cache/u',\n",
    "            'armv8_pmuv3/l1i_cache_refill/u',\n",
    "            'armv8_pmuv3/l1i_tlb/u',\n",
    "            'armv8_pmuv3/l1i_tlb_refill/u',\n",
    "            'armv8_pmuv3/l2d_cache/u',\n",
    "            'armv8_pmuv3/l2d_cache_allocate/u',\n",
    "            'armv8_pmuv3/l2d_cache_refill/u',\n",
    "            'armv8_pmuv3/l2d_cache_wb/u',\n",
    "            'armv8_pmuv3/l2d_tlb/u',\n",
    "            'armv8_pmuv3/l2d_tlb_refill/u',\n",
    "            'armv8_pmuv3/l3d_cache/u',\n",
    "            'armv8_pmuv3/l3d_cache_allocate/u',\n",
    "            'armv8_pmuv3/l3d_cache_refill/u',\n",
    "            'armv8_pmuv3/l3d_cache_wb/u',\n",
    "            'armv8_pmuv3/mem_access/u',\n",
    "            'ipc'])  # Features\n",
    "    y = pd.Series(out_of_order_ipc) \n",
    "    \n",
    "    # split \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    rf_model = RandomForestRegressor(n_estimators=100)\n",
    "    gb_model = GradientBoostingRegressor(n_estimators=100)\n",
    "    ensemble_model = gb_model\n",
    "    #ensemble_model = VotingRegressor([('rf', rf_model), ('gb', gb_model)])\n",
    "    ensemble_model.fit(X_train, y_train)\n",
    "    \n",
    "    ensemble_predictions = ensemble_model.predict(X_test)\n",
    "    # Calculate R²\n",
    "    ensemble_r2 = r2_score(y_test, ensemble_predictions)\n",
    "    \n",
    "    #bruh wut\n",
    "    train_predictions = ensemble_model.predict(X_train)\n",
    "    test_predictions = ensemble_model.predict(X_test)\n",
    "    \n",
    "    # Calculate R² for training and testing sets\n",
    "    train_r2 = r2_score(y_train, train_predictions)\n",
    "    train_mape = mean_absolute_percentage_error(y_train, train_predictions)\n",
    "    test_r2 = r2_score(y_test, test_predictions)\n",
    "    test_mape = mean_absolute_percentage_error(y_test, test_predictions)\n",
    "    \n",
    "    \n",
    "    print(f\"Training R²: {train_r2}\")\n",
    "    print(f\"Testing R²: {test_r2}\")\n",
    "    print(f\"Training Mape: {train_mape}\")\n",
    "    print(f\"Testing Mape: {test_mape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7568794-df2e-4edf-9fe3-074ff0c5bf84",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_size: 1000000000\n",
      "Training R²: 0.8084556425972838\n",
      "Testing R²: 0.7942720050014952\n",
      "Training Mape: 0.18952037339928293\n",
      "Testing Mape: 0.1971868858108733\n",
      "chunk_size: 2000000000\n",
      "Training R²: 0.8541934844793713\n",
      "Testing R²: 0.8495632783609975\n",
      "Training Mape: 0.15075144370975768\n",
      "Testing Mape: 0.15351737382703654\n",
      "chunk_size: 5000000000\n",
      "Training R²: 0.9092485919581642\n",
      "Testing R²: 0.8862237795742417\n",
      "Training Mape: 0.10098535446490975\n",
      "Testing Mape: 0.1120730363871137\n",
      "chunk_size: 10000000000\n",
      "Training R²: 0.9480275223181854\n",
      "Testing R²: 0.9117486278681909\n",
      "Training Mape: 0.07336499029231897\n",
      "Testing Mape: 0.09377770100588151\n",
      "chunk_size: 20000000000\n",
      "Training R²: 0.9758491316520689\n",
      "Testing R²: 0.9197356222041405\n",
      "Training Mape: 0.04820812560396087\n",
      "Testing Mape: 0.07492378894448797\n"
     ]
    }
   ],
   "source": [
    "# OOO to InO\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "def process_csv_feautures(file_paths, chunk_size):\n",
    "    all_features = []\n",
    "    for file_path in file_paths:\n",
    "        df = pd.read_csv(file_path + '_' + str(chunk_size) + '.csv')\n",
    "        df = df.dropna()\n",
    "        features = []\n",
    "        for index, row in df.iterrows():\n",
    "            inst = row['instructions:u']\n",
    "            cyc = row['cpu-cycles:u']\n",
    "            ipc = inst/cyc\n",
    "            features.append([\n",
    "                row['instructions:u'],\n",
    "                row['cpu-cycles:u'],\n",
    "                row['armv8_pmuv3/stall_backend/u'],\n",
    "                row['armv8_pmuv3/stall_frontend/u'],\n",
    "                row['armv8_pmuv3/br_mis_pred/u'],\n",
    "                row['armv8_pmuv3/br_pred/u'],\n",
    "                row['armv8_pmuv3/br_retired/u'],\n",
    "                row['armv8_pmuv3/inst_spec/u'],\n",
    "                row['armv8_pmuv3/inst_retired/u'],\n",
    "                row['armv8_pmuv3/l1d_cache/u'],\n",
    "                row['armv8_pmuv3/l1d_cache_refill/u'],\n",
    "                row['armv8_pmuv3/l1d_cache_wb/u'],\n",
    "                row['armv8_pmuv3/l1d_tlb/u'],\n",
    "                row['armv8_pmuv3/l1d_tlb_refill/u'],\n",
    "                row['armv8_pmuv3/l1i_cache/u'],\n",
    "                row['armv8_pmuv3/l1i_cache_refill/u'],\n",
    "                row['armv8_pmuv3/l1i_tlb/u'],\n",
    "                row['armv8_pmuv3/l1i_tlb_refill/u'],\n",
    "                row['armv8_pmuv3/l2d_cache/u'],\n",
    "                row['armv8_pmuv3/l2d_cache_allocate/u'],\n",
    "                row['armv8_pmuv3/l2d_cache_refill/u'],\n",
    "                row['armv8_pmuv3/l2d_cache_wb/u'],\n",
    "                row['armv8_pmuv3/l2d_tlb/u'],\n",
    "                row['armv8_pmuv3/l2d_tlb_refill/u'],\n",
    "                row['armv8_pmuv3/l3d_cache/u'],\n",
    "                row['armv8_pmuv3/l3d_cache_allocate/u'],\n",
    "                row['armv8_pmuv3/l3d_cache_refill/u'],\n",
    "                row['armv8_pmuv3/l3d_cache_wb/u'],\n",
    "                row['armv8_pmuv3/mem_access/u'],\n",
    "                ipc])\n",
    "        all_features.extend(features)\n",
    "    return all_features\n",
    "\n",
    "\n",
    "def process_csv_ipc(file_paths, chunk_size):\n",
    "    all_ipc_values = []\n",
    "    for file_path in file_paths:\n",
    "        df = pd.read_csv(file_path + '_' + str(chunk_size) + '.csv')\n",
    "        df = df.dropna()\n",
    "        for _, row in df.iterrows():\n",
    "            all_ipc_values.append(row['instructions:u'] / row['cpu-cycles:u'])  \n",
    "    return all_ipc_values\n",
    "\n",
    "source = [\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_500',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_502',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_505',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_520',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_523',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_525',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_531',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_541',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_548',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_557',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_503',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_507',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_508',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_510',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_511',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_519',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_521',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_527',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_538',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_544',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_549',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_554',\n",
    "]\n",
    "    \n",
    "target = [\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_500',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_502',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_505',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_520',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_523',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_525',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_531',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_541',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_548',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_557',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_503',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_507',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_508',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_510',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_511',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_519',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_521',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_527',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_538',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_544',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_549',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_554',\n",
    "]\n",
    "\n",
    "chunks = [1000000000, 2000000000, 5000000000, 10000000000, 20000000000]\n",
    "for chunk_size in chunks:\n",
    "    in_order_hardware_counters_features = process_csv_feautures(source, chunk_size)\n",
    "    out_of_order_ipc = process_csv_ipc(target, chunk_size)\n",
    "    print('chunk_size: ' + str(chunk_size))\n",
    "    \n",
    "    data = {\n",
    "        'In Order': in_order_hardware_counters_features,\n",
    "        'Out of Order': out_of_order_ipc\n",
    "    }\n",
    "    \n",
    "    length = min(len(in_order_hardware_counters_features), len(out_of_order_ipc))\n",
    "    \n",
    "    # Shorten the longen array\n",
    "    in_order_hardware_counters_features = in_order_hardware_counters_features[:length]\n",
    "    out_of_order_ipc = out_of_order_ipc[:length]\n",
    "    \n",
    "    # separate features and target\n",
    "    X = pd.DataFrame(\n",
    "        in_order_hardware_counters_features, \n",
    "        columns=[\n",
    "            'instructions:u',\n",
    "            'cpu-cycles:u',\n",
    "            'armv8_pmuv3/stall_backend/u',\n",
    "            'armv8_pmuv3/stall_frontend/u',\n",
    "            'armv8_pmuv3/br_mis_pred/u',\n",
    "            'armv8_pmuv3/br_pred/u',\n",
    "            'armv8_pmuv3/br_retired/u',\n",
    "            'armv8_pmuv3/inst_spec/u',\n",
    "            'armv8_pmuv3/inst_retired/u',\n",
    "            'armv8_pmuv3/l1d_cache/u',\n",
    "            'armv8_pmuv3/l1d_cache_refill/u',\n",
    "            'armv8_pmuv3/l1d_cache_wb/u',\n",
    "            'armv8_pmuv3/l1d_tlb/u',\n",
    "            'armv8_pmuv3/l1d_tlb_refill/u',\n",
    "            'armv8_pmuv3/l1i_cache/u',\n",
    "            'armv8_pmuv3/l1i_cache_refill/u',\n",
    "            'armv8_pmuv3/l1i_tlb/u',\n",
    "            'armv8_pmuv3/l1i_tlb_refill/u',\n",
    "            'armv8_pmuv3/l2d_cache/u',\n",
    "            'armv8_pmuv3/l2d_cache_allocate/u',\n",
    "            'armv8_pmuv3/l2d_cache_refill/u',\n",
    "            'armv8_pmuv3/l2d_cache_wb/u',\n",
    "            'armv8_pmuv3/l2d_tlb/u',\n",
    "            'armv8_pmuv3/l2d_tlb_refill/u',\n",
    "            'armv8_pmuv3/l3d_cache/u',\n",
    "            'armv8_pmuv3/l3d_cache_allocate/u',\n",
    "            'armv8_pmuv3/l3d_cache_refill/u',\n",
    "            'armv8_pmuv3/l3d_cache_wb/u',\n",
    "            'armv8_pmuv3/mem_access/u',\n",
    "            'ipc'])  # Features\n",
    "    y = pd.Series(out_of_order_ipc) \n",
    "    \n",
    "    # split \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    rf_model = RandomForestRegressor(n_estimators=100)\n",
    "    gb_model = GradientBoostingRegressor(n_estimators=100)\n",
    "    ensemble_model = gb_model\n",
    "    #ensemble_model = VotingRegressor([('rf', rf_model), ('gb', gb_model)])\n",
    "    ensemble_model.fit(X_train, y_train)\n",
    "    \n",
    "    ensemble_predictions = ensemble_model.predict(X_test)\n",
    "    # Calculate R²\n",
    "    ensemble_r2 = r2_score(y_test, ensemble_predictions)\n",
    "    \n",
    "    #bruh wut\n",
    "    train_predictions = ensemble_model.predict(X_train)\n",
    "    test_predictions = ensemble_model.predict(X_test)\n",
    "    \n",
    "    # Calculate R² for training and testing sets\n",
    "    train_r2 = r2_score(y_train, train_predictions)\n",
    "    train_mape = mean_absolute_percentage_error(y_train, train_predictions)\n",
    "    test_r2 = r2_score(y_test, test_predictions)\n",
    "    test_mape = mean_absolute_percentage_error(y_test, test_predictions)\n",
    "    \n",
    "    \n",
    "    print(f\"Training R²: {train_r2}\")\n",
    "    print(f\"Testing R²: {test_r2}\")\n",
    "    print(f\"Training Mape: {train_mape}\")\n",
    "    print(f\"Testing Mape: {test_mape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f01428c-6bb4-456a-bfef-8d37b8b7b34c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_size: 1000000000\n",
      "Training R²: 0.761208923173745\n",
      "Testing R²: 0.7544301858923387\n",
      "Training Mape: 0.13794224463727361\n",
      "Testing Mape: 0.1394903123685226\n",
      "chunk_size: 2000000000\n",
      "Training R²: 0.7946997257162717\n",
      "Testing R²: 0.7632360612471405\n",
      "Training Mape: 0.12388706681966721\n",
      "Testing Mape: 0.12954491661265713\n",
      "chunk_size: 5000000000\n",
      "Training R²: 0.8558833857095969\n",
      "Testing R²: 0.7854155731611675\n",
      "Training Mape: 0.10438644769689547\n",
      "Testing Mape: 0.12086987447959778\n",
      "chunk_size: 10000000000\n",
      "Training R²: 0.9005756363753425\n",
      "Testing R²: 0.7868930296223301\n",
      "Training Mape: 0.08683386807568876\n",
      "Testing Mape: 0.10718014051047435\n",
      "chunk_size: 20000000000\n",
      "Training R²: 0.9227198850244636\n",
      "Testing R²: 0.7900861460122972\n",
      "Training Mape: 0.07885915514825077\n",
      "Testing Mape: 0.11197320678232948\n"
     ]
    }
   ],
   "source": [
    "# OOO to desktop\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "def process_csv_feautures(file_paths, chunk_size):\n",
    "    all_features = []\n",
    "    for file_path in file_paths:\n",
    "        df = pd.read_csv(file_path + '_' + str(chunk_size) + '.csv')\n",
    "        df = df.dropna()\n",
    "        features = []\n",
    "        for index, row in df.iterrows():\n",
    "            inst = row['instructions:u']\n",
    "            cyc = row['cpu-cycles:u']\n",
    "            ipc = inst/cyc\n",
    "            features.append([\n",
    "                row['instructions:u'],\n",
    "                row['cpu-cycles:u'],\n",
    "                row['armv8_pmuv3/stall_backend/u'],\n",
    "                row['armv8_pmuv3/stall_frontend/u'],\n",
    "                row['armv8_pmuv3/br_mis_pred/u'],\n",
    "                row['armv8_pmuv3/br_pred/u'],\n",
    "                row['armv8_pmuv3/br_retired/u'],\n",
    "                row['armv8_pmuv3/inst_spec/u'],\n",
    "                row['armv8_pmuv3/inst_retired/u'],\n",
    "                row['armv8_pmuv3/l1d_cache/u'],\n",
    "                row['armv8_pmuv3/l1d_cache_refill/u'],\n",
    "                row['armv8_pmuv3/l1d_cache_wb/u'],\n",
    "                row['armv8_pmuv3/l1d_tlb/u'],\n",
    "                row['armv8_pmuv3/l1d_tlb_refill/u'],\n",
    "                row['armv8_pmuv3/l1i_cache/u'],\n",
    "                row['armv8_pmuv3/l1i_cache_refill/u'],\n",
    "                row['armv8_pmuv3/l1i_tlb/u'],\n",
    "                row['armv8_pmuv3/l1i_tlb_refill/u'],\n",
    "                row['armv8_pmuv3/l2d_cache/u'],\n",
    "                row['armv8_pmuv3/l2d_cache_allocate/u'],\n",
    "                row['armv8_pmuv3/l2d_cache_refill/u'],\n",
    "                row['armv8_pmuv3/l2d_cache_wb/u'],\n",
    "                row['armv8_pmuv3/l2d_tlb/u'],\n",
    "                row['armv8_pmuv3/l2d_tlb_refill/u'],\n",
    "                row['armv8_pmuv3/l3d_cache/u'],\n",
    "                row['armv8_pmuv3/l3d_cache_allocate/u'],\n",
    "                row['armv8_pmuv3/l3d_cache_refill/u'],\n",
    "                row['armv8_pmuv3/l3d_cache_wb/u'],\n",
    "                row['armv8_pmuv3/mem_access/u'],\n",
    "                ipc])\n",
    "        all_features.extend(features)\n",
    "    return all_features\n",
    "\n",
    "\n",
    "def process_csv_ipc(file_paths, chunk_size):\n",
    "    all_ipc_values = []\n",
    "    for file_path in file_paths:\n",
    "        df = pd.read_csv(file_path + '_' + str(chunk_size) + '.csv')\n",
    "        df = df.dropna()\n",
    "        for _, row in df.iterrows():\n",
    "            all_ipc_values.append(row['instructions'] / row['cpu-cycles'])  \n",
    "    return all_ipc_values\n",
    "\n",
    "source = [\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_500',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_502',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_505',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_520',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_523',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_525',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_531',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_541',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_548',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_557',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_503',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_507',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_508',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_510',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_511',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_519',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_521',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_527',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_538',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_544',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_549',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_554',\n",
    "]\n",
    "    \n",
    "target = [\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_500',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_502',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_505',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_520',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_523',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_525',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_531',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_541',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_548',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_557',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_503',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_507',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_508',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_510',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_511',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_519',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_521',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_527',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_538',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_544',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_549',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_554',\n",
    "]\n",
    "chunks = [1000000000, 2000000000, 5000000000, 10000000000, 20000000000]\n",
    "for chunk_size in chunks:\n",
    "    in_order_hardware_counters_features = process_csv_feautures(source, chunk_size)\n",
    "    out_of_order_ipc = process_csv_ipc(target, chunk_size)\n",
    "    print('chunk_size: ' + str(chunk_size))\n",
    "    \n",
    "    data = {\n",
    "        'In Order': in_order_hardware_counters_features,\n",
    "        'Out of Order': out_of_order_ipc\n",
    "    }\n",
    "    \n",
    "    length = min(len(in_order_hardware_counters_features), len(out_of_order_ipc))\n",
    "    \n",
    "    # Shorten the longen array\n",
    "    in_order_hardware_counters_features = in_order_hardware_counters_features[:length]\n",
    "    out_of_order_ipc = out_of_order_ipc[:length]\n",
    "    \n",
    "    # separate features and target\n",
    "    X = pd.DataFrame(\n",
    "        in_order_hardware_counters_features, \n",
    "        columns=[\n",
    "            'instructions:u',\n",
    "            'cpu-cycles:u',\n",
    "            'armv8_pmuv3/stall_backend/u',\n",
    "            'armv8_pmuv3/stall_frontend/u',\n",
    "            'armv8_pmuv3/br_mis_pred/u',\n",
    "            'armv8_pmuv3/br_pred/u',\n",
    "            'armv8_pmuv3/br_retired/u',\n",
    "            'armv8_pmuv3/inst_spec/u',\n",
    "            'armv8_pmuv3/inst_retired/u',\n",
    "            'armv8_pmuv3/l1d_cache/u',\n",
    "            'armv8_pmuv3/l1d_cache_refill/u',\n",
    "            'armv8_pmuv3/l1d_cache_wb/u',\n",
    "            'armv8_pmuv3/l1d_tlb/u',\n",
    "            'armv8_pmuv3/l1d_tlb_refill/u',\n",
    "            'armv8_pmuv3/l1i_cache/u',\n",
    "            'armv8_pmuv3/l1i_cache_refill/u',\n",
    "            'armv8_pmuv3/l1i_tlb/u',\n",
    "            'armv8_pmuv3/l1i_tlb_refill/u',\n",
    "            'armv8_pmuv3/l2d_cache/u',\n",
    "            'armv8_pmuv3/l2d_cache_allocate/u',\n",
    "            'armv8_pmuv3/l2d_cache_refill/u',\n",
    "            'armv8_pmuv3/l2d_cache_wb/u',\n",
    "            'armv8_pmuv3/l2d_tlb/u',\n",
    "            'armv8_pmuv3/l2d_tlb_refill/u',\n",
    "            'armv8_pmuv3/l3d_cache/u',\n",
    "            'armv8_pmuv3/l3d_cache_allocate/u',\n",
    "            'armv8_pmuv3/l3d_cache_refill/u',\n",
    "            'armv8_pmuv3/l3d_cache_wb/u',\n",
    "            'armv8_pmuv3/mem_access/u',\n",
    "            'ipc'])  # Features\n",
    "    y = pd.Series(out_of_order_ipc) \n",
    "    \n",
    "    # split \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    rf_model = RandomForestRegressor(n_estimators=100)\n",
    "    gb_model = GradientBoostingRegressor(n_estimators=100)\n",
    "    ensemble_model = gb_model\n",
    "    #ensemble_model = VotingRegressor([('rf', rf_model), ('gb', gb_model)])\n",
    "    ensemble_model.fit(X_train, y_train)\n",
    "    \n",
    "    ensemble_predictions = ensemble_model.predict(X_test)\n",
    "    # Calculate R²\n",
    "    ensemble_r2 = r2_score(y_test, ensemble_predictions)\n",
    "    \n",
    "    #bruh wut\n",
    "    train_predictions = ensemble_model.predict(X_train)\n",
    "    test_predictions = ensemble_model.predict(X_test)\n",
    "    \n",
    "    # Calculate R² for training and testing sets\n",
    "    train_r2 = r2_score(y_train, train_predictions)\n",
    "    train_mape = mean_absolute_percentage_error(y_train, train_predictions)\n",
    "    test_r2 = r2_score(y_test, test_predictions)\n",
    "    test_mape = mean_absolute_percentage_error(y_test, test_predictions)\n",
    "    \n",
    "    \n",
    "    print(f\"Training R²: {train_r2}\")\n",
    "    print(f\"Testing R²: {test_r2}\")\n",
    "    print(f\"Training Mape: {train_mape}\")\n",
    "    print(f\"Testing Mape: {test_mape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06a70f2c-cf59-42c3-b5a5-4314d11da410",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_size: 1000000000\n",
      "Training R²: 0.7953919698712484\n",
      "Testing R²: 0.7883923831092776\n",
      "Training Mape: 0.10511711227650125\n",
      "Testing Mape: 0.10519044707616329\n",
      "chunk_size: 2000000000\n",
      "Training R²: 0.8234162377403759\n",
      "Testing R²: 0.7950323022023714\n",
      "Training Mape: 0.09535139601877823\n",
      "Testing Mape: 0.10183901132201915\n",
      "chunk_size: 5000000000\n",
      "Training R²: 0.8643771186445893\n",
      "Testing R²: 0.81386045628594\n",
      "Training Mape: 0.08080926467530662\n",
      "Testing Mape: 0.09018442661394242\n",
      "chunk_size: 10000000000\n",
      "Training R²: 0.8915108348181636\n",
      "Testing R²: 0.821651299486331\n",
      "Training Mape: 0.07283456258522264\n",
      "Testing Mape: 0.08533094352212348\n",
      "chunk_size: 20000000000\n",
      "Training R²: 0.9176876179971158\n",
      "Testing R²: 0.8182531462284474\n",
      "Training Mape: 0.06153235032400901\n",
      "Testing Mape: 0.08349336046356916\n"
     ]
    }
   ],
   "source": [
    "# OOO to server\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "def process_csv_feautures(file_paths, chunk_size):\n",
    "    all_features = []\n",
    "    for file_path in file_paths:\n",
    "        df = pd.read_csv(file_path + '_' + str(chunk_size) + '.csv')\n",
    "        df = df.dropna()\n",
    "        features = []\n",
    "        for index, row in df.iterrows():\n",
    "            inst = row['instructions:u']\n",
    "            cyc = row['cpu-cycles:u']\n",
    "            ipc = inst/cyc\n",
    "            features.append([\n",
    "                row['instructions:u'],\n",
    "                row['cpu-cycles:u'],\n",
    "                row['armv8_pmuv3/stall_backend/u'],\n",
    "                row['armv8_pmuv3/stall_frontend/u'],\n",
    "                row['armv8_pmuv3/br_mis_pred/u'],\n",
    "                row['armv8_pmuv3/br_pred/u'],\n",
    "                row['armv8_pmuv3/br_retired/u'],\n",
    "                row['armv8_pmuv3/inst_spec/u'],\n",
    "                row['armv8_pmuv3/inst_retired/u'],\n",
    "                row['armv8_pmuv3/l1d_cache/u'],\n",
    "                row['armv8_pmuv3/l1d_cache_refill/u'],\n",
    "                row['armv8_pmuv3/l1d_cache_wb/u'],\n",
    "                row['armv8_pmuv3/l1d_tlb/u'],\n",
    "                row['armv8_pmuv3/l1d_tlb_refill/u'],\n",
    "                row['armv8_pmuv3/l1i_cache/u'],\n",
    "                row['armv8_pmuv3/l1i_cache_refill/u'],\n",
    "                row['armv8_pmuv3/l1i_tlb/u'],\n",
    "                row['armv8_pmuv3/l1i_tlb_refill/u'],\n",
    "                row['armv8_pmuv3/l2d_cache/u'],\n",
    "                row['armv8_pmuv3/l2d_cache_allocate/u'],\n",
    "                row['armv8_pmuv3/l2d_cache_refill/u'],\n",
    "                row['armv8_pmuv3/l2d_cache_wb/u'],\n",
    "                row['armv8_pmuv3/l2d_tlb/u'],\n",
    "                row['armv8_pmuv3/l2d_tlb_refill/u'],\n",
    "                row['armv8_pmuv3/l3d_cache/u'],\n",
    "                row['armv8_pmuv3/l3d_cache_allocate/u'],\n",
    "                row['armv8_pmuv3/l3d_cache_refill/u'],\n",
    "                row['armv8_pmuv3/l3d_cache_wb/u'],\n",
    "                row['armv8_pmuv3/mem_access/u'],\n",
    "                ipc])\n",
    "        all_features.extend(features)\n",
    "    return all_features\n",
    "\n",
    "\n",
    "def process_csv_ipc(file_paths, chunk_size):\n",
    "    all_ipc_values = []\n",
    "    for file_path in file_paths:\n",
    "        df = pd.read_csv(file_path + '_' + str(chunk_size) + '.csv')\n",
    "        df = df.dropna()\n",
    "        for _, row in df.iterrows():\n",
    "            all_ipc_values.append(row['instructions'] / row['cpu-cycles'])  \n",
    "    return all_ipc_values\n",
    "\n",
    "source = [\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_500',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_502',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_505',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_520',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_523',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_525',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_531',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_541',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_548',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_557',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_503',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_507',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_508',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_510',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_511',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_519',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_521',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_527',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_538',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_544',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_549',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_554',\n",
    "]\n",
    "    \n",
    "target = [\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_500',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_502',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_505',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_520',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_523',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_525',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_531',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_541',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_548',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_557',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_503',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_507',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_508',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_510',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_511',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_519',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_521',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_527',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_538',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_544',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_549',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_554',\n",
    "]\n",
    "\n",
    "chunks = [1000000000, 2000000000, 5000000000, 10000000000, 20000000000]\n",
    "for chunk_size in chunks:\n",
    "    in_order_hardware_counters_features = process_csv_feautures(source, chunk_size)\n",
    "    out_of_order_ipc = process_csv_ipc(target, chunk_size)\n",
    "    print('chunk_size: ' + str(chunk_size))\n",
    "    \n",
    "    data = {\n",
    "        'In Order': in_order_hardware_counters_features,\n",
    "        'Out of Order': out_of_order_ipc\n",
    "    }\n",
    "    \n",
    "    length = min(len(in_order_hardware_counters_features), len(out_of_order_ipc))\n",
    "    \n",
    "    # Shorten the longen array\n",
    "    in_order_hardware_counters_features = in_order_hardware_counters_features[:length]\n",
    "    out_of_order_ipc = out_of_order_ipc[:length]\n",
    "    \n",
    "    # separate features and target\n",
    "    X = pd.DataFrame(\n",
    "        in_order_hardware_counters_features, \n",
    "        columns=[\n",
    "            'instructions:u',\n",
    "            'cpu-cycles:u',\n",
    "            'armv8_pmuv3/stall_backend/u',\n",
    "            'armv8_pmuv3/stall_frontend/u',\n",
    "            'armv8_pmuv3/br_mis_pred/u',\n",
    "            'armv8_pmuv3/br_pred/u',\n",
    "            'armv8_pmuv3/br_retired/u',\n",
    "            'armv8_pmuv3/inst_spec/u',\n",
    "            'armv8_pmuv3/inst_retired/u',\n",
    "            'armv8_pmuv3/l1d_cache/u',\n",
    "            'armv8_pmuv3/l1d_cache_refill/u',\n",
    "            'armv8_pmuv3/l1d_cache_wb/u',\n",
    "            'armv8_pmuv3/l1d_tlb/u',\n",
    "            'armv8_pmuv3/l1d_tlb_refill/u',\n",
    "            'armv8_pmuv3/l1i_cache/u',\n",
    "            'armv8_pmuv3/l1i_cache_refill/u',\n",
    "            'armv8_pmuv3/l1i_tlb/u',\n",
    "            'armv8_pmuv3/l1i_tlb_refill/u',\n",
    "            'armv8_pmuv3/l2d_cache/u',\n",
    "            'armv8_pmuv3/l2d_cache_allocate/u',\n",
    "            'armv8_pmuv3/l2d_cache_refill/u',\n",
    "            'armv8_pmuv3/l2d_cache_wb/u',\n",
    "            'armv8_pmuv3/l2d_tlb/u',\n",
    "            'armv8_pmuv3/l2d_tlb_refill/u',\n",
    "            'armv8_pmuv3/l3d_cache/u',\n",
    "            'armv8_pmuv3/l3d_cache_allocate/u',\n",
    "            'armv8_pmuv3/l3d_cache_refill/u',\n",
    "            'armv8_pmuv3/l3d_cache_wb/u',\n",
    "            'armv8_pmuv3/mem_access/u',\n",
    "            'ipc'])  # Features\n",
    "    y = pd.Series(out_of_order_ipc) \n",
    "    \n",
    "    # split \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    rf_model = RandomForestRegressor(n_estimators=100)\n",
    "    gb_model = GradientBoostingRegressor(n_estimators=100)\n",
    "    ensemble_model = gb_model\n",
    "    #ensemble_model = VotingRegressor([('rf', rf_model), ('gb', gb_model)])\n",
    "    ensemble_model.fit(X_train, y_train)\n",
    "    \n",
    "    ensemble_predictions = ensemble_model.predict(X_test)\n",
    "    # Calculate R²\n",
    "    ensemble_r2 = r2_score(y_test, ensemble_predictions)\n",
    "    \n",
    "    #bruh wut\n",
    "    train_predictions = ensemble_model.predict(X_train)\n",
    "    test_predictions = ensemble_model.predict(X_test)\n",
    "    \n",
    "    # Calculate R² for training and testing sets\n",
    "    train_r2 = r2_score(y_train, train_predictions)\n",
    "    train_mape = mean_absolute_percentage_error(y_train, train_predictions)\n",
    "    test_r2 = r2_score(y_test, test_predictions)\n",
    "    test_mape = mean_absolute_percentage_error(y_test, test_predictions)\n",
    "    \n",
    "    \n",
    "    print(f\"Training R²: {train_r2}\")\n",
    "    print(f\"Testing R²: {test_r2}\")\n",
    "    print(f\"Training Mape: {train_mape}\")\n",
    "    print(f\"Testing Mape: {test_mape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6dc43da5-edad-4f8b-b5f3-ccd64577191b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_size: 1000000000\n",
      "Training R²: 0.7164028408080513\n",
      "Testing R²: 0.711908908703033\n",
      "Training Mape: 0.1260095504238039\n",
      "Testing Mape: 0.12808632909600512\n",
      "chunk_size: 2000000000\n",
      "Training R²: 0.7866494086107454\n",
      "Testing R²: 0.7667430133414316\n",
      "Training Mape: 0.10323937342382715\n",
      "Testing Mape: 0.1082613177343805\n",
      "chunk_size: 5000000000\n",
      "Training R²: 0.8559967729792806\n",
      "Testing R²: 0.8106362336982278\n",
      "Training Mape: 0.08223983036882627\n",
      "Testing Mape: 0.089423131580187\n",
      "chunk_size: 10000000000\n",
      "Training R²: 0.898024515641687\n",
      "Testing R²: 0.8281995418601392\n",
      "Training Mape: 0.06965088598793484\n",
      "Testing Mape: 0.08440637919438503\n",
      "chunk_size: 20000000000\n",
      "Training R²: 0.9219748891071893\n",
      "Testing R²: 0.8095961160996125\n",
      "Training Mape: 0.05901438827494415\n",
      "Testing Mape: 0.08380700864718284\n"
     ]
    }
   ],
   "source": [
    "# desktop to server\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "def process_csv_feautures(file_paths, chunk_size):\n",
    "    all_features = []\n",
    "    for file_path in file_paths:\n",
    "        df = pd.read_csv(file_path + '_' + str(chunk_size) + '.csv')\n",
    "        df = df.dropna()\n",
    "        features = []\n",
    "        for index, row in df.iterrows():\n",
    "            inst = row['instructions']\n",
    "            cyc = row['cpu-cycles']\n",
    "            ipc = inst/cyc\n",
    "            features.append([\n",
    "                row['instructions'],\n",
    "                row['cpu-cycles'],\n",
    "                row['br_pred'],\n",
    "                row['br_mis_pred'],\n",
    "                row['l1d_cache_rd'],\n",
    "                row['l1d_cache_wr'],\n",
    "                row['l1d_cache'],\n",
    "                row['l1i_cache'],\n",
    "                row['l1i_cache_refill'],\n",
    "                row['context-switches'],\n",
    "                row['l2d_cache_rd'],\n",
    "                row['l2d_cache_wr'],\n",
    "                row['l2d_cache'],\n",
    "                row['l1d_tlb'],\n",
    "                row['l1d_tlb_refill_rd'],\n",
    "                row['l1d_tlb_refill_wr'],\n",
    "                row['dtlb_walk'],\n",
    "                row['itlb_walk'],\n",
    "                row['page-faults'],\n",
    "                row['l2d_tlb_access'],\n",
    "                row['l2i_tlb_access'],\n",
    "                row['l1i_tlb_refill'],\n",
    "                row['iTLB-loads'],\n",
    "                row['iTLB-load-misses'],\n",
    "                row['branch-loads'],\n",
    "                row['dTLB-loads'],\n",
    "                row['dTLB-load-misses'],\n",
    "                row['branch-load-misses'],\n",
    "                row['vfp_spec'],\n",
    "                row['inst_spec'],\n",
    "                row['ase_spec'],\n",
    "                row['bx_stall'],\n",
    "                row['decode_stall'],\n",
    "                row['dispatch_stall'],\n",
    "                row['fx_stall'],\n",
    "                row['ixa_stall'],\n",
    "                row['ixb_stall'],\n",
    "                row['lx_stall'],\n",
    "                row['sx_stall'],\n",
    "                row['bus_access'],\n",
    "                row['mem_access'],\n",
    "                row['mem_access_rd'],\n",
    "                row['mem_access_wr'],\n",
    "                ipc])\n",
    "        all_features.extend(features)\n",
    "    return all_features\n",
    "\n",
    "\n",
    "def process_csv_ipc(file_paths, chunk_size):\n",
    "    all_ipc_values = []\n",
    "    for file_path in file_paths:\n",
    "        df = pd.read_csv(file_path + '_' + str(chunk_size) + '.csv')\n",
    "        df = df.dropna()\n",
    "        for _, row in df.iterrows():\n",
    "            all_ipc_values.append(row['instructions'] / row['cpu-cycles'])  \n",
    "    return all_ipc_values\n",
    "\n",
    "source = [\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_500',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_502',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_505',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_520',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_523',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_525',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_531',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_541',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_548',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_557',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_503',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_507',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_508',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_510',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_511',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_519',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_521',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_527',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_538',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_544',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_549',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_554',\n",
    "]\n",
    "    \n",
    "target = [\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_500',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_502',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_505',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_520',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_523',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_525',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_531',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_541',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_548',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_557',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_503',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_507',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_508',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_510',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_511',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_519',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_521',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_527',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_538',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_544',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_549',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_3.0GHz_554',\n",
    "]\n",
    "\n",
    "chunks = [1000000000, 2000000000, 5000000000, 10000000000, 20000000000]\n",
    "for chunk_size in chunks:\n",
    "    in_order_hardware_counters_features = process_csv_feautures(source, chunk_size)\n",
    "    out_of_order_ipc = process_csv_ipc(target, chunk_size)\n",
    "    print('chunk_size: ' + str(chunk_size))\n",
    "    \n",
    "    data = {\n",
    "        'In Order': in_order_hardware_counters_features,\n",
    "        'Out of Order': out_of_order_ipc\n",
    "    }\n",
    "    \n",
    "    length = min(len(in_order_hardware_counters_features), len(out_of_order_ipc))\n",
    "    \n",
    "    # Shorten the longen array\n",
    "    in_order_hardware_counters_features = in_order_hardware_counters_features[:length]\n",
    "    out_of_order_ipc = out_of_order_ipc[:length]\n",
    "    \n",
    "    # separate features and target\n",
    "    X = pd.DataFrame(\n",
    "        in_order_hardware_counters_features, \n",
    "        columns=[\n",
    "            'instructions',\n",
    "            'cpu-cycles',\n",
    "            'br_pred',\n",
    "            'br_mis_pred',\n",
    "            'l1d_cache_rd',\n",
    "            'l1d_cache_wr',\n",
    "            'l1d_cache',\n",
    "            'l1i_cache',\n",
    "            'l1i_cache_refill',\n",
    "            'context-switches',\n",
    "            'l2d_cache_rd',\n",
    "            'l2d_cache_wr',\n",
    "            'l2d_cache',\n",
    "            'l1d_tlb',\n",
    "            'l1d_tlb_refill_rd',\n",
    "            'l1d_tlb_refill_wr',\n",
    "            'dtlb_walk',\n",
    "            'itlb_walk',\n",
    "            'page-faults',\n",
    "            'l2d_tlb_access',\n",
    "            'l2i_tlb_access',\n",
    "            'l1i_tlb_refill',\n",
    "            'iTLB-loads',\n",
    "            'iTLB-load-misses',\n",
    "            'branch-loads',\n",
    "            'dTLB-loads',\n",
    "            'dTLB-load-misses',\n",
    "            'branch-load-misses',\n",
    "            'vfp_spec',\n",
    "            'inst_spec',\n",
    "            'ase_spec',\n",
    "            'bx_stall',\n",
    "            'decode_stall',\n",
    "            'dispatch_stall',\n",
    "            'fx_stall',\n",
    "            'ixa_stall',\n",
    "            'ixb_stall',\n",
    "            'lx_stall',\n",
    "            'sx_stall',\n",
    "            'bus_access',\n",
    "            'mem_access',\n",
    "            'mem_access_rd',\n",
    "            'mem_access_wr',\n",
    "            'ipc'])  # Features\n",
    "    y = pd.Series(out_of_order_ipc) \n",
    "    \n",
    "    # split \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    rf_model = RandomForestRegressor(n_estimators=100)\n",
    "    gb_model = GradientBoostingRegressor(n_estimators=100)\n",
    "    ensemble_model = gb_model\n",
    "    #ensemble_model = VotingRegressor([('rf', rf_model), ('gb', gb_model)])\n",
    "    ensemble_model.fit(X_train, y_train)\n",
    "    \n",
    "    ensemble_predictions = ensemble_model.predict(X_test)\n",
    "    # Calculate R²\n",
    "    ensemble_r2 = r2_score(y_test, ensemble_predictions)\n",
    "    \n",
    "    #bruh wut\n",
    "    train_predictions = ensemble_model.predict(X_train)\n",
    "    test_predictions = ensemble_model.predict(X_test)\n",
    "    \n",
    "    # Calculate R² for training and testing sets\n",
    "    train_r2 = r2_score(y_train, train_predictions)\n",
    "    train_mape = mean_absolute_percentage_error(y_train, train_predictions)\n",
    "    test_r2 = r2_score(y_test, test_predictions)\n",
    "    test_mape = mean_absolute_percentage_error(y_test, test_predictions)\n",
    "    \n",
    "    \n",
    "    print(f\"Training R²: {train_r2}\")\n",
    "    print(f\"Testing R²: {test_r2}\")\n",
    "    print(f\"Training Mape: {train_mape}\")\n",
    "    print(f\"Testing Mape: {test_mape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2983595c-dd6d-4752-835a-596ca83e0da6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_size: 1000000000\n",
      "Training R²: 0.7681905034887146\n",
      "Testing R²: 0.75103042071978\n",
      "Training Mape: 0.20766488407016598\n",
      "Testing Mape: 0.21240815832016677\n",
      "chunk_size: 2000000000\n",
      "Training R²: 0.8048919607315685\n",
      "Testing R²: 0.7846317555043021\n",
      "Training Mape: 0.18665674452622577\n",
      "Testing Mape: 0.19013275094554902\n",
      "chunk_size: 5000000000\n",
      "Training R²: 0.8465978703919669\n",
      "Testing R²: 0.8143178877191011\n",
      "Training Mape: 0.16122188240636906\n",
      "Testing Mape: 0.1658640390565357\n",
      "chunk_size: 10000000000\n",
      "Training R²: 0.8790775376307212\n",
      "Testing R²: 0.8037202616525296\n",
      "Training Mape: 0.14055683309009886\n",
      "Testing Mape: 0.17030839804160114\n",
      "chunk_size: 20000000000\n",
      "Training R²: 0.901458618474261\n",
      "Testing R²: 0.7658292172705863\n",
      "Training Mape: 0.13059676639638815\n",
      "Testing Mape: 0.19408688258903006\n"
     ]
    }
   ],
   "source": [
    "# desktop to InO\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "def process_csv_feautures(file_paths, chunk_size):\n",
    "    all_features = []\n",
    "    for file_path in file_paths:\n",
    "        df = pd.read_csv(file_path + '_' + str(chunk_size) + '.csv')\n",
    "        df = df.dropna()\n",
    "        features = []\n",
    "        for index, row in df.iterrows():\n",
    "            inst = row['instructions']\n",
    "            cyc = row['cpu-cycles']\n",
    "            ipc = inst/cyc\n",
    "            features.append([\n",
    "                row['instructions'],\n",
    "                row['cpu-cycles'],\n",
    "                row['br_pred'],\n",
    "                row['br_mis_pred'],\n",
    "                row['l1d_cache_rd'],\n",
    "                row['l1d_cache_wr'],\n",
    "                row['l1d_cache'],\n",
    "                row['l1i_cache'],\n",
    "                row['l1i_cache_refill'],\n",
    "                row['context-switches'],\n",
    "                row['l2d_cache_rd'],\n",
    "                row['l2d_cache_wr'],\n",
    "                row['l2d_cache'],\n",
    "                row['l1d_tlb'],\n",
    "                row['l1d_tlb_refill_rd'],\n",
    "                row['l1d_tlb_refill_wr'],\n",
    "                row['dtlb_walk'],\n",
    "                row['itlb_walk'],\n",
    "                row['page-faults'],\n",
    "                row['l2d_tlb_access'],\n",
    "                row['l2i_tlb_access'],\n",
    "                row['l1i_tlb_refill'],\n",
    "                row['iTLB-loads'],\n",
    "                row['iTLB-load-misses'],\n",
    "                row['branch-loads'],\n",
    "                row['dTLB-loads'],\n",
    "                row['dTLB-load-misses'],\n",
    "                row['branch-load-misses'],\n",
    "                row['vfp_spec'],\n",
    "                row['inst_spec'],\n",
    "                row['ase_spec'],\n",
    "                row['bx_stall'],\n",
    "                row['decode_stall'],\n",
    "                row['dispatch_stall'],\n",
    "                row['fx_stall'],\n",
    "                row['ixa_stall'],\n",
    "                row['ixb_stall'],\n",
    "                row['lx_stall'],\n",
    "                row['sx_stall'],\n",
    "                row['bus_access'],\n",
    "                row['mem_access'],\n",
    "                row['mem_access_rd'],\n",
    "                row['mem_access_wr'],\n",
    "                ipc])\n",
    "        all_features.extend(features)\n",
    "    return all_features\n",
    "\n",
    "\n",
    "def process_csv_ipc(file_paths, chunk_size):\n",
    "    all_ipc_values = []\n",
    "    for file_path in file_paths:\n",
    "        df = pd.read_csv(file_path + '_' + str(chunk_size) + '.csv')\n",
    "        df = df.dropna()\n",
    "        for _, row in df.iterrows():\n",
    "            all_ipc_values.append(row['instructions:u'] / row['cpu-cycles:u'])  \n",
    "    return all_ipc_values\n",
    "\n",
    "source = [\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_500',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_502',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_505',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_520',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_523',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_525',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_531',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_541',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_548',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_557',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_503',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_507',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_508',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_510',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_511',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_519',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_521',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_527',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_538',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_544',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_549',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_554',\n",
    "]\n",
    "    \n",
    "target = [\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_500',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_502',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_505',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_520',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_523',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_525',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_531',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_541',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_548',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_557',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_503',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_507',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_508',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_510',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_511',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_519',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_521',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_527',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_538',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_544',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_549',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_554',\n",
    "]\n",
    "\n",
    "chunks = [1000000000, 2000000000, 5000000000, 10000000000, 20000000000]\n",
    "for chunk_size in chunks:\n",
    "    in_order_hardware_counters_features = process_csv_feautures(source, chunk_size)\n",
    "    out_of_order_ipc = process_csv_ipc(target, chunk_size)\n",
    "    print('chunk_size: ' + str(chunk_size))\n",
    "    \n",
    "    data = {\n",
    "        'In Order': in_order_hardware_counters_features,\n",
    "        'Out of Order': out_of_order_ipc\n",
    "    }\n",
    "    \n",
    "    length = min(len(in_order_hardware_counters_features), len(out_of_order_ipc))\n",
    "    \n",
    "    # Shorten the longen array\n",
    "    in_order_hardware_counters_features = in_order_hardware_counters_features[:length]\n",
    "    out_of_order_ipc = out_of_order_ipc[:length]\n",
    "    \n",
    "    # separate features and target\n",
    "    X = pd.DataFrame(\n",
    "        in_order_hardware_counters_features, \n",
    "        columns=[\n",
    "            'instructions',\n",
    "            'cpu-cycles',\n",
    "            'br_pred',\n",
    "            'br_mis_pred',\n",
    "            'l1d_cache_rd',\n",
    "            'l1d_cache_wr',\n",
    "            'l1d_cache',\n",
    "            'l1i_cache',\n",
    "            'l1i_cache_refill',\n",
    "            'context-switches',\n",
    "            'l2d_cache_rd',\n",
    "            'l2d_cache_wr',\n",
    "            'l2d_cache',\n",
    "            'l1d_tlb',\n",
    "            'l1d_tlb_refill_rd',\n",
    "            'l1d_tlb_refill_wr',\n",
    "            'dtlb_walk',\n",
    "            'itlb_walk',\n",
    "            'page-faults',\n",
    "            'l2d_tlb_access',\n",
    "            'l2i_tlb_access',\n",
    "            'l1i_tlb_refill',\n",
    "            'iTLB-loads',\n",
    "            'iTLB-load-misses',\n",
    "            'branch-loads',\n",
    "            'dTLB-loads',\n",
    "            'dTLB-load-misses',\n",
    "            'branch-load-misses',\n",
    "            'vfp_spec',\n",
    "            'inst_spec',\n",
    "            'ase_spec',\n",
    "            'bx_stall',\n",
    "            'decode_stall',\n",
    "            'dispatch_stall',\n",
    "            'fx_stall',\n",
    "            'ixa_stall',\n",
    "            'ixb_stall',\n",
    "            'lx_stall',\n",
    "            'sx_stall',\n",
    "            'bus_access',\n",
    "            'mem_access',\n",
    "            'mem_access_rd',\n",
    "            'mem_access_wr',\n",
    "            'ipc'])  # Features\n",
    "    y = pd.Series(out_of_order_ipc) \n",
    "    \n",
    "    # split \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    rf_model = RandomForestRegressor(n_estimators=100)\n",
    "    gb_model = GradientBoostingRegressor(n_estimators=100)\n",
    "    ensemble_model = gb_model\n",
    "    #ensemble_model = VotingRegressor([('rf', rf_model), ('gb', gb_model)])\n",
    "    ensemble_model.fit(X_train, y_train)\n",
    "    \n",
    "    ensemble_predictions = ensemble_model.predict(X_test)\n",
    "    # Calculate R²\n",
    "    ensemble_r2 = r2_score(y_test, ensemble_predictions)\n",
    "    \n",
    "    #bruh wut\n",
    "    train_predictions = ensemble_model.predict(X_train)\n",
    "    test_predictions = ensemble_model.predict(X_test)\n",
    "    \n",
    "    # Calculate R² for training and testing sets\n",
    "    train_r2 = r2_score(y_train, train_predictions)\n",
    "    train_mape = mean_absolute_percentage_error(y_train, train_predictions)\n",
    "    test_r2 = r2_score(y_test, test_predictions)\n",
    "    test_mape = mean_absolute_percentage_error(y_test, test_predictions)\n",
    "    \n",
    "    \n",
    "    print(f\"Training R²: {train_r2}\")\n",
    "    print(f\"Testing R²: {test_r2}\")\n",
    "    print(f\"Training Mape: {train_mape}\")\n",
    "    print(f\"Testing Mape: {test_mape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2bb2831e-56ab-49c6-b9d7-6a047be1c524",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_size: 1000000000\n",
      "Training R²: 0.7292282709498279\n",
      "Testing R²: 0.7269668361489405\n",
      "Training Mape: 0.135210109700634\n",
      "Testing Mape: 0.1337559315277693\n",
      "chunk_size: 2000000000\n",
      "Training R²: 0.7602566098373049\n",
      "Testing R²: 0.7433977152101148\n",
      "Training Mape: 0.12289585966464808\n",
      "Testing Mape: 0.13100429082834475\n",
      "chunk_size: 5000000000\n",
      "Training R²: 0.8348287612713305\n",
      "Testing R²: 0.7741977416694028\n",
      "Training Mape: 0.10128875189988004\n",
      "Testing Mape: 0.10435203395692301\n",
      "chunk_size: 10000000000\n",
      "Training R²: 0.8906711209324534\n",
      "Testing R²: 0.7948232289430217\n",
      "Training Mape: 0.08314660508275007\n",
      "Testing Mape: 0.10646621779945556\n",
      "chunk_size: 20000000000\n",
      "Training R²: 0.9165349727967995\n",
      "Testing R²: 0.7431618258980827\n",
      "Training Mape: 0.07376172618660941\n",
      "Testing Mape: 0.11868468399981931\n"
     ]
    }
   ],
   "source": [
    "# desktop to OOO\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "def process_csv_feautures(file_paths, chunk_size):\n",
    "    all_features = []\n",
    "    for file_path in file_paths:\n",
    "        df = pd.read_csv(file_path + '_' + str(chunk_size) + '.csv')\n",
    "        df = df.dropna()\n",
    "        features = []\n",
    "        for index, row in df.iterrows():\n",
    "            inst = row['instructions']\n",
    "            cyc = row['cpu-cycles']\n",
    "            ipc = inst/cyc\n",
    "            features.append([\n",
    "                row['instructions'],\n",
    "                row['cpu-cycles'],\n",
    "                row['br_pred'],\n",
    "                row['br_mis_pred'],\n",
    "                row['l1d_cache_rd'],\n",
    "                row['l1d_cache_wr'],\n",
    "                row['l1d_cache'],\n",
    "                row['l1i_cache'],\n",
    "                row['l1i_cache_refill'],\n",
    "                row['context-switches'],\n",
    "                row['l2d_cache_rd'],\n",
    "                row['l2d_cache_wr'],\n",
    "                row['l2d_cache'],\n",
    "                row['l1d_tlb'],\n",
    "                row['l1d_tlb_refill_rd'],\n",
    "                row['l1d_tlb_refill_wr'],\n",
    "                row['dtlb_walk'],\n",
    "                row['itlb_walk'],\n",
    "                row['page-faults'],\n",
    "                row['l2d_tlb_access'],\n",
    "                row['l2i_tlb_access'],\n",
    "                row['l1i_tlb_refill'],\n",
    "                row['iTLB-loads'],\n",
    "                row['iTLB-load-misses'],\n",
    "                row['branch-loads'],\n",
    "                row['dTLB-loads'],\n",
    "                row['dTLB-load-misses'],\n",
    "                row['branch-load-misses'],\n",
    "                row['vfp_spec'],\n",
    "                row['inst_spec'],\n",
    "                row['ase_spec'],\n",
    "                row['bx_stall'],\n",
    "                row['decode_stall'],\n",
    "                row['dispatch_stall'],\n",
    "                row['fx_stall'],\n",
    "                row['ixa_stall'],\n",
    "                row['ixb_stall'],\n",
    "                row['lx_stall'],\n",
    "                row['sx_stall'],\n",
    "                row['bus_access'],\n",
    "                row['mem_access'],\n",
    "                row['mem_access_rd'],\n",
    "                row['mem_access_wr'],\n",
    "                ipc])\n",
    "        all_features.extend(features)\n",
    "    return all_features\n",
    "\n",
    "\n",
    "def process_csv_ipc(file_paths, chunk_size):\n",
    "    all_ipc_values = []\n",
    "    for file_path in file_paths:\n",
    "        df = pd.read_csv(file_path + '_' + str(chunk_size) + '.csv')\n",
    "        df = df.dropna()\n",
    "        for _, row in df.iterrows():\n",
    "            all_ipc_values.append(row['instructions:u'] / row['cpu-cycles:u'])  \n",
    "    return all_ipc_values\n",
    "\n",
    "source = [\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_500',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_502',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_505',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_520',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_523',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_525',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_531',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_541',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_548',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_557',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_503',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_507',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_508',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_510',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_511',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_519',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_521',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_527',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_538',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_544',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_549',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_554',\n",
    "]\n",
    "    \n",
    "target = [\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_500',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_502',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_505',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_520',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_523',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_525',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_531',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_541',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_548',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_557',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_503',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_507',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_508',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_510',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_511',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_519',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_521',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_527',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_538',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_544',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_549',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_554',\n",
    "]\n",
    "\n",
    "chunks = [1000000000, 2000000000, 5000000000, 10000000000, 20000000000]\n",
    "for chunk_size in chunks:\n",
    "    in_order_hardware_counters_features = process_csv_feautures(source, chunk_size)\n",
    "    out_of_order_ipc = process_csv_ipc(target, chunk_size)\n",
    "    print('chunk_size: ' + str(chunk_size))\n",
    "    \n",
    "    data = {\n",
    "        'In Order': in_order_hardware_counters_features,\n",
    "        'Out of Order': out_of_order_ipc\n",
    "    }\n",
    "    \n",
    "    length = min(len(in_order_hardware_counters_features), len(out_of_order_ipc))\n",
    "    \n",
    "    # Shorten the longen array\n",
    "    in_order_hardware_counters_features = in_order_hardware_counters_features[:length]\n",
    "    out_of_order_ipc = out_of_order_ipc[:length]\n",
    "    \n",
    "    # separate features and target\n",
    "    X = pd.DataFrame(\n",
    "        in_order_hardware_counters_features, \n",
    "        columns=[\n",
    "            'instructions',\n",
    "            'cpu-cycles',\n",
    "            'br_pred',\n",
    "            'br_mis_pred',\n",
    "            'l1d_cache_rd',\n",
    "            'l1d_cache_wr',\n",
    "            'l1d_cache',\n",
    "            'l1i_cache',\n",
    "            'l1i_cache_refill',\n",
    "            'context-switches',\n",
    "            'l2d_cache_rd',\n",
    "            'l2d_cache_wr',\n",
    "            'l2d_cache',\n",
    "            'l1d_tlb',\n",
    "            'l1d_tlb_refill_rd',\n",
    "            'l1d_tlb_refill_wr',\n",
    "            'dtlb_walk',\n",
    "            'itlb_walk',\n",
    "            'page-faults',\n",
    "            'l2d_tlb_access',\n",
    "            'l2i_tlb_access',\n",
    "            'l1i_tlb_refill',\n",
    "            'iTLB-loads',\n",
    "            'iTLB-load-misses',\n",
    "            'branch-loads',\n",
    "            'dTLB-loads',\n",
    "            'dTLB-load-misses',\n",
    "            'branch-load-misses',\n",
    "            'vfp_spec',\n",
    "            'inst_spec',\n",
    "            'ase_spec',\n",
    "            'bx_stall',\n",
    "            'decode_stall',\n",
    "            'dispatch_stall',\n",
    "            'fx_stall',\n",
    "            'ixa_stall',\n",
    "            'ixb_stall',\n",
    "            'lx_stall',\n",
    "            'sx_stall',\n",
    "            'bus_access',\n",
    "            'mem_access',\n",
    "            'mem_access_rd',\n",
    "            'mem_access_wr',\n",
    "            'ipc'])  # Features\n",
    "    y = pd.Series(out_of_order_ipc) \n",
    "    \n",
    "    # split \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    rf_model = RandomForestRegressor(n_estimators=100)\n",
    "    gb_model = GradientBoostingRegressor(n_estimators=100)\n",
    "    ensemble_model = gb_model\n",
    "    #ensemble_model = VotingRegressor([('rf', rf_model), ('gb', gb_model)])\n",
    "    ensemble_model.fit(X_train, y_train)\n",
    "    \n",
    "    ensemble_predictions = ensemble_model.predict(X_test)\n",
    "    # Calculate R²\n",
    "    ensemble_r2 = r2_score(y_test, ensemble_predictions)\n",
    "    \n",
    "    #bruh wut\n",
    "    train_predictions = ensemble_model.predict(X_train)\n",
    "    test_predictions = ensemble_model.predict(X_test)\n",
    "    \n",
    "    # Calculate R² for training and testing sets\n",
    "    train_r2 = r2_score(y_train, train_predictions)\n",
    "    train_mape = mean_absolute_percentage_error(y_train, train_predictions)\n",
    "    test_r2 = r2_score(y_test, test_predictions)\n",
    "    test_mape = mean_absolute_percentage_error(y_test, test_predictions)\n",
    "    \n",
    "    \n",
    "    print(f\"Training R²: {train_r2}\")\n",
    "    print(f\"Testing R²: {test_r2}\")\n",
    "    print(f\"Training Mape: {train_mape}\")\n",
    "    print(f\"Testing Mape: {test_mape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "242a57a0-408a-4931-907a-3f28e5b9663d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_size: 1000000000\n",
      "Training R²: 0.7828286130221445\n",
      "Testing R²: 0.775437223131473\n",
      "Training Mape: 0.20098233869816903\n",
      "Testing Mape: 0.20295054352803368\n",
      "chunk_size: 2000000000\n",
      "Training R²: 0.7773650363005499\n",
      "Testing R²: 0.7633906971760802\n",
      "Training Mape: 0.20436276290333363\n",
      "Testing Mape: 0.20815643173526102\n",
      "chunk_size: 5000000000\n",
      "Training R²: 0.8244725408738768\n",
      "Testing R²: 0.7787972442260931\n",
      "Training Mape: 0.17587574167143402\n",
      "Testing Mape: 0.20053347331433152\n",
      "chunk_size: 10000000000\n",
      "Training R²: 0.8513420992399294\n",
      "Testing R²: 0.7960026669900511\n",
      "Training Mape: 0.1641909860062184\n",
      "Testing Mape: 0.18463902726447476\n",
      "chunk_size: 20000000000\n",
      "Training R²: 0.8935190056216302\n",
      "Testing R²: 0.8351264088057747\n",
      "Training Mape: 0.13552700553611538\n",
      "Testing Mape: 0.18121803355415217\n"
     ]
    }
   ],
   "source": [
    "# server to InO\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "def process_csv_feautures(file_paths, chunk_size):\n",
    "    all_features = []\n",
    "    for file_path in file_paths:\n",
    "        df = pd.read_csv(file_path + '_' + str(chunk_size) + '.csv')\n",
    "        df = df.dropna()\n",
    "        features = []\n",
    "        for index, row in df.iterrows():\n",
    "            inst = row['instructions']\n",
    "            cyc = row['cpu-cycles']\n",
    "            ipc = inst/cyc\n",
    "            features.append([\n",
    "                row['instructions'],\n",
    "                row['cpu-cycles'], \n",
    "                row['br_pred'], \n",
    "                row['br_mis_pred'], \n",
    "                row['l1d_cache'], \n",
    "                row['l1d_tlb'], \n",
    "                row['l1d_tlb_rd'], \n",
    "                row['l1d_tlb_wr'], \n",
    "                row['l2d_cache_rd'], \n",
    "                row['l2d_cache_wr'], \n",
    "                row['l2d_cache'], \n",
    "                row['l1i_cache'], \n",
    "                row['l1i_cache_refill'], \n",
    "                row['remote_access'], \n",
    "                row['dtlb_walk'], \n",
    "                row['itlb_walk'], \n",
    "                row['l1i_tlb'], \n",
    "                row['l2d_tlb'], \n",
    "                row['l2d_tlb_rd'], \n",
    "                row['l2d_tlb_wr'], \n",
    "                row['vfp_spec'], \n",
    "                row['inst_spec'], \n",
    "                row['ase_spec'], \n",
    "                row['stall_backend'], \n",
    "                row['stall_frontend'], \n",
    "                row['ll_cache_miss_rd'], \n",
    "                row['mem_access'], \n",
    "                row['mem_access_rd'], \n",
    "                row['mem_access_wr'],\n",
    "                ipc])\n",
    "        all_features.extend(features)\n",
    "    return all_features\n",
    "\n",
    "\n",
    "def process_csv_ipc(file_paths, chunk_size):\n",
    "    all_ipc_values = []\n",
    "    for file_path in file_paths:\n",
    "        df = pd.read_csv(file_path + '_' + str(chunk_size) + '.csv')\n",
    "        df = df.dropna()\n",
    "        for _, row in df.iterrows():\n",
    "            all_ipc_values.append(row['instructions:u'] / row['cpu-cycles:u'])  \n",
    "    return all_ipc_values\n",
    "\n",
    "source = [\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_500',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_502',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_505',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_520',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_523',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_525',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_531',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_541',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_548',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_557',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_503',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_507',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_508',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_510',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_511',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_519',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_521',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_527',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_538',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_544',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_549',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_554',\n",
    "]\n",
    "    \n",
    "target = [\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_500',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_502',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_505',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_520',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_523',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_525',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_531',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_541',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_548',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_557',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_503',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_507',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_508',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_510',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_511',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_519',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_521',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_527',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_538',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_544',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_549',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_InO_554',\n",
    "]\n",
    "\n",
    "chunks = [1000000000, 2000000000, 5000000000, 10000000000, 20000000000]\n",
    "for chunk_size in chunks:\n",
    "    in_order_hardware_counters_features = process_csv_feautures(source, chunk_size)\n",
    "    out_of_order_ipc = process_csv_ipc(target, chunk_size)\n",
    "    print('chunk_size: ' + str(chunk_size))\n",
    "    \n",
    "    data = {\n",
    "        'In Order': in_order_hardware_counters_features,\n",
    "        'Out of Order': out_of_order_ipc\n",
    "    }\n",
    "    \n",
    "    length = min(len(in_order_hardware_counters_features), len(out_of_order_ipc))\n",
    "    \n",
    "    # Shorten the longen array\n",
    "    in_order_hardware_counters_features = in_order_hardware_counters_features[:length]\n",
    "    out_of_order_ipc = out_of_order_ipc[:length]\n",
    "    \n",
    "    # separate features and target\n",
    "    X = pd.DataFrame(\n",
    "        in_order_hardware_counters_features, \n",
    "        columns=[\n",
    "            'instructions',\n",
    "            'cpu-cycles', \n",
    "            'br_pred', \n",
    "            'br_mis_pred', \n",
    "            'l1d_cache', \n",
    "            'l1d_tlb', \n",
    "            'l1d_tlb_rd', \n",
    "            'l1d_tlb_wr', \n",
    "            'l2d_cache_rd', \n",
    "            'l2d_cache_wr', \n",
    "            'l2d_cache', \n",
    "            'l1i_cache', \n",
    "            'l1i_cache_refill', \n",
    "            'remote_access', \n",
    "            'dtlb_walk', \n",
    "            'itlb_walk', \n",
    "            'l1i_tlb', \n",
    "            'l2d_tlb', \n",
    "            'l2d_tlb_rd', \n",
    "            'l2d_tlb_wr', \n",
    "            'vfp_spec', \n",
    "            'inst_spec', \n",
    "            'ase_spec', \n",
    "            'stall_backend', \n",
    "            'stall_frontend', \n",
    "            'll_cache_miss_rd', \n",
    "            'mem_access', \n",
    "            'mem_access_rd', \n",
    "            'mem_access_wr',\n",
    "            'ipc'])  # Features\n",
    "    y = pd.Series(out_of_order_ipc) \n",
    "    \n",
    "    # split \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    rf_model = RandomForestRegressor(n_estimators=100)\n",
    "    gb_model = GradientBoostingRegressor(n_estimators=100)\n",
    "    ensemble_model = gb_model\n",
    "    #ensemble_model = VotingRegressor([('rf', rf_model), ('gb', gb_model)])\n",
    "    ensemble_model.fit(X_train, y_train)\n",
    "    \n",
    "    ensemble_predictions = ensemble_model.predict(X_test)\n",
    "    # Calculate R²\n",
    "    ensemble_r2 = r2_score(y_test, ensemble_predictions)\n",
    "    \n",
    "    #bruh wut\n",
    "    train_predictions = ensemble_model.predict(X_train)\n",
    "    test_predictions = ensemble_model.predict(X_test)\n",
    "    \n",
    "    # Calculate R² for training and testing sets\n",
    "    train_r2 = r2_score(y_train, train_predictions)\n",
    "    train_mape = mean_absolute_percentage_error(y_train, train_predictions)\n",
    "    test_r2 = r2_score(y_test, test_predictions)\n",
    "    test_mape = mean_absolute_percentage_error(y_test, test_predictions)\n",
    "    \n",
    "    \n",
    "    print(f\"Training R²: {train_r2}\")\n",
    "    print(f\"Testing R²: {test_r2}\")\n",
    "    print(f\"Training Mape: {train_mape}\")\n",
    "    print(f\"Testing Mape: {test_mape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec5f8efc-7df1-419f-9947-e3cc17903d60",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_size: 1000000000\n",
      "Training R²: 0.7533145571310998\n",
      "Testing R²: 0.7419637152091139\n",
      "Training Mape: 0.12418444499106003\n",
      "Testing Mape: 0.12853438774774842\n",
      "chunk_size: 2000000000\n",
      "Training R²: 0.7809928461612492\n",
      "Testing R²: 0.7741168940579111\n",
      "Training Mape: 0.11251462438696033\n",
      "Testing Mape: 0.11952472798233589\n",
      "chunk_size: 5000000000\n",
      "Training R²: 0.8297198488173694\n",
      "Testing R²: 0.7826559403605657\n",
      "Training Mape: 0.09496954142705173\n",
      "Testing Mape: 0.10412125240410419\n",
      "chunk_size: 10000000000\n",
      "Training R²: 0.862156365163679\n",
      "Testing R²: 0.7887779825869905\n",
      "Training Mape: 0.08769031020547606\n",
      "Testing Mape: 0.10581199471287454\n",
      "chunk_size: 20000000000\n",
      "Training R²: 0.9053415104975564\n",
      "Testing R²: 0.800402649971035\n",
      "Training Mape: 0.07527537279632007\n",
      "Testing Mape: 0.09807163471901054\n"
     ]
    }
   ],
   "source": [
    "# server to OOO\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "def process_csv_feautures(file_paths, chunk_size):\n",
    "    all_features = []\n",
    "    for file_path in file_paths:\n",
    "        df = pd.read_csv(file_path + '_' + str(chunk_size) + '.csv')\n",
    "        df = df.dropna()\n",
    "        features = []\n",
    "        for index, row in df.iterrows():\n",
    "            inst = row['instructions']\n",
    "            cyc = row['cpu-cycles']\n",
    "            ipc = inst/cyc\n",
    "            features.append([\n",
    "                row['instructions'],\n",
    "                row['cpu-cycles'], \n",
    "                row['br_pred'], \n",
    "                row['br_mis_pred'], \n",
    "                row['l1d_cache'], \n",
    "                row['l1d_tlb'], \n",
    "                row['l1d_tlb_rd'], \n",
    "                row['l1d_tlb_wr'], \n",
    "                row['l2d_cache_rd'], \n",
    "                row['l2d_cache_wr'], \n",
    "                row['l2d_cache'], \n",
    "                row['l1i_cache'], \n",
    "                row['l1i_cache_refill'], \n",
    "                row['remote_access'], \n",
    "                row['dtlb_walk'], \n",
    "                row['itlb_walk'], \n",
    "                row['l1i_tlb'], \n",
    "                row['l2d_tlb'], \n",
    "                row['l2d_tlb_rd'], \n",
    "                row['l2d_tlb_wr'], \n",
    "                row['vfp_spec'], \n",
    "                row['inst_spec'], \n",
    "                row['ase_spec'], \n",
    "                row['stall_backend'], \n",
    "                row['stall_frontend'], \n",
    "                row['ll_cache_miss_rd'], \n",
    "                row['mem_access'], \n",
    "                row['mem_access_rd'], \n",
    "                row['mem_access_wr'],\n",
    "                ipc])\n",
    "        all_features.extend(features)\n",
    "    return all_features\n",
    "\n",
    "\n",
    "def process_csv_ipc(file_paths, chunk_size):\n",
    "    all_ipc_values = []\n",
    "    for file_path in file_paths:\n",
    "        df = pd.read_csv(file_path + '_' + str(chunk_size) + '.csv')\n",
    "        df = df.dropna()\n",
    "        for _, row in df.iterrows():\n",
    "            all_ipc_values.append(row['instructions:u'] / row['cpu-cycles:u'])  \n",
    "    return all_ipc_values\n",
    "\n",
    "source = [\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_500',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_502',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_505',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_520',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_523',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_525',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_531',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_541',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_548',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_557',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_503',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_507',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_508',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_510',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_511',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_519',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_521',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_527',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_538',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_544',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_549',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_554',\n",
    "]\n",
    "    \n",
    "target = [\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_500',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_502',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_505',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_520',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_523',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_525',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_531',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_541',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_548',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_557',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_503',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_507',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_508',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_510',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_511',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_519',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_521',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_527',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_538',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_544',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_549',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_edge_heterogeneous/arm_edge_heterogeneous_OOO_554',\n",
    "]\n",
    "\n",
    "chunks = [1000000000, 2000000000, 5000000000, 10000000000, 20000000000]\n",
    "for chunk_size in chunks:\n",
    "    in_order_hardware_counters_features = process_csv_feautures(source, chunk_size)\n",
    "    out_of_order_ipc = process_csv_ipc(target, chunk_size)\n",
    "    print('chunk_size: ' + str(chunk_size))\n",
    "    \n",
    "    data = {\n",
    "        'In Order': in_order_hardware_counters_features,\n",
    "        'Out of Order': out_of_order_ipc\n",
    "    }\n",
    "    \n",
    "    length = min(len(in_order_hardware_counters_features), len(out_of_order_ipc))\n",
    "    \n",
    "    # Shorten the longen array\n",
    "    in_order_hardware_counters_features = in_order_hardware_counters_features[:length]\n",
    "    out_of_order_ipc = out_of_order_ipc[:length]\n",
    "    \n",
    "    # separate features and target\n",
    "    X = pd.DataFrame(\n",
    "        in_order_hardware_counters_features, \n",
    "        columns=[\n",
    "            'instructions',\n",
    "            'cpu-cycles', \n",
    "            'br_pred', \n",
    "            'br_mis_pred', \n",
    "            'l1d_cache', \n",
    "            'l1d_tlb', \n",
    "            'l1d_tlb_rd', \n",
    "            'l1d_tlb_wr', \n",
    "            'l2d_cache_rd', \n",
    "            'l2d_cache_wr', \n",
    "            'l2d_cache', \n",
    "            'l1i_cache', \n",
    "            'l1i_cache_refill', \n",
    "            'remote_access', \n",
    "            'dtlb_walk', \n",
    "            'itlb_walk', \n",
    "            'l1i_tlb', \n",
    "            'l2d_tlb', \n",
    "            'l2d_tlb_rd', \n",
    "            'l2d_tlb_wr', \n",
    "            'vfp_spec', \n",
    "            'inst_spec', \n",
    "            'ase_spec', \n",
    "            'stall_backend', \n",
    "            'stall_frontend', \n",
    "            'll_cache_miss_rd', \n",
    "            'mem_access', \n",
    "            'mem_access_rd', \n",
    "            'mem_access_wr',\n",
    "            'ipc'])  # Features\n",
    "    y = pd.Series(out_of_order_ipc) \n",
    "    \n",
    "    # split \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    rf_model = RandomForestRegressor(n_estimators=100)\n",
    "    gb_model = GradientBoostingRegressor(n_estimators=100)\n",
    "    ensemble_model = gb_model\n",
    "    #ensemble_model = VotingRegressor([('rf', rf_model), ('gb', gb_model)])\n",
    "    ensemble_model.fit(X_train, y_train)\n",
    "    \n",
    "    ensemble_predictions = ensemble_model.predict(X_test)\n",
    "    # Calculate R²\n",
    "    ensemble_r2 = r2_score(y_test, ensemble_predictions)\n",
    "    \n",
    "    #bruh wut\n",
    "    train_predictions = ensemble_model.predict(X_train)\n",
    "    test_predictions = ensemble_model.predict(X_test)\n",
    "    \n",
    "    # Calculate R² for training and testing sets\n",
    "    train_r2 = r2_score(y_train, train_predictions)\n",
    "    train_mape = mean_absolute_percentage_error(y_train, train_predictions)\n",
    "    test_r2 = r2_score(y_test, test_predictions)\n",
    "    test_mape = mean_absolute_percentage_error(y_test, test_predictions)\n",
    "    \n",
    "    \n",
    "    print(f\"Training R²: {train_r2}\")\n",
    "    print(f\"Testing R²: {test_r2}\")\n",
    "    print(f\"Training Mape: {train_mape}\")\n",
    "    print(f\"Testing Mape: {test_mape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3da4ac5-3733-449b-a193-d1a4a8c500cb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_size: 1000000000\n",
      "Training R²: 0.7403851209103898\n",
      "Testing R²: 0.7277453093305843\n",
      "Training Mape: 0.1374821198635464\n",
      "Testing Mape: 0.1417419360400983\n",
      "chunk_size: 2000000000\n",
      "Training R²: 0.7915146614085045\n",
      "Testing R²: 0.7500708235875935\n",
      "Training Mape: 0.11947697616725945\n",
      "Testing Mape: 0.12806107254567628\n",
      "chunk_size: 5000000000\n",
      "Training R²: 0.8484089557859702\n",
      "Testing R²: 0.7931998255199902\n",
      "Training Mape: 0.09785705611159505\n",
      "Testing Mape: 0.10648711944578076\n",
      "chunk_size: 10000000000\n",
      "Training R²: 0.8795095297308809\n",
      "Testing R²: 0.821515110184059\n",
      "Training Mape: 0.09026721542000497\n",
      "Testing Mape: 0.09832747479150447\n",
      "chunk_size: 20000000000\n",
      "Training R²: 0.9345669582281096\n",
      "Testing R²: 0.8152725647040243\n",
      "Training Mape: 0.0699186405144482\n",
      "Testing Mape: 0.10085184652976158\n"
     ]
    }
   ],
   "source": [
    "# server to desktop\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "def process_csv_feautures(file_paths, chunk_size):\n",
    "    all_features = []\n",
    "    for file_path in file_paths:\n",
    "        df = pd.read_csv(file_path + '_' + str(chunk_size) + '.csv')\n",
    "        df = df.dropna()\n",
    "        features = []\n",
    "        for index, row in df.iterrows():\n",
    "            inst = row['instructions']\n",
    "            cyc = row['cpu-cycles']\n",
    "            ipc = inst/cyc\n",
    "            features.append([\n",
    "                row['instructions'],\n",
    "                row['cpu-cycles'], \n",
    "                row['br_pred'], \n",
    "                row['br_mis_pred'], \n",
    "                row['l1d_cache'], \n",
    "                row['l1d_tlb'], \n",
    "                row['l1d_tlb_rd'], \n",
    "                row['l1d_tlb_wr'], \n",
    "                row['l2d_cache_rd'], \n",
    "                row['l2d_cache_wr'], \n",
    "                row['l2d_cache'], \n",
    "                row['l1i_cache'], \n",
    "                row['l1i_cache_refill'], \n",
    "                row['remote_access'], \n",
    "                row['dtlb_walk'], \n",
    "                row['itlb_walk'], \n",
    "                row['l1i_tlb'], \n",
    "                row['l2d_tlb'], \n",
    "                row['l2d_tlb_rd'], \n",
    "                row['l2d_tlb_wr'], \n",
    "                row['vfp_spec'], \n",
    "                row['inst_spec'], \n",
    "                row['ase_spec'], \n",
    "                row['stall_backend'], \n",
    "                row['stall_frontend'], \n",
    "                row['ll_cache_miss_rd'], \n",
    "                row['mem_access'], \n",
    "                row['mem_access_rd'], \n",
    "                row['mem_access_wr'],\n",
    "                ipc])\n",
    "        all_features.extend(features)\n",
    "    return all_features\n",
    "\n",
    "\n",
    "def process_csv_ipc(file_paths, chunk_size):\n",
    "    all_ipc_values = []\n",
    "    for file_path in file_paths:\n",
    "        df = pd.read_csv(file_path + '_' + str(chunk_size) + '.csv')\n",
    "        df = df.dropna()\n",
    "        for _, row in df.iterrows():\n",
    "            all_ipc_values.append(row['instructions'] / row['cpu-cycles'])  \n",
    "    return all_ipc_values\n",
    "\n",
    "source = [\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_500',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_502',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_505',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_520',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_523',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_525',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_531',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_541',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_548',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_557',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_503',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_507',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_508',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_510',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_511',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_519',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_521',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_527',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_538',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_544',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_549',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_server/arm_server_1.5GHz_554',\n",
    "]\n",
    "    \n",
    "target = [\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_500',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_502',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_505',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_520',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_523',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_525',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_531',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_541',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_548',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_557',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_503',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_507',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_508',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_510',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_511',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_519',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_521',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_527',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_538',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_544',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_549',\n",
    "    '../../Data/traces/inst_aligned_traces/arm_desktop/arm_desktop_1.5GHz_554',\n",
    "]\n",
    "\n",
    "chunks = [1000000000, 2000000000, 5000000000, 10000000000, 20000000000]\n",
    "for chunk_size in chunks:\n",
    "    in_order_hardware_counters_features = process_csv_feautures(source, chunk_size)\n",
    "    out_of_order_ipc = process_csv_ipc(target, chunk_size)\n",
    "    print('chunk_size: ' + str(chunk_size))\n",
    "    \n",
    "    data = {\n",
    "        'In Order': in_order_hardware_counters_features,\n",
    "        'Out of Order': out_of_order_ipc\n",
    "    }\n",
    "    \n",
    "    length = min(len(in_order_hardware_counters_features), len(out_of_order_ipc))\n",
    "    \n",
    "    # Shorten the longen array\n",
    "    in_order_hardware_counters_features = in_order_hardware_counters_features[:length]\n",
    "    out_of_order_ipc = out_of_order_ipc[:length]\n",
    "    \n",
    "    # separate features and target\n",
    "    X = pd.DataFrame(\n",
    "        in_order_hardware_counters_features, \n",
    "        columns=[\n",
    "            'instructions',\n",
    "            'cpu-cycles', \n",
    "            'br_pred', \n",
    "            'br_mis_pred', \n",
    "            'l1d_cache', \n",
    "            'l1d_tlb', \n",
    "            'l1d_tlb_rd', \n",
    "            'l1d_tlb_wr', \n",
    "            'l2d_cache_rd', \n",
    "            'l2d_cache_wr', \n",
    "            'l2d_cache', \n",
    "            'l1i_cache', \n",
    "            'l1i_cache_refill', \n",
    "            'remote_access', \n",
    "            'dtlb_walk', \n",
    "            'itlb_walk', \n",
    "            'l1i_tlb', \n",
    "            'l2d_tlb', \n",
    "            'l2d_tlb_rd', \n",
    "            'l2d_tlb_wr', \n",
    "            'vfp_spec', \n",
    "            'inst_spec', \n",
    "            'ase_spec', \n",
    "            'stall_backend', \n",
    "            'stall_frontend', \n",
    "            'll_cache_miss_rd', \n",
    "            'mem_access', \n",
    "            'mem_access_rd', \n",
    "            'mem_access_wr',\n",
    "            'ipc'])  # Features\n",
    "    y = pd.Series(out_of_order_ipc) \n",
    "    \n",
    "    # split \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    rf_model = RandomForestRegressor(n_estimators=100)\n",
    "    gb_model = GradientBoostingRegressor(n_estimators=100)\n",
    "    ensemble_model = gb_model\n",
    "    #ensemble_model = VotingRegressor([('rf', rf_model), ('gb', gb_model)])\n",
    "    ensemble_model.fit(X_train, y_train)\n",
    "    \n",
    "    ensemble_predictions = ensemble_model.predict(X_test)\n",
    "    # Calculate R²\n",
    "    ensemble_r2 = r2_score(y_test, ensemble_predictions)\n",
    "    \n",
    "    #bruh wut\n",
    "    train_predictions = ensemble_model.predict(X_train)\n",
    "    test_predictions = ensemble_model.predict(X_test)\n",
    "    \n",
    "    # Calculate R² for training and testing sets\n",
    "    train_r2 = r2_score(y_train, train_predictions)\n",
    "    train_mape = mean_absolute_percentage_error(y_train, train_predictions)\n",
    "    test_r2 = r2_score(y_test, test_predictions)\n",
    "    test_mape = mean_absolute_percentage_error(y_test, test_predictions)\n",
    "    \n",
    "    \n",
    "    print(f\"Training R²: {train_r2}\")\n",
    "    print(f\"Testing R²: {test_r2}\")\n",
    "    print(f\"Training Mape: {train_mape}\")\n",
    "    print(f\"Testing Mape: {test_mape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4742e29-4899-4697-ac5a-3686ece51cdb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
